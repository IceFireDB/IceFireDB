[{"id":0,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // ‰∏ªÂä®ËøûÊé• if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":1,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase OrbitDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb„ÄÅbadger„ÄÅIPFS„ÄÅCRDT„ÄÅIPFS-LOG„ÄÅOSS network support type P2P P2P„ÄÅRAFT„ÄÅNATS Data type support KV„ÄÅPubSub KV„ÄÅStrings„ÄÅHashes„ÄÅLists„ÄÅSorted Sets„ÄÅSets„ÄÅSQL„ÄÅPubSub Software integration method Software library integration Software library integration, binary software integration„ÄÅweb3 platform integration web3 support No smart contract plan Smart contracts are being supported„ÄÅBuild data dao database platform computer language used to implement Javascript Go Ecological client language Javascript Any client that supports the redis„ÄÅmysql protocol Thanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":2,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/overview/","title":"Overview","section":"Designs","content":" IceFireDB NoSQL Engine Overview # Introduction # IceFireDB is a decentralized database infrastructure that combines cutting-edge research in distributed systems, concurrent databases, and peer-to-peer networking. The NoSQL engine provides a high-performance, decentralized key-value store with support for multiple storage backends and distributed consensus mechanisms.\nCore Architecture # System Components # Component Description Technologies Network Layer Manages node communication and data synchronization RAFT, P2P, NATS Storage Layer Pluggable storage backends with unified interface LevelDB, BadgerDB, IPFS, CRDT, OSS Protocol Layer Client communication protocols RESP (Redis), MySQL, GraphQL (planned) Codec Layer Data encoding/decoding and operation abstraction KV, Strings, Hashes, Lists, Sets, Sorted Sets Storage Drivers # IceFireDB supports multiple storage backends through a unified driver interface:\nLevelDB/BadgerDB: High-performance local disk storage IPFS: Decentralized content-addressable storage CRDT: Conflict-free replicated data types for eventual consistency IPFS-LOG: Append-only log-based storage with IPFS OSS: Object storage service integration HybridDB: Tiered hot/cold storage management Key Features # Decentralization Capabilities # P2P Automatic Networking: Nodes automatically discover and connect to each other CRDT-based Replication: Conflict-free data synchronization across nodes IPFS Integration: Leverages decentralized storage for data persistence RAFT Consensus: Strong consistency within availability zones Performance Characteristics # High Throughput: Optimized for read/write operations with minimal latency Scalable Architecture: Horizontal scaling through node addition Flexible Storage: Multiple backend options for different use cases Efficient Networking: Optimized protocol handling and data transfer Protocol Support # Redis RESP Protocol: Full compatibility with Redis clients and commands Custom Extensions: Enhanced commands for decentralized operations Future Protocols: GraphQL and additional protocol support planned Use Cases # Web2 Applications # Traditional distributed databases with RAFT consensus High-performance key-value stores with disk persistence Redis-compatible caching and session storage Web3 Applications # Decentralized applications requiring data immutability P2P data synchronization across distributed nodes IPFS-based storage for content-addressable data Blockchain integration and transparent logging Getting Started # For quick start guides and deployment instructions, see the Quick Start section.\nPerformance Benchmarks # LevelDB Driver Performance # # Benchmark results with 512 concurrent connections SET: 253,232 requests per second GET: 2,130,875 requests per second Storage Backend Comparison # Backend Read Latency Write Latency Persistence Decentralized LevelDB Low Low Disk No BadgerDB Very Low Low Disk No IPFS Medium Medium Network Yes CRDT Low Medium Eventually Consistent Yes Architecture Diagrams # Overall system architecture showing components and data flow\nBridge architecture connecting web2 and web3 applications\nDevelopment Status # ‚úÖ Production Ready: LevelDB, BadgerDB drivers üü° Beta: IPFS, CRDT, IPFS-LOG drivers üöß In Development: AI vector database, NATS integration üìã Planned: GraphQL protocol, advanced caching, witness layer Community and Support # IceFireDB is developed with support from:\nProtocol Labs Filecoin Foundation FVM (Filecoin Virtual Machine) libp2p community For issues, contributions, and discussions, please visit our GitHub repository.\n"},{"id":3,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":4,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":5,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":6,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":7,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/getting-started/","title":"Getting Started Tutorial","section":"Tutorials","content":" Getting Started with IceFireDB # Introduction # This tutorial will guide you through setting up and using IceFireDB for the first time. We\u0026rsquo;ll cover installation, basic operations, and some advanced features.\nPrerequisites # Go 1.18+ installed Basic understanding of Redis commands Terminal/shell access Installation # Method 1: From Source # # Clone the repository git clone https://github.com/IceFireDB/IceFireDB.git cd IceFireDB # Build the project make build # The binary will be in the bin/ directory ls bin/ Method 2: Using Docker # # Pull the Docker image docker pull icefiredb/icefiredb:latest # Run the container docker run -d -p 11001:11001 --name icefiredb icefiredb/icefiredb:latest Method 3: Pre-built Binaries # Download the latest release from the GitHub releases page.\nBasic Setup # 1. Create Configuration File # Create a basic config.yaml:\nnetwork: port: 11001 storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data\u0026#34; log: level: \u0026#34;info\u0026#34; 2. Start IceFireDB # # Start with configuration ./bin/icefiredb -c config.yaml # Or use default settings ./bin/icefiredb You should see output like:\n[INFO] Starting IceFireDB on :11001 [INFO] Storage driver: leveldb [INFO] Data directory: ./data Basic Redis Operations # Connecting with redis-cli # # Connect to IceFireDB redis-cli -p 11001 # Or if using Docker redis-cli -h localhost -p 11001 Basic Key-Value Operations # # Set and get values 127.0.0.1:11001\u0026gt; SET mykey \u0026#34;Hello IceFireDB\u0026#34; OK 127.0.0.1:11001\u0026gt; GET mykey \u0026#34;Hello IceFireDB\u0026#34; # Work with numbers 127.0.0.1:11001\u0026gt; SET counter 100 OK 127.0.0.1:11001\u0026gt; INCR counter (integer) 101 127.0.0.1:11001\u0026gt; INCRBY counter 50 (integer) 151 # Set with expiration 127.0.0.1:11001\u0026gt; SETEX session:user123 3600 \u0026#34;active\u0026#34; OK 127.0.0.1:11001\u0026gt; TTL session:user123 (integer) 3600 Hash Operations # # Store user data as hash 127.0.0.1:11001\u0026gt; HSET user:1000 name \u0026#34;Alice\u0026#34; email \u0026#34;alice@example.com\u0026#34; age 30 (integer) 3 # Get specific fields 127.0.0.1:11001\u0026gt; HGET user:1000 name \u0026#34;Alice\u0026#34; 127.0.0.1:11001\u0026gt; HGET user:1000 email \u0026#34;alice@example.com\u0026#34; # Get all fields 127.0.0.1:11001\u0026gt; HGETALL user:1000 1) \u0026#34;name\u0026#34; 2) \u0026#34;Alice\u0026#34; 3) \u0026#34;email\u0026#34; 4) \u0026#34;alice@example.com\u0026#34; 5) \u0026#34;age\u0026#34; 6) \u0026#34;30\u0026#34; List Operations # # Push items to list 127.0.0.1:11001\u0026gt; LPUSH tasks \u0026#34;task1\u0026#34; (integer) 1 127.0.0.1:11001\u0026gt; LPUSH tasks \u0026#34;task2\u0026#34; (integer) 2 127.0.0.1:11001\u0026gt; RPUSH tasks \u0026#34;task3\u0026#34; (integer) 3 # Get list range 127.0.0.1:11001\u0026gt; LRANGE tasks 0 -1 1) \u0026#34;task2\u0026#34; 2) \u0026#34;task1\u0026#34; 3) \u0026#34;task3\u0026#34; # Pop items 127.0.0.1:11001\u0026gt; LPOP tasks \u0026#34;task2\u0026#34; 127.0.0.1:11001\u0026gt; RPOP tasks \u0026#34;task3\u0026#34; Using Different Storage Drivers # Switching Storage Backends # # Check current driver 127.0.0.1:11001\u0026gt; DRIVER.INFO 1) \u0026#34;leveldb\u0026#34; 2) \u0026#34;./data\u0026#34; # Switch to BadgerDB 127.0.0.1:11001\u0026gt; DRIVER.SELECT badger OK # Switch to IPFS (decentralized) 127.0.0.1:11001\u0026gt; DRIVER.SELECT ipfs OK Configuration for Different Drivers # Update your config.yaml for specific drivers:\n# For LevelDB storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data/leveldb\u0026#34; # For BadgerDB storage: driver: \u0026#34;badger\u0026#34; data_dir: \u0026#34;./data/badger\u0026#34; # For IPFS storage: driver: \u0026#34;ipfs\u0026#34; ipfs_repo_path: \u0026#34;./data/ipfs\u0026#34; p2p_listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; P2P Networking # Setting Up a Decentralized Cluster # First Node Configuration network: port: 11001 storage: driver: \u0026#34;ipfs\u0026#34; p2p: enable: true discovery_id: \u0026#34;my_cluster\u0026#34; listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; Second Node Configuration network: port: 11002 storage: driver: \u0026#34;ipfs\u0026#34; p2p: enable: true discovery_id: \u0026#34;my_cluster\u0026#34; listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4002\u0026#34; bootstrap_peers: - \u0026#34;/ip4/192.168.1.100/tcp/4001/p2p/QmFirstNodePeerID\u0026#34; Verify Connection # On either node 127.0.0.1:11001\u0026gt; P2P.PEERS 1) \u0026#34;/ip4/192.168.1.101/tcp/4002/p2p/QmSecondNodePeerID\u0026#34; # Data will automatically sync between nodes 127.0.0.1:11001\u0026gt; SET shared_key \u0026#34;cluster_value\u0026#34; OK 127.0.0.1:11002\u0026gt; GET shared_key \u0026#34;cluster_value\u0026#34; Advanced Features # CRDT-based Data Sync # # Enable CRDT mode 127.0.0.1:11001\u0026gt; DRIVER.SELECT crdt OK # Data written in CRDT mode will automatically resolve conflicts 127.0.0.1:11001\u0026gt; SET user:preferences \u0026#39;{\u0026#34;theme\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;}\u0026#39; OK # Force synchronization 127.0.0.1:11001\u0026gt; CRDT.SYNC OK # Check sync status 127.0.0.1:11001\u0026gt; CRDT.STATUS 1) \u0026#34;synced\u0026#34; 2) \u0026#34;2 nodes\u0026#34; 3) \u0026#34;0 conflicts\u0026#34; Hybrid Storage Mode # # config.yaml for hybrid storage storage: driver: \u0026#34;hybriddb\u0026#34; hot_storage: \u0026#34;badger\u0026#34; cold_storage: \u0026#34;ipfs\u0026#34; hot_data_dir: \u0026#34;./data/hot\u0026#34; cold_data_dir: \u0026#34;./data/cold\u0026#34; migration_threshold: 604800 # 7 days in seconds # Data automatically moves between hot and cold storage 127.0.0.1:11001\u0026gt; SET recent_data \u0026#34;frequently accessed\u0026#34; OK 127.0.0.1:11001\u0026gt; SET old_data \u0026#34;rarely accessed\u0026#34; OK # After threshold, old_data moves to cold storage Programming Examples # Python Example # import redis # Connect to IceFireDB r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Basic operations r.set(\u0026#39;python_key\u0026#39;, \u0026#39;Hello from Python!\u0026#39;) print(r.get(\u0026#39;python_key\u0026#39;)) # Hash operations r.hset(\u0026#39;user:python\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;Python User\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;python\u0026#39;) print(r.hgetall(\u0026#39;user:python\u0026#39;)) # Use IceFireDB extensions r.execute_command(\u0026#39;DRIVER.SELECT\u0026#39;, \u0026#39;ipfs\u0026#39;) Node.js Example # const redis = require(\u0026#39;redis\u0026#39;); const client = redis.createClient({ host: \u0026#39;localhost\u0026#39;, port: 11001 }); client.on(\u0026#39;connect\u0026#39;, () =\u0026gt; { console.log(\u0026#39;Connected to IceFireDB\u0026#39;); // Basic operations client.set(\u0026#39;node_key\u0026#39;, \u0026#39;Hello from Node.js!\u0026#39;, redis.print); client.get(\u0026#39;node_key\u0026#39;, (err, reply) =\u0026gt; { console.log(\u0026#39;Value:\u0026#39;, reply); }); // IceFireDB specific command client.send_command(\u0026#39;DRIVER.INFO\u0026#39;, (err, reply) =\u0026gt; { console.log(\u0026#39;Driver info:\u0026#39;, reply); }); }); Go Example # package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) func main() { ctx := context.Background() // Connect to IceFireDB client := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:11001\u0026#34;, Password: \u0026#34;\u0026#34;, // no password set DB: 0, // use default DB }) // Basic operations err := client.Set(ctx, \u0026#34;go_key\u0026#34;, \u0026#34;Hello from Go!\u0026#34;, 0).Err() if err != nil { panic(err) } val, err := client.Get(ctx, \u0026#34;go_key\u0026#34;).Result() if err != nil { panic(err) } fmt.Println(\u0026#34;go_key:\u0026#34;, val) // IceFireDB extension result, err := client.Do(ctx, \u0026#34;DRIVER.INFO\u0026#34;).Result() fmt.Println(\u0026#34;Driver info:\u0026#34;, result) } Monitoring and Maintenance # Checking Server Info # # Get comprehensive server information 127.0.0.1:11001\u0026gt; INFO # Server redis_version:IceFireDB redis_mode:standalone os:Linux # Memory used_memory:1048576 used_memory_human:1.00M # Stats total_connections_received:100 total_commands_processed:500 # Persistence rdb_last_save_time:1633046400 # Replication role:master connected_slaves:0 # CPU used_cpu_sys:10.50 used_cpu_user:15.25 Performance Monitoring # # Use redis-benchmark for performance testing redis-benchmark -h localhost -p 11001 -t set,get -c 50 -n 100000 # Example output: # SET: 253232.12 requests per second # GET: 2130875.50 requests per second Troubleshooting Common Issues # Connection Issues # # Check if IceFireDB is running netstat -tlnp | grep 11001 # Check logs tail -f icefiredb.log Storage Issues # # Check disk space df -h # Check data directory ls -la ./data/ Performance Issues # # Monitor memory usage 127.0.0.1:11001\u0026gt; INFO memory # Check slow logs 127.0.0.1:11001\u0026gt; SLOWLOG GET 10 Next Steps # Explore Advanced Features: Try CRDT mode, IPFS storage, and P2P clustering Read Architecture Docs: Understand how IceFireDB works internally Check Configuration Options: Optimize for your specific use case Join the Community: Contribute and get support on GitHub Additional Resources # API Reference - Complete command documentation Configuration Guide - Detailed configuration options Architecture Overview - System design and components GitHub Repository - Source code and issues Support # If you encounter issues:\nCheck the GitHub issues Join the Discussions Review the documentation Happy building with IceFireDB! üöÄ\n"},{"id":8,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-valueÔºömemory-kv leveldb kv Ôºöleveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // ‰∏ªÂä®ËøûÊé• if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":9,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":10,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":11,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":12,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":13,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/network/","title":"Network layer","section":"Designs","content":" Network layer design # The network layer undertakes the work of inter-node networking, inter-node data distribution, and inter-node data consistency consensus. The network layer of IceFireDB is divided into two layers according to the distance of the physical network link:\nData consistency network layer for short-distance networks. Decentralized database network layer for wide-distance network. The above two different network layers are supported by different technologies and have different requirements for data consistency sensitivity and timeliness.\nParallel cluster network # The smallest component unit of the IceFireDB network cluster is a parallel cluster network guaranteed by the RAFT algorithm, rather than a decentralized P2P network.\nThe IceFireDB parallel cluster uses the RAFT algorithm to form a data consistency layer. Each cluster has a master and multiple slave nodes. They synchronize with each other and maintain the master-slave state of the cluster. Each node stores the same data and writes data. The input is undertaken by the master node, and the master node is responsible for distributing data to the slave nodes. A RAFT cluster internally guarantees the consistency of each data write.\nDecentralized network # The data jumps out of the parallel cluster network and enters the decentralized network. The IceFireDB decentralized network mainly uses P2P technology for automatic networking, and relies on the P2P PubSub communication method to synchronize data commands.\nIceFireDB\u0026rsquo;s use of P2P networking is not only in node discovery, but also provides users with decentralized publish and subscribe middleware by bridging the PubSub network to the RESP protocol. Increase the decentralization capability of the SQLite database by bridging the PubSub network to the SQL protocol.\nDecentralized log synchronization network # Relying solely on P2P and PubSub technologies cannot meet the database requirements for data decentralized synchronization and decentralized data consistency.The current popular CRDT technology can meet the final data consistency, but it cannot guarantee the synchronization order of the data, so it will castrate some data instruction functions of IceFireDB-NoSQL, and the IPFS-LOG technology very well makes up for the functional gap of the decentralized log.\nIceFireDB-NoSQL has established a decentralized log synchronization network based on IPFS-LOG technology. The decentralized IceFireDB nodes broadcast database command logs and build orderly logs to complete the function of data decentralization and consistency.\nIceGiant Network structure # IceFireDB\u0026rsquo;s RAFT has exactly the same data set within the same group of nodes, and in a larger network, we are providing the IceGiant network structure, which aims to break up different tenant database data.\nAlthough all IceGiant nodes are in the same P2P network, the structure of the network can be decomposed according to the data set, which is the data tenant isolation area, which refers to the database area used by a specific application on top of the IceGiant protocol. Different from the traditional blockchain network, IceGiant nodes are only responsible for interacting with peers operating the same data set, and are not responsible for any data of other data sets. Peer IceGiant node sets form a high-availability data storage area.\nIn-process network IO model # IceFireDB supports the following two IO models:\nGolang classic netpoll model: goroutine-per-connection, suitable for situations where the number of connections is not a bottleneck.\nRawEpoll model: that is, Reactor mode, I/O multiplexing (I/O multiplexing) + non-blocking I/O (non-blocking I/O) mode. For scenarios where there are a large number of long links between the access layer and the gateway, it is more suitable for the RawEpoll model.\n"},{"id":14,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb„ÄÅbadger„ÄÅIPFS„ÄÅCRDT„ÄÅIPFS-LOG„ÄÅOSS network support type P2P P2P„ÄÅRAFT„ÄÅNATS Data type support SQL KV„ÄÅStrings„ÄÅHashes„ÄÅLists„ÄÅSorted Sets„ÄÅSets„ÄÅSQL„ÄÅPubSub Software integration method Binary software integration Software library integration, binary software integration„ÄÅweb3 platform integration web3 support No smart contract plan Smart contracts are being supported„ÄÅBuild data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis„ÄÅmysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":15,"href":"/icefiredb_docs/icefiredb/engine_overview/","title":"Engine Overview","section":"Overview","content":" IceFireDB - Decentralized database engine # Engine Description # IPFS and Filecoin are excellent decentralized data storage infrastructures, which have revolutionary significance for the construction of web3. However, with the development of the application ecology, archived data such as pictures and videos can be stored in IPFS or FileCoin, but there is still a lack of database-level storage expression. Although the community also has a decentralized storage solution similar to the KV model, it is only the KV model. It cannot meet the use of upper-layer applications. We believe that more and more complex data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as hash, list, set data structures) and SQL data models. Just like the prosperity of web2\u0026rsquo;s application ecology is inseparable from the contribution of database infrastructures such as memcached, redis, and mysql, the IPFS ecosystem also needs database infrastructure with NoSQL and SQL models.\nApart from storage, IPFS has many excellent components in the ecology, such as libp2p, crdt, ipfs-log and ipld. These components will greatly support the decentralization of data and the ecology of web2. However, since many databases under web2 are mysql and redis at present, if you want to help the applications using these databases achieve decentralization, you need to design more database middleware to facilitate application docking and connect the preceding and the following.\nIn addition to the huge demand for decentralized data storage in the web3 ecosystem, in the current digital age, decentralized networking, decentralized subscriptions, and decentralized storage are used in scenarios such as edge computing, big data, and cloud native. There are more and more strong demands. We combine the demands of web2 and the current situation of web3 to propose the IceFireDB Storage Stack project.\nIceFireDB Storage Stack is committed to creating a complete database storage and database middleware software system for the data decentralization ecosystem. The project currently mainly includes three directions of development: decentralized NoSQL database, decentralized SQLite database, decentralized communication Components and database decentralized middleware (ecological application communication middleware, database decentralized middleware).\nLet\u0026rsquo;s elaborate on the three main construction directions of IceFireDB Storage Stack:\nDecentralized NoSQL database (IceFireDB-NoSQL): Using IPFS as the underlying KV engine, using KV coding technology to achieve more complex data structures, such as hash, set, list, etc., integrate the standard Redis network protocol, allowing applications The IceFIreDB-IPFS-NoSQL database can be used by using the Redis client; the network networking is performed by using libp2p, and the decentralized NoSQL database is constructed by crdt, ipfs-log, and ipld. Decentralized SQLite database (IceFireDB-SQLite): The SQL protocol is currently widely used in the web2 application layer. We design and implement a decentralized SQLite database. The bottom layer uses IPFS-libp2p to build a decentralized network and IPFS-pubsub and peer-to-peer Wait for nodes to synchronize data, and use IPFS CRDT, ipfs-log, and ipld to ensure decentralized consistency of SQL statements. Decentralized database middleware (IceFireDB-PubSub, IceFireDB-Redis-Proxy, IceFireDB-SQLProxy): While we are paying attention to the application development of web3, we also see that IPFS provides data decentralization for applications around the world. Very good libp2p, crdt and other components, we should provide decentralized capabilities for traditional web2 databases and traditional web2 applications, but most web2 applications currently use redis, mysql databases, IceFireDB combined with libp2p, crdt, ipld, ipfs-log The technology adds the wings of data decentralization to traditional mysql and redis, and provides insensitive data decentralization and application decentralization communication capabilities for massive web2 applications, traditional nosql and SQL databases. Engine value # For the web3 world, there is currently no way to write a full-blown client-side application as easily and completely decentralized as in Web2. In Web2, you spin up a database on AWS and have your client applications call that database for reading and writing. But there is nothing like that in Web3. You can\u0026rsquo;t just write data to Ethereum, it\u0026rsquo;s too expensive for most users. Storage protocols such as Filecoin and Arweave are mainly used for archiving data, but do not provide enterprise-level performance guarantees for writing and reading data. IceFireDB Storage Stack uses IPFS as the underlying KV engine, and uses KV encoding technology to achieve more complex NoSQL data structures, For example, hash, set, list, etc. use libp2p, crdt, ipld, ipfs-log to build decentralized NoSQL and SQL databases, and provide RESP and SQL protocol support for easy application access, so that existing massive applications can be changed Solve the decentralized communication and storage capabilities of IPFS at the lowest cost, and expand the application access speed and application ecology of IPFS and Filecoin.\nFor the traditional application fields of web2, most applications currently use Nosql and SQL databases such as redis and mysql. However, with the development of edge computing, big data, cloud native and other fields, these fields also need the support of decentralized network communication and decentralized database storage. We should provide decentralized nosql database and decentralized SQL database to provide decentralized database storage and use support for these applications. It is also necessary to provide decentralized database middleware to increase the capabilities of data broadcast communication and decentralized data storage for traditional redis and mysql databases. The decentralized database, decentralized networking subscription and decentralized database middleware provided by IceFireDB Storage Stack will help IPFS and Filecoin ecology to support decentralized web2 massive applications.\nDatabase technology is the foundation of application storage and application innovation. The development of decentralized application ecology in any era is inseparable from the support of databases and database middleware. IceFireDB Storage Stack is the wings that add data decentralization to applications. It is believed that IceFireDB Storage Stack will be able to support a large number of new applications.\nIceFireDB Storage Stack has been striving to build decentralized NoSQL\\SQL databases and database middleware with richer functions and easier access to application systems based on the decentralized network and decentralized storage technologies of IPFS and Filecoin, helping massive applications to improve Easy access to network decentralization and data decentralization technologies, helping the decentralized application ecosystem to build storage infrastructure, and helping the prosperity of IPFS, Filecoin and the decentralized application ecosystem.\nEngine composition # IceFireDB-NoSQL # The Redis database based on IPFS technology can break the simple kv situation of the current IPFS database and support complex data structures such as hash and list.\nIceFireDB-SQLite # IceFireDB-SQLite database is a decentralized SQLite database. Provide a convenient mechanism to build a global distributed database system. Support users to write data to IceFireDB-SQLite using MySQL protocol. IceFireDB-SQLite stores the data in the SQLite database and synchronizes the data among the nodes in the P2P automatic network.\nIceFireDB-SQLProxy # IceFireDB-SQLProxy is a decentralized SQL database networking system that helps web2 traditional SQL database data decentralization. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. Commands are automatically synchronized between IceFireDB-SQLProxy in the network, and each IceFireDB-SQLProxy writes data to MySQL storage.\nDecentralized networking through IceFireDB-SQLProxy provides web2 program read and write support for SQL, enabling decentralized data synchronization for MySQL database read and write scenarios commonly used in web2 applications.\nIceFireDB-Redis-Proxy # IceFireDB-Redis-proxy database proxy adds decentralization wings to traditional redis databases. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. The instructions are automatically synchronized between the networked redis agents, and the redis agent writes data to the cluster or single-point redis storage. Through the decentralized middleware network proxy, decentralized data synchronization can be enabled for the Redis database commonly used in web2 applications.\nIceFireDB-PubSub # IceFireDB-PubSub is a high performance, high availability and decentralized subscription system.It can seamlessly migrate web2 applications using redis publish and subscribe into a decentralized p2p subscription network.\n"},{"id":16,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb_proxy/","title":"icefiredb-proxy","section":"Develop","content":" IceFireDB-Proxy # Project introduction # IceFireDB-Proxy is a high-performance, high-availability, and user-friendly Resp protocol cluster proxy solution. It is supporting P2P networking and is a network component in the IceFireDB ecosystem.\nFeatures # Complete data source mode support: stand-alone, cluster mode Rich command support Excellent cluster state management and failover Excellent traffic control policies: Traffic read/write separation and multi-tenant data isolation Excellent command telemetry features Bottom-fishing use of mind and base abilities that are closer to cloud native Supports P2P automatic networking, and Proxy helps traditional Redis databases achieve data decentralization. New framework for faster network, will be upgraded soon. redhub Architecture # Network Communication Model # Installing # 1. Install Go 2. git clone https://github.com/IceFireDB/IceFireDB-Proxy.git $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 3. cd $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 4. make Usage # Run a binary file directly, if you need to run in the background can be added to the systemd system management\n./bin/Icefiredb-proxy -c ./config/config.yaml Command support # String # APPEND BITCOUNT BITPOS DECR DECRBY DEL EXISTS GET GETBIT SETBIT GETRANGE GETSET INCR INCRBY MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Set # SADD SCARD SETBIT SISMEMBER SMEMBERS SPOP SRANDMEMBER SREM SSCAN List # LINDEX LINSERT LLEN LPOP LPUSH LPUSHX LRANGE LREM LSET LTRIM RPOP RPUSH RPUSHX Hash # HDEL HEXISTS HGET HGETALL HINCRBY HINCRBYFLOAT HKEYS HLEN HMGET HMSET HSCAN HSET HSETNX HSTRLEN HVALS Sorted Sets # ZADD ZCARD ZCOUNT ZINCRBY ZLEXCOUNT ZPOPMAX ZPOPMIN ZLEXCOUNT ZRANGE ZRANGEBYLEX ZRANGEBYSCORE ZRANK ZREM ZREMRANGEBYLEX ZREMRANGEBYRANK ZREMRANGEBYSCORE ZREVRANGE ZREVRANGEBYLEX ZREVRANGEBYSCORE ZREVRANK ZSCAN ZSCORE Stream # XACK XADD XCLAIM XDEL XLEN XINFO XPENDING XRANGE XREADGROUP XREVRANGE XTRIM XGROUP Others # COMMAND PING QUIT "},{"id":17,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":18,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":19,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":20,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":21,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/storage/","title":"Storage layer","section":"Designs","content":" Storage layer design # The storage layer is responsible for data storage, and the data storage here includes different storage media of web2 and web3. For web2, the storage media we face includes disk, OSS, and for web3, the storage media we face includes IPFS, blockchain, and smart contracts.Currently, the storage types supported by IceFireDB mainly include the following.\nEngine type describe Driver directory LevelDB LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Default Badger BadgerDB is an embeddable, persistent and fast key-value (KV) database written in pure Go. Badger OSS Object storage is a technology that stores and manages data in an unstructured format called objects. OSS IPFS IPFS (the InterPlanetary File System) is a hypermedia distribution protocol addressed by content and identities. It enables the creation of completely distributed applications, and in doing so aims to make the web faster, safer, and more open. IPFS CRDT-KV The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS. CRDT-KV IPFS-LOG icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log. IPFS-LOG OrbitDB OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. OrbitDB Storage model # The NoSQL storage layer of each individual IceGiant mainly includes the codec layer and the underlying KV storage layer. the underlying KV engine currently supports levelDB, badgerDB, IPFS and OSS, and the main data storage includes two ways:\ninstruction broadcast model based on IPFS-LOG\\CRDT_KV\\OrbitDB\nNative data storage model based on LevelDB\\Badger\\OSS\\IPFS\nMultiple IceFireDB nodes will be divided into groups according to data sets, and each group will form a highly available storage area structure.\nNoSQL storage engine # The core of each node is the database engine. By default, IceGiant node integrates KV storage engines such as levelDB, badgerDB, IPFS, OSS, etc., and implements the protocol coding layer of NoSQL on the KV storage relationship. Currently, the data storage of NoSQL mainly includes the following two ways:\nInstruction broadcast model # Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Full storage model # In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on LevelDB\\Badger\\OSS\\IPFS.\n+-------------------------------------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv„ÄÅlist„ÄÅhash„ÄÅset | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ IceGiant Synchronizer # The storage layer of IceFireDB not only includes a complete storage server, but IceGiant Synchronizer, which is currently under construction, also belongs to the ecological software layer of the storage layer.\nIceGiant Synchronizer is an application directly above the database engine. All incoming database requests pass through IceGiant Synchronizer, which determines whether the requests should be processed, whether data writes should be propagated to other parts of the network, and whether local data should be written and customer requests should be responded to.\nIceGiant Synchronizer can also provide data write aggregation function, allowing multiple data requests to be merged and written into a single network storage request. It also allows users to cross-mix data sets between different nodes, encouraging further data decentralization, while keeping the operation overhead low.\n"},{"id":22,"href":"/icefiredb_docs/icefiredb/vocabulary/","title":"Vocabulary","section":"Overview","content":" Vocabulary # The main terms used in IceFireDB products are listed below.\nItem describe NoSQL Non-relational databases, which store data in a format different from relational tables. LSM In computer science, the log-structured merge-tree (also known as LSM tree, or LSMT) is a data structure with performance characteristics that make it attractive for providing indexed access to files with high insert volume, such as transactional log data. OSS Object Storage Service IPFS The InterPlanetary File System (IPFS) is a protocol, hypermedia and file sharing peer-to-peer network for storing and sharing data in a distributed file system. P2P peer-to-peer network EVM Smart contracts A smart contract is a computer program or a transaction protocol that is intended to automatically execute, control or document legally-relevant events and actions according to the terms of a contract or an agreement. CRDT In distributed computing, a conflict-free replicated data type (CRDT) is a data structure that is replicated across multiple computers in a network, RAFT Raft is a consensus algorithm that is designed to be easy to understand. IPLD IPLD stands for InterPlanetary Linked Data,IPLD is an ecosystem of formats and data structures for building applications that can be fully decentralized. IPFS-LOG IPFS-log is an immutable, operation-based collision-free replication data structure (CRDT) for distributed systems. It is an append-only log that can be used to model the variable sharing state between peers in p2p applications. "},{"id":23,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":24,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":25,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":26,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":27,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/protocol/","title":"Protocol layer","section":"Designs","content":" Protocol layer design # A good access method of the application can accelerate the growth of the application ecology, and a good protocol design can reduce the transformation cost of the stock application, so the protocol layer is an important component of the IceFireDB software stack. The communication protocol of IceFireDB-NoSQL fully integrates the Redis RESP protocol, which mainly includes the following two parts of the protocol:\nData control protocol: Complete support for RESP clients, supporting functional requirements for database data access.\nCluster control protocol: Satisfy the client\u0026rsquo;s command protocol for controlling the nodes of the IceFireDB cluster and viewing the status and availability status of the cluster nodes.\n+-------------------------------------------------------------+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +-------------------------------------------------------------+ As can be seen from the figure above, the cluster control protocol is located above the data read and write protocol. The client (redis cluster client, IceFireDB-Proxy) obtains the master-slave structure of the cluster nodes according to the cluster status, and selects the relevant master and slave nodes Perform data read and write operations. For the request traffic carried by IceFireDB, it will enter the request processing cycle. During the request processing cycle, it will analyze and optimize the client request.\nProtocol advantages # At present, the web2 ecology of RESP protocol clients is very rich, and mainstream computer languages have been fully covered. IceFireDB is firstly compatible with the RESP NoSQL protocol, which can quickly meet the needs of existing application systems to access IceFireDB-NoSQL.\nAs a decentralized database, IceFireDB is compatible with the RESP protocol, and can hide high technical intelligence from the user layer. Users do not need to understand P2P, RAFT, CRDT, IPFS-LOG and other technologies, and only need to follow the RESP protocol to choose the appropriate client for application You can operate the database to read and write, and quickly meet the access and transformation of the business system.\nData control protocol # Strings Hashes Lists Sets Sorted Sets APPEND HSET RPUSH SADD ZADD BITCOUNT HGET LPOP SCARD ZCARD BITOP HDEL LINDEX SDIFF ZCOUNT BITPOS HEXISTS LPUSH SDIFFSTORE ZREM DECR HGETALL RPOP SINTER ZCLEAR DECRBY HINCRBY LRANGE SINTERSTORE ZRANK DEL HKEYS LSET SISMEMBER ZRANGE EXISTS HLEN LLEN SMEMBERS ZREVRANGE GET HMGET RPOPLPUSH SREM ZSCORE GETBIT HMSET LCLEAR SUNION ZINCRBY SETBIT HSETEX LCLEAR SUNIONSTORE ZREVRANK GETRANGE HSTRLEN LMCLEAR SCLEAR ZRANGEBYSCORE GETSET HVALS LEXPIRE SMCLEAR ZREVRANGEBYSCORE INCR HCLEAR LEXPIREAT SEXPIRE ZREMRANGEBYSCORE EXISTS HMCLEAR LKEYEXISTS SEXPIRE ZREMRANGEBYRANK GET HEXPIRE LTRIM SEXPIREAT GETBIT HEXPIREAT LTTL STTL SETBIT HKEYEXIST SPERSIST GETRANGE HTTL SKEYEXISTS GETSET INCRBY GET MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Cluster control protocol # IceFireDB-NoSQL integrates some Redis cluster status instructions in the RAFT parallel database scenario.\nVERSION # show the application version MACHINE # show information about the state machine RAFT LEADER # show the address of the current raft leader RAFT INFO [pattern] # show information about the raft server and cluster RAFT SERVER LIST # show all servers in cluster RAFT SERVER ADD id address # add a server to cluster RAFT SERVER REMOVE id # remove a server from the cluster RAFT SNAPSHOT NOW # make a snapshot of the data RAFT SNAPSHOT LIST # show a list of all snapshots on server RAFT SNAPSHOT FILE id # show the file path of a snapshot on server RAFT SNAPSHOT READ id [RANGE start end] # download all or part of a snapshot "},{"id":28,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 „ÄêRedis-server5.0.3„Äë Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second „ÄêRedis-server6.2.5„Äë Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second „ÄêRedis-server6.2.5„Äë Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second „ÄêRedCon„Äë Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second „ÄêRedHub„Äë Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":29,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/codec/","title":"Codec","section":"Architecture","content":" Codec Engine Details # "},{"id":30,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/codec/","title":"Codec layer","section":"Designs","content":" Codec layer design # The codec layer is the glue of the IceFireDB data expression layer, because the bottom layer of IceFireDB supports many storage engines, including centralized storage such as web2 disk, OSS, leveldb, and badger, as well as web3\u0026rsquo;s IPFS, crdt-kv, and IPFS-LOG For this kind of decentralized storage, the storage interface provided by any kind of storage is simple and not standardized. The codec layer of IceFireDB-NoSQL is abstracted through a unified driver layer, and by encoding and decoding many instruction semantics into a KV model, a richer data expression layer is built to support more data scenarios, such as Strings\\Hashs\\Sets\\Lists \\Sorted Sets.\n+-------------------------------------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv„ÄÅlist„ÄÅhash„ÄÅset | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ As an important glue layer, the codec layer connects the network layer, request layer, and KV storage layer.\nCodec advantages # In the decentralized database scenario, although the community also has a decentralized storage solution similar to the KV model, the KV model cannot meet the complex use of upper-level applications. We believe that rich data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as strings\\hashs\\lists\\sets\\sorted sets data structures). Just as the prosperity of the web2 application ecology is inseparable from the contribution of database infrastructure such as memcached, redis, and mysql, IceFireDB provides richer data types and can support data decentralization in more complex scenarios.\nThe encoding and decoding layer encodes data commands and parameters to meet the conversion of many rich instruction data models to KV models. Under the function of this conversion layer, the complex shielding layer of the data model layer and the storage engine layer is effectively constructed.\nThe data instruction layer does not need to care about the underlying storage engine, so it can adapt to various storage engine drivers with the help of the codec layer.\nThe underlying storage engine continues to provide a simple data operation interface (put\\get\\del\\iterator), without direct coupling and customization with the data presentation layer, and the abstract storage driver layer of the codec layer maintains the easy work of the two layers.\nAny problem in computer science can be solved by another layer of indirection.\nCodec Layer value # In addition to adding a richer data model and isolating the complexity of the request layer and storage layer, the codec layer of IceFireDB also has the following functions:\nImprove data access speed: A memory buffer layer is added to the encoding layer to effectively improve the performance of data access. Calculate the middle layerÔºöThe support of complex data structures often requires a certain amount of calculation. At present, the coding layer mainly performs CPU calculations, and subsequent calculations can be combined with GPU and FPGA, which can expand high-performance data calculations and add more possibilities for decentralized databases. blockchain/web3 connection layer: At present, whether it is a web2 or web3 database scenario, data access is the focus of attention, but with the development of blockchain and web3, trusted computing of data and non-tamperable proofs are becoming more and more important. The codec layer of IceFireDB is playing an important role , can be used to combine the blockchain data structure to build web3 side chain infrastructure, reducing the calculation and storage burden of the blockchain layer. "},{"id":31,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/deploy/configuration/","title":"Configuration Guide","section":"Deploy","content":" IceFireDB Configuration Guide # Overview # IceFireDB uses YAML configuration files to manage various aspects of the database system. This guide covers all available configuration options for different IceFireDB components.\nConfiguration File Structure # Main Configuration File # The main configuration file is typically located at config/config.yaml and follows this structure:\n# IceFireDB Main Configuration # Network and binding settings network: bind: \u0026#34;0.0.0.0\u0026#34; port: 11001 max_connections: 10000 # Storage configuration storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data\u0026#34; # P2P networking settings p2p: enable: true discovery_id: \u0026#34;icefiredb_cluster\u0026#34; # Logging configuration log: level: \u0026#34;info\u0026#34; output: \u0026#34;stdout\u0026#34; # Performance tuning performance: cache_size: 1024 max_memory: 1073741824 Core Configuration Options # Network Configuration # Option Description Default network.bind IP address to bind to \u0026quot;0.0.0.0\u0026quot; network.port Port to listen on 11001 network.max_connections Maximum client connections 10000 network.timeout Connection timeout (seconds) 30 Storage Configuration # Option Description Default storage.driver Storage backend driver \u0026quot;leveldb\u0026quot; storage.data_dir Data directory path \u0026quot;./data\u0026quot; storage.compression Enable compression true storage.cache_size Cache size (MB) 1024 Supported Storage Drivers # leveldb - Google LevelDB (default) badger - Dgraph BadgerDB ipfs - IPFS decentralized storage crdt - CRDT-based storage ipfs-log - IPFS append-only log oss - Object storage service hybriddb - Tiered hot/cold storage P2P Network Configuration # Option Description Default p2p.enable Enable P2P networking false p2p.discovery_id Network discovery ID \u0026quot;icefiredb_cluster\u0026quot; p2p.bootstrap_peers Bootstrap peer addresses [] p2p.listen_address P2P listen address \u0026quot;/ip4/0.0.0.0/tcp/4001\u0026quot; Logging Configuration # Option Description Default log.level Log level (debug, info, warn, error) \u0026quot;info\u0026quot; log.output Output destination (stdout, file) \u0026quot;stdout\u0026quot; log.file_path Log file path (if output=file) \u0026quot;./icefiredb.log\u0026quot; log.max_size Max log file size (MB) 100 log.max_backups Max backup files 7 Component-Specific Configuration # IceFireDB-PubSub Configuration # # PubSub Proxy Configuration proxy: local_port: 16379 enable_mtls: false # P2P Config for PubSub p2p: enable: true service_discovery_id: \u0026#34;p2p_redis_proxy_service\u0026#34; service_command_topic: \u0026#34;p2p_redis_proxy_topic\u0026#34; service_discover_mode: \u0026#34;advertise\u0026#34; node_host_ip: \u0026#34;127.0.0.1\u0026#34; node_host_port: 0 # Remote Redis Configuration redisdb: type: \u0026#34;node\u0026#34; # or \u0026#34;cluster\u0026#34; start_nodes: \u0026#34;127.0.0.1:6379\u0026#34; conn_timeout: 5 conn_pool_size: 80 # IP Whitelist ip_white_list: enable: true list: - \u0026#34;127.0.0.1\u0026#34; - \u0026#34;localhost\u0026#34; # Caching cache: enable: true max_items_size: 2048 default_expiration: 5000 cleanup_interval: 120 IceFireDB-Redis-Proxy Configuration # # Similar to PubSub but for Redis proxy proxy: local_port: 16379 redisdb: type: \u0026#34;cluster\u0026#34; start_nodes: \u0026#34;192.168.1.100:7001,192.168.1.100:7002,192.168.1.100:7003\u0026#34; conn_timeout: 5 p2p: enable: true service_discovery_id: \u0026#34;redis_proxy_cluster\u0026#34; IceFireDB-SQLite Configuration # # SQLite-specific configuration sqlite: database_path: \u0026#34;./data/sqlite.db\u0026#34; journal_mode: \u0026#34;WAL\u0026#34; synchronous: \u0026#34;NORMAL\u0026#34; cache_size: 2000 mysql: port: 3306 max_connections: 100 IceFireDB-SQLProxy Configuration # # SQL Proxy configuration mysql: port: 3306 backend_host: \u0026#34;localhost\u0026#34; backend_port: 3306 backend_user: \u0026#34;root\u0026#34; backend_password: \u0026#34;password\u0026#34; p2p: enable: true command_topic: \u0026#34;sql_proxy_commands\u0026#34; Environment Variables # IceFireDB supports configuration through environment variables:\nEnvironment Variable Configuration Equivalent ICEFIREDB_PORT network.port ICEFIREDB_DATA_DIR storage.data_dir ICEFIREDB_STORAGE_DRIVER storage.driver ICEFIREDB_LOG_LEVEL log.level ICEFIREDB_P2P_ENABLE p2p.enable Security Configuration # TLS/SSL Configuration # security: tls: enable: false cert_file: \u0026#34;./certs/server.crt\u0026#34; key_file: \u0026#34;./certs/server.key\u0026#34; ca_file: \u0026#34;./certs/ca.crt\u0026#34; client_auth: \u0026#34;none\u0026#34; Authentication # security: authentication: enable: false password: \u0026#34;\u0026#34; requirepass: \u0026#34;\u0026#34; IP Whitelisting # ip_white_list: enable: true list: - \u0026#34;192.168.1.0/24\u0026#34; - \u0026#34;10.0.0.1\u0026#34; - \u0026#34;localhost\u0026#34; Performance Tuning # Memory Management # performance: max_memory: 1073741824 # 1GB max_memory_policy: \u0026#34;volatile-lru\u0026#34; cache_size: 1024 # 1GB cache max_clients: 10000 Storage Optimization # storage: compression: true block_size: 4096 write_buffer_size: 4194304 max_open_files: 1000 Monitoring and Debugging # Metrics Export # metrics: enable: true port: 9090 path: \u0026#34;/metrics\u0026#34; interval: 60 Profiling # pprof: enable: true port: 6060 Health Checks # health: enable: true port: 8080 path: \u0026#34;/health\u0026#34; Example Configuration Files # Minimal Configuration # network: port: 11001 storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data\u0026#34; log: level: \u0026#34;info\u0026#34; Production Configuration # network: bind: \u0026#34;0.0.0.0\u0026#34; port: 11001 max_connections: 10000 storage: driver: \u0026#34;badger\u0026#34; data_dir: \u0026#34;/var/lib/icefiredb\u0026#34; cache_size: 2048 p2p: enable: true discovery_id: \u0026#34;production_cluster\u0026#34; bootstrap_peers: - \u0026#34;/ip4/10.0.1.100/tcp/4001/p2p/QmPeer1\u0026#34; - \u0026#34;/ip4/10.0.1.101/tcp/4001/p2p/QmPeer2\u0026#34; log: level: \u0026#34;warn\u0026#34; output: \u0026#34;file\u0026#34; file_path: \u0026#34;/var/log/icefiredb.log\u0026#34; max_size: 100 max_backups: 7 security: tls: enable: true cert_file: \u0026#34;/etc/ssl/certs/icefiredb.crt\u0026#34; key_file: \u0026#34;/etc/ssl/private/icefiredb.key\u0026#34; metrics: enable: true port: 9090 Configuration Validation # IceFireDB validates configuration files on startup. Common validation errors include:\nInvalid YAML syntax Unknown configuration options Invalid file paths Port conflicts Memory limits exceeding available system memory Dynamic Configuration # Some configuration options can be changed at runtime using the CONFIG command:\n# Get configuration CONFIG GET log.level # Set configuration CONFIG SET log.level \u0026#34;debug\u0026#34; # Reload configuration CONFIG RELOAD Best Practices # Use environment variables for sensitive data like passwords Regularly backup configuration files Validate configurations before deployment Use comments to document non-obvious settings Version control configuration files Test different storage drivers for your workload Monitor performance and adjust settings accordingly Troubleshooting # Common Issues # Port already in use: Change network.port Permission denied: Check file permissions for data directory Out of memory: Adjust performance.max_memory Storage driver not found: Verify driver name and dependencies Debug Mode # Enable debug logging for troubleshooting:\nlog: level: \u0026#34;debug\u0026#34; output: \u0026#34;stdout\u0026#34; See Also # Deployment Guide - Deployment instructions API Reference - Command documentation Storage Drivers - Storage backend details "},{"id":32,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/api-reference/","title":"API Reference","section":"Develop","content":" IceFireDB NoSQL API Reference # Overview # IceFireDB provides a Redis-compatible API with additional commands for decentralized operations. This document covers the supported commands, their syntax, and usage examples.\nCommand Categories # Server Management Commands # INFO # Returns server information and statistics.\nSyntax:\nINFO [section] Example:\nINFO INFO memory INFO replication Response: Returns a multi-bulk string with server information.\nFLUSHALL # Removes all keys from all databases.\nSyntax:\nFLUSHALL Response: Returns the number of keys removed.\nFLUSHDB # Removes all keys from the current database.\nSyntax:\nFLUSHDB Response: Returns the number of keys removed.\nString Commands # Supported String Operations # Command Description Status SET Set key value ‚úÖ Implemented GET Get key value ‚úÖ Implemented APPEND Append to string ‚úÖ Implemented INCR Increment integer ‚úÖ Implemented DECR Decrement integer ‚úÖ Implemented MGET Get multiple values ‚úÖ Implemented MSET Set multiple values ‚úÖ Implemented SETEX Set with expiration ‚úÖ Implemented Hash Commands # Supported Hash Operations # Command Description Status HSET Set hash field ‚úÖ Implemented HGET Get hash field ‚úÖ Implemented HGETALL Get all fields ‚úÖ Implemented HDEL Delete field ‚úÖ Implemented HINCRBY Increment field ‚úÖ Implemented HKEYS Get all keys ‚úÖ Implemented HVALS Get all values ‚úÖ Implemented List Commands # Supported List Operations # Command Description Status LPUSH Push to left ‚úÖ Implemented RPUSH Push to right ‚úÖ Implemented LPOP Pop from left ‚úÖ Implemented RPOP Pop from right ‚úÖ Implemented LLEN Get list length ‚úÖ Implemented LRANGE Get range ‚úÖ Implemented LINDEX Get by index ‚úÖ Implemented Set Commands # Supported Set Operations # Command Description Status SADD Add to set ‚úÖ Implemented SREM Remove from set ‚úÖ Implemented SMEMBERS Get all members ‚úÖ Implemented SISMEMBER Check membership ‚úÖ Implemented SCARD Get cardinality ‚úÖ Implemented SINTER Intersection ‚úÖ Implemented SUNION Union ‚úÖ Implemented Sorted Set Commands # Supported Sorted Set Operations # Command Description Status ZADD Add to sorted set ‚úÖ Implemented ZREM Remove from sorted set ‚úÖ Implemented ZRANGE Get range ‚úÖ Implemented ZREVRANGE Get reverse range ‚úÖ Implemented ZCARD Get cardinality ‚úÖ Implemented ZSCORE Get score ‚úÖ Implemented ZRANK Get rank ‚úÖ Implemented Extended Commands for Decentralized Operations # Storage Driver Management # DRIVER.SELECT # Select active storage driver.\nSyntax:\nDRIVER.SELECT driver_name Supported Drivers:\nleveldb - Local LevelDB storage badger - Local BadgerDB storage ipfs - IPFS decentralized storage crdt - CRDT-based storage ipfs-log - IPFS log storage oss - Object storage service hybriddb - Tiered storage Example:\nDRIVER.SELECT ipfs DRIVER.INFO # Get current driver information.\nSyntax:\nDRIVER.INFO P2P Network Commands # P2P.CONNECT # Connect to P2P network node.\nSyntax:\nP2P.CONNECT peer_address Example:\nP2P.CONNECT /ip4/192.168.1.100/tcp/4001/p2p/QmPeerID P2P.PEERS # List connected P2P peers.\nSyntax:\nP2P.PEERS CRDT Operations # CRDT.SYNC # Force CRDT synchronization.\nSyntax:\nCRDT.SYNC CRDT.STATUS # Get CRDT synchronization status.\nSyntax:\nCRDT.STATUS Error Codes # IceFireDB uses standard Redis error responses with additional error codes:\nERR wrong number of arguments - Incorrect command syntax ERR unknown command - Unsupported command ERR storage driver error - Storage backend failure ERR p2p network error - P2P connectivity issues ERR crdt sync conflict - CRDT synchronization conflict Response Types # IceFireDB supports all Redis response types:\nSimple Strings: +OK\\r\\n Errors: -ERR message\\r\\n Integers: :1000\\r\\n Bulk Strings: $5\\r\\nhello\\r\\n Arrays: *2\\r\\n$5\\r\\nhello\\r\\n$5\\r\\nworld\\r\\n Protocol Compatibility # IceFireDB is fully compatible with Redis RESP (REdis Serialization Protocol). Clients can use any Redis client library to connect to IceFireDB.\nConnection Example # import redis # Connect to IceFireDB r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Standard Redis operations r.set(\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;) value = r.get(\u0026#39;key\u0026#39;) print(value) # Output: \u0026#39;value\u0026#39; # IceFireDB extended operations r.execute_command(\u0026#39;DRIVER.SELECT\u0026#39;, \u0026#39;ipfs\u0026#39;) Performance Considerations # Use pipelining for bulk operations Choose appropriate storage driver for your use case Monitor P2P network latency for decentralized operations Consider data consistency requirements when selecting CRDT vs RAFT mode See Also # Command Implementation - Detailed command implementation details Storage Drivers - Storage backend documentation Network Protocol - Protocol layer documentation "},{"id":33,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/performance-benchmarking/","title":"Performance Benchmarking","section":"Tutorials","content":" Performance Benchmarking Guide # Overview # This guide covers performance testing and benchmarking methodologies for IceFireDB. Learn how to measure, analyze, and optimize IceFireDB performance for different workloads and storage drivers.\nBenchmarking Tools # redis-benchmark (Recommended) # The standard Redis benchmarking tool works perfectly with IceFireDB:\n# Basic benchmark redis-benchmark -h localhost -p 11001 -t set,get -c 50 -n 100000 # Comprehensive test redis-benchmark -h localhost -p 11001 \\ -t set,get,incr,lpush,lpop,sadd,spop,lpush,lrange \\ -c 100 -n 1000000 -P 16 # Specific command testing redis-benchmark -h localhost -p 11001 \\ -t set -c 50 -n 500000 --csv # Pipeline testing redis-benchmark -h localhost -p 11001 \\ -t set,get -c 100 -n 1000000 -P 100 memtier_benchmark # For more advanced benchmarking:\nmemtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=1:1 --test-time=300 --key-pattern=S:S Custom Scripts # Python example for custom workload testing:\nimport redis import time import statistics r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Warm up for i in range(1000): r.set(f\u0026#39;key_{i}\u0026#39;, f\u0026#39;value_{i}\u0026#39;) # Benchmark SET operations times = [] for i in range(10000): start = time.time() r.set(f\u0026#39;bench_{i}\u0026#39;, \u0026#39;x\u0026#39; * 100) times.append(time.time() - start) print(f\u0026#34;SET ops/sec: {10000/sum(times):.0f}\u0026#34;) print(f\u0026#34;Average latency: {statistics.mean(times)*1000:.2f}ms\u0026#34;) print(f\u0026#34;P95 latency: {sorted(times)[9500]*1000:.2f}ms\u0026#34;) Benchmarking Methodology # Test Environment Setup # Hardware Considerations:\nCPU: Modern multi-core processor Memory: Sufficient RAM for workload Storage: SSD recommended for disk-based drivers Network: Gigabit Ethernet for distributed tests IceFireDB Configuration:\nnetwork: max_connections: 10000 performance: cache_size: 2048 max_memory: 4294967296 log: level: \u0026#34;warn\u0026#34; # Reduce logging overhead System Tuning:\n# Increase file limits ulimit -n 100000 # Network tuning sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.tcp_max_syn_backlog=65535 Benchmark Scenarios # 1. Throughput Testing # # Maximum throughput test redis-benchmark -h localhost -p 11001 \\ -t set -c 512 -n 10000000 -P 512 -q # Expected output: # SET: 253232.12 requests per second 2. Latency Testing # # Low concurrency latency test redis-benchmark -h localhost -p 11001 \\ -t set -c 1 -n 10000 -P 1 --csv # High percentiles memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=10 --threads=2 \\ --ratio=1:0 --test-time=60 --key-pattern=S:S \\ --hide-histogram 3. Mixed Workload Testing # # Read-heavy workload (80% read, 20% write) memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=4:1 --test-time=300 # Write-heavy workload memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=1:4 --test-time=300 4. Data Size Impact # # Different value sizes for size in 100 500 1000 5000; do redis-benchmark -h localhost -p 11001 \\ -t set -c 50 -n 100000 -d $size -q echo \u0026#34;Value size: $size bytes\u0026#34; done Performance by Storage Driver # LevelDB Driver Performance # Best for: Balanced read/write workloads\n# LevelDB benchmark results SET: 250,000 - 300,000 ops/sec GET: 2,000,000 - 2,500,000 ops/sec Latency: 0.1 - 2ms (P99) Optimization Tips:\nIncrease write_buffer_size for write-heavy workloads Use compression for smaller data sizes Adjust block_size for your access patterns BadgerDB Driver Performance # Best for: Write-intensive workloads\n# BadgerDB benchmark results SET: 300,000 - 400,000 ops/sec GET: 1,800,000 - 2,200,000 ops/sec Latency: 0.1 - 3ms (P99) Optimization Tips:\nUse SSD storage for best performance Tune value_log_file_size for your workload Enable compression for value logs IPFS Driver Performance # Best for: Decentralized storage\n# IPFS benchmark results (local node) SET: 15,000 - 25,000 ops/sec GET: 40,000 - 60,000 ops/sec Latency: 5 - 50ms (P99) # IPFS benchmark results (remote nodes) SET: 5,000 - 10,000 ops/sec GET: 10,000 - 20,000 ops/sec Latency: 20 - 200ms (P99) Optimization Tips:\nIncrease hot_cache_size for better read performance Use local IPFS nodes for lower latency Optimize network connectivity between nodes CRDT Driver Performance # Best for: Distributed consistency\n# CRDT benchmark results SET: 120,000 - 180,000 ops/sec GET: 700,000 - 900,000 ops/sec Latency: 1 - 5ms (P99) Sync Latency: 10 - 100ms (cross-node) Optimization Tips:\nAdjust sync_interval based on consistency requirements Monitor conflict resolution overhead Use appropriate conflict resolution strategy Advanced Benchmarking # Long-running Tests # # 24-hour endurance test memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=100 --threads=8 \\ --ratio=1:1 --test-time=86400 --key-pattern=R:R # Monitor memory usage over time while true; do redis-cli -p 11001 info memory | grep used_memory_human sleep 60 done Cluster Benchmarking # # Multi-node cluster test # On node 1 memtier_benchmark -s node1 -p 11001 --clients=50 # On node 2 memtier_benchmark -s node2 -p 11001 --clients=50 # Monitor cluster sync performance redis-cli -p 11001 info replication Custom Workload Generation # Python script for realistic workload simulation:\nimport redis import random import time r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001) # Realistic key distribution (power law) def generate_key(): if random.random() \u0026lt; 0.8: # 80% hot keys return f\u0026#39;hot_{random.randint(1, 1000)}\u0026#39; else: # 20% cold keys return f\u0026#39;cold_{random.randint(1, 100000)}\u0026#39; # Simulate real workload operations = [] for _ in range(1000000): key = generate_key() if random.random() \u0026lt; 0.7: # 70% reads operations.append((\u0026#39;GET\u0026#39;, key)) else: # 30% writes operations.append((\u0026#39;SET\u0026#39;, key, f\u0026#39;value_{random.randint(1, 10000)}\u0026#39;)) # Execute and measure start = time.time() for op in operations: if op[0] == \u0026#39;GET\u0026#39;: r.get(op[1]) else: r.set(op[1], op[2]) duration = time.time() - start print(f\u0026#34;Operations: {len(operations)}\u0026#34;) print(f\u0026#34;Duration: {duration:.2f}s\u0026#34;) print(f\u0026#34;Throughput: {len(operations)/duration:.0f} ops/sec\u0026#34;) Performance Metrics # Key Metrics to Monitor # Throughput: Operations per second Latency: Response time percentiles (P50, P95, P99) Memory Usage: RSS, used_memory, peak memory CPU Utilization: User vs system time Network I/O: Bytes in/out, connections Disk I/O: Read/write operations, throughput Monitoring Commands # # Real-time monitoring redis-cli -p 11001 --stat # Detailed metrics redis-cli -p 11001 info all # Specific sections redis-cli -p 11001 info memory redis-cli -p 11001 info stats redis-cli -p 11001 info persistence # Slow log analysis redis-cli -p 11001 slowlog get 10 Optimization Techniques # Configuration Optimizations # # High-performance configuration network: max_connections: 100000 tcp_keepalive: 300 performance: cache_size: 4096 # 4GB max_memory: 8589934592 # 8GB max_memory_policy: \u0026#34;volatile-lru\u0026#34; storage: driver: \u0026#34;badger\u0026#34; value_log_file_size: 2147483648 # 2GB num_compactors: 4 num_level_zero_tables: 8 OS-Level Optimizations # # Linux performance tuning echo \u0026#39;net.core.somaxconn=65535\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026#39;vm.overcommit_memory=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026#39;vm.swappiness=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf # SSD optimization echo \u0026#39;deadline\u0026#39; \u0026gt; /sys/block/sda/queue/scheduler # Memory management echo \u0026#39;never\u0026#39; \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled Application-Level Optimizations # Use pipelining for bulk operations Batch small operations together Use appropriate data types for your workload Monitor and evict unused keys regularly Use compression for large values Troubleshooting Performance Issues # Common Performance Problems # High Latency:\nCheck network connectivity Monitor system resource usage Review slow log queries Low Throughput:\nVerify client configuration Check for resource bottlenecks Review storage driver performance Memory Issues:\nMonitor memory usage patterns Adjust max memory policy Implement key eviction strategies Diagnostic Commands # # Check current performance redis-cli -p 11001 info commandstats # Monitor real-time performance redis-cli -p 11001 monitor | head -100 # Analyze memory usage redis-cli -p 11001 memory stats # Check persistence performance redis-cli -p 11001 info persistence Benchmark Results Interpretation # Expected Performance Ranges # Driver SET ops/sec GET ops/sec P99 Latency LevelDB 200-300K 1.8-2.5M 1-2ms BadgerDB 300-400K 1.6-2.2M 1-3ms IPFS 15-25K 40-60K 5-50ms CRDT 120-180K 700-900K 1-5ms Factors Affecting Performance # Data Size: Larger values reduce throughput Concurrency: Higher concurrency increases throughput but may increase latency Network: Latency and bandwidth affect distributed performance Hardware: CPU, memory, and storage speed determine maximum performance Workload Pattern: Read vs write ratio affects optimal configuration Continuous Performance Testing # Automated Benchmarking # Set up automated performance testing:\n#!/bin/bash # daily-benchmark.sh DATE=$(date +%Y%m%d) RESULTS=\u0026#34;benchmark_results_$DATE.csv\u0026#34; echo \u0026#34;date,driver,test,throughput,latency_p99\u0026#34; \u0026gt; $RESULTS # Test different drivers for driver in leveldb badger ipfs; do # Switch driver redis-cli -p 11001 DRIVER.SELECT $driver # Run benchmarks redis-benchmark -h localhost -p 11001 -t set -c 50 -n 100000 -q | \\ awk -v d=\u0026#34;$driver\u0026#34; -v date=\u0026#34;$DATE\u0026#34; \u0026#39;{print date \u0026#34;,\u0026#34; d \u0026#34;,set,\u0026#34; $1 \u0026#34;,\u0026#34;}\u0026#39; \u0026gt;\u0026gt; $RESULTS redis-benchmark -h localhost -p 11001 -t get -c 50 -n 100000 -q | \\ awk -v d=\u0026#34;$driver\u0026#34; -v date=\u0026#34;$DATE\u0026#34; \u0026#39;{print date \u0026#34;,\u0026#34; d \u0026#34;,get,\u0026#34; $1 \u0026#34;,\u0026#34;}\u0026#39; \u0026gt;\u0026gt; $RESULTS done Performance Regression Testing # Monitor performance over time to detect regressions:\n# Weekly performance report #!/bin/bash WEEK=$(date +%Y-%U) redis-cli -p 11001 info stats \u0026gt; \u0026#34;stats_$WEEK.log\u0026#34; redis-cli -p 11001 info memory \u0026gt; \u0026#34;memory_$WEEK.log\u0026#34; # Compare with previous week # Alert on significant changes Best Practices # Test realistic workloads that match production usage Run long-term tests to identify memory leaks or degradation Monitor system resources during testing Document benchmark configurations for reproducibility Compare across versions to track performance improvements Test failure scenarios and recovery performance Validate with multiple tools for comprehensive analysis See Also # Storage Drivers - Driver-specific performance characteristics Configuration Guide - Performance-related configuration options API Reference - Performance monitoring commands "},{"id":34,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/quick_start/","title":"Quick Start","section":"IceFireDB NoSQL Engine","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test clusterÔºåSimulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key cmd value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write testsÔºö\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normallyÔºåThe current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":35,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/storage-drivers/","title":"Storage Drivers","section":"Designs","content":" IceFireDB Storage Drivers # Overview # IceFireDB supports multiple storage backends through a unified driver interface. This allows you to choose the best storage engine for your specific use case, whether you need high-performance local storage or decentralized distributed storage.\nDriver Architecture # All storage drivers implement the same interface:\ntype IDB interface { Put(key, value []byte) error Get(key []byte) ([]byte, error) Delete(key []byte) error Close() error // ... additional methods } This consistent interface enables seamless switching between different storage engines.\nAvailable Drivers # LevelDB Driver # Best for: General purpose, high-performance local storage\nDescription: Google LevelDB is a fast key-value storage library that provides ordered mapping from string keys to string values.\nFeatures:\nHigh performance for read-intensive workloads Snappy compression support Atomic batch operations Crash consistency Configuration:\nstorage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data/leveldb\u0026#34; compression: true block_size: 4096 write_buffer_size: 4194304 Performance:\nRead: ~2M ops/sec Write: ~250K ops/sec Latency: \u0026lt;1ms BadgerDB Driver # Best for: Write-intensive workloads, SSD optimization\nDescription: Dgraph BadgerDB is a fast key-value store written purely in Go, optimized for SSDs.\nFeatures:\nFaster writes than LevelDB SSD-optimized design Memory-mapped I/O Encryption support Configuration:\nstorage: driver: \u0026#34;badger\u0026#34; data_dir: \u0026#34;./data/badger\u0026#34; value_log_file_size: 1073741824 sync_writes: false encryption_key: \u0026#34;\u0026#34; Performance:\nRead: ~1.8M ops/sec Write: ~300K ops/sec Latency: \u0026lt;1ms IPFS Driver # Best for: Decentralized storage, content-addressable data\nDescription: InterPlanetary File System driver stores data on IPFS network with local caching.\nFeatures:\nContent-addressable storage Distributed and decentralized Data deduplication Local hot cache for performance Configuration:\nstorage: driver: \u0026#34;ipfs\u0026#34; ipfs_repo_path: \u0026#34;./data/ipfs\u0026#34; endpoint_connection: \u0026#34;http://localhost:5001\u0026#34; hot_cache_size: 1024 p2p_listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; Performance:\nRead: ~50K ops/sec (depends on network) Write: ~20K ops/sec (depends on network) Latency: 10-100ms CRDT Driver # Best for: Conflict-free replicated data, eventual consistency\nDescription: Conflict-free Replicated Data Types driver for automatic conflict resolution in distributed systems.\nFeatures:\nAutomatic conflict resolution Eventual consistency Multi-master replication Type-specific merge semantics Configuration:\nstorage: driver: \u0026#34;crdt\u0026#34; data_dir: \u0026#34;./data/crdt\u0026#34; conflict_resolution: \u0026#34;last_write_wins\u0026#34; sync_interval: 5000 max_conflict_history: 1000 Performance:\nRead: ~800K ops/sec Write: ~150K ops/sec Latency: 2-5ms IPFS-LOG Driver # Best for: Append-only logs, audit trails, immutable data\nDescription: IPFS-based append-only log storage for immutable data records.\nFeatures:\nImmutable append-only storage Cryptographic verification Distributed audit trails Historical data preservation Configuration:\nstorage: driver: \u0026#34;ipfs-log\u0026#34; data_dir: \u0026#34;./data/ipfs-log\u0026#34; log_segment_size: 10485760 compression: true encryption: false Performance:\nAppend: ~15K ops/sec Read: ~100K ops/sec Latency: 5-20ms OSS Driver # Best for: Cloud object storage integration, large datasets\nDescription: Object Storage Service driver for cloud storage integration (S3-compatible).\nFeatures:\nCloud storage integration Cost-effective for large datasets Durability and availability Multi-region support Configuration:\nstorage: driver: \u0026#34;oss\u0026#34; endpoint: \u0026#34;https://s3.amazonaws.com\u0026#34; bucket: \u0026#34;my-icefiredb-bucket\u0026#34; access_key: \u0026#34;AKIA...\u0026#34; secret_key: \u0026#34;secret...\u0026#34; region: \u0026#34;us-east-1\u0026#34; Performance:\nRead: ~5K ops/sec (depends on cloud) Write: ~2K ops/sec (depends on cloud) Latency: 50-200ms HybridDB Driver # Best for: Tiered storage, cost optimization\nDescription: Hybrid storage driver that automatically moves data between hot and cold storage tiers.\nFeatures:\nAutomatic data tiering Cost optimization Performance tuning Custom migration policies Configuration:\nstorage: driver: \u0026#34;hybriddb\u0026#34; hot_storage: \u0026#34;badger\u0026#34; cold_storage: \u0026#34;oss\u0026#34; hot_data_dir: \u0026#34;./data/hot\u0026#34; migration_threshold: 604800 hot_cache_size: 2048 cold_bucket: \u0026#34;my-cold-storage\u0026#34; Performance:\nHot storage: Same as underlying driver Cold storage: Same as underlying driver Migration: Background process Driver Selection Guide # Performance Considerations # Driver Read Performance Write Performance Latency Best Use Case LevelDB ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê General purpose BadgerDB ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Write-intensive IPFS ‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê Decentralized CRDT ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Distributed sync IPFS-LOG ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Immutable logs OSS ‚≠ê ‚≠ê ‚≠ê Cloud storage HybridDB ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Tiered storage Consistency Models # Driver Consistency Model Replication Conflict Resolution LevelDB Strong Single-node N/A BadgerDB Strong Single-node N/A IPFS Eventual Multi-master Manual CRDT Eventual Multi-master Automatic IPFS-LOG Strong Multi-master Append-only OSS Eventual Multi-region Last-write-wins HybridDB Configurable Depends on tiers Depends on tiers Storage Requirements # Driver Memory Disk Network Special Requirements LevelDB Medium High Low None BadgerDB High High Low SSD recommended IPFS Medium Medium High IPFS node CRDT High Medium Medium None IPFS-LOG Low High High IPFS node OSS Low Low High Cloud credentials HybridDB Medium Variable Variable Multiple backends Configuration Examples # Basic LevelDB Configuration # storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;/var/lib/icefiredb\u0026#34; compression: true block_size: 4096 write_buffer_size: 4194304 max_open_files: 1000 Production IPFS Configuration # storage: driver: \u0026#34;ipfs\u0026#34; ipfs_repo_path: \u0026#34;/var/lib/ipfs\u0026#34; endpoint_connection: \u0026#34;http://localhost:5001\u0026#34; hot_cache_size: 2048 p2p_listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; bootstrap_peers: - \u0026#34;/ip4/192.168.1.100/tcp/4001/p2p/QmPeer1\u0026#34; - \u0026#34;/dns4/bootstrap.libp2p.io/tcp/443/p2p/QmNnoo...\u0026#34; Hybrid Storage Configuration # storage: driver: \u0026#34;hybriddb\u0026#34; hot_storage: \u0026#34;badger\u0026#34; cold_storage: \u0026#34;oss\u0026#34; hot_data_dir: \u0026#34;/var/lib/icefiredb/hot\u0026#34; migration_threshold: 2592000 # 30 days hot_cache_size: 4096 # OSS configuration endpoint: \u0026#34;https://oss-cn-hangzhou.aliyuncs.com\u0026#34; bucket: \u0026#34;icefiredb-cold-storage\u0026#34; access_key: \u0026#34;your-access-key\u0026#34; secret_key: \u0026#34;your-secret-key\u0026#34; region: \u0026#34;cn-hangzhou\u0026#34; Runtime Driver Management # Switching Drivers # # Check current driver 127.0.0.1:11001\u0026gt; DRIVER.INFO # Switch to different driver 127.0.0.1:11001\u0026gt; DRIVER.SELECT badger # Verify switch 127.0.0.1:11001\u0026gt; DRIVER.INFO Driver-specific Commands # Each driver may support additional commands:\n# CRDT synchronization 127.0.0.1:11001\u0026gt; CRDT.SYNC # IPFS peer management 127.0.0.1:11001\u0026gt; P2P.PEERS # HybridDB migration status 127.0.0.1:11001\u0026gt; HYBRID.STATUS Performance Tuning # LevelDB/BadgerDB Tuning # performance: cache_size: 2048 # 2GB cache max_open_files: 5000 compression: true write_buffer_size: 67108864 # 64MB IPFS Performance Optimization # storage: driver: \u0026#34;ipfs\u0026#34; hot_cache_size: 4096 # 4GB cache chunk_size: 262144 # 256KB chunks replication_factor: 3 low_water: 100 high_water: 200 Monitoring Storage Performance # # Get storage statistics 127.0.0.1:11001\u0026gt; INFO storage # Monitor driver performance 127.0.0.1:11001\u0026gt; DRIVER.STATS Migration Between Drivers # Data Migration Process # Backup current data Install new driver dependencies Update configuration Restart IceFireDB Verify data integrity Automated Migration Tools # IceFireDB provides tools for migrating between storage drivers while maintaining data availability.\nTroubleshooting # Common Issues # Driver not found: Install required dependencies Permission denied: Check filesystem permissions Out of memory: Adjust cache sizes Network issues: Check connectivity for cloud/ipfs drivers Debug Commands # # Get detailed driver information 127.0.0.1:11001\u0026gt; DRIVER.DEBUG # Check storage health 127.0.0.1:11001\u0026gt; STORAGE.HEALTH # Reset driver configuration 127.0.0.1:11001\u0026gt; DRIVER.RESET Best Practices # Choose the right driver for your workload Monitor performance and adjust configuration Regularly backup important data Test migrations in staging environment Keep drivers updated with latest versions Use hybrid approaches for cost optimization Monitor storage health proactively See Also # Configuration Guide - General configuration options Performance Benchmarking - Performance testing guide API Reference - Driver management commands "},{"id":36,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/","title":"Designs","section":"IceFireDB NoSQL Engine","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv„ÄÅlist„ÄÅhash„ÄÅset | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer 1. RAFT guarantees data consistency within a single availability zone. 2. P2P network construction decentralized database communication. 3. NATS is a new network layer being built. P2P„ÄÅRAFT„ÄÅNATS Storage layer Many types of storage are currently supported. Under the codec computing layer, we abstract the KV storage driver layer, which is compatible with different storage engines of web2 and web3. goleveldb„ÄÅbadger„ÄÅIPFS„ÄÅCRDT„ÄÅIPFS-LOG„ÄÅOSS Protocol layer Based on the codec layer, we have built a protocol layer. A good communication protocol allows more applications to easily access the IceFireDB data network. Currently, we support the Redis-RESP NoSQL protocol and the MySQL protocol. RESP„ÄÅSQL Codec layer The codec layer is the core of our system. For NoSQL scenarios, any data type will be abstracted into a KV storage model. With the flexible coding layer, we can build rich data operation structures and instructions, such as hash, sets, strings, etc. KV„ÄÅStrings„ÄÅHashes„ÄÅLists„ÄÅSorted Sets„ÄÅSets„ÄÅSQL„ÄÅPubSub "},{"id":37,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/","title":"IceFireDB NoSQL Engine","section":"Overview","content":" IceFireDB NoSQL Engine # Overview # IceFireDB NoSQL is a high-performance, decentralized key-value database that provides Redis compatibility with advanced distributed systems capabilities. It supports multiple storage backends, P2P networking, and various consistency models.\nKey Features # Redis Protocol Compatibility: Full support for Redis RESP protocol Multiple Storage Backends: LevelDB, BadgerDB, IPFS, CRDT, and more Decentralized Architecture: P2P automatic networking and data synchronization High Performance: Optimized for low latency and high throughput Flexible Consistency: RAFT for strong consistency, CRDT for eventual consistency Documentation Sections # Designs # Architecture overview and system design Network layer implementation Storage engine details Protocol specifications Codec layer architecture Develop # API reference and command documentation Development guides and examples Custom driver development Extension development Deploy # Installation and configuration Deployment strategies Performance tuning Monitoring and maintenance Quick Start # Getting started tutorials Basic usage examples First steps with IceFireDB Tutorials # Step-by-step guides Practical examples Advanced usage scenarios Integration tutorials Getting Started # Quick Installation # # Using Docker docker run -d -p 11001:11001 icefiredb/icefiredb:latest # From source git clone https://github.com/IceFireDB/IceFireDB.git cd IceFireDB \u0026amp;\u0026amp; make build Basic Usage # # Connect with redis-cli redis-cli -p 11001 # Basic operations SET mykey \u0026#34;Hello IceFireDB\u0026#34; GET mykey Use Cases # Web2 Applications # Traditional distributed caching Session storage Real-time data processing High-performance key-value stores Web3 Applications # Decentralized application data storage P2P data synchronization Immutable data logging Blockchain-integrated databases Hybrid Applications # Tiered storage (hot/cold data) Multi-cloud data distribution Edge computing data persistence Cross-region data replication Performance # IceFireDB delivers excellent performance across different workloads:\nLevelDB Driver: 250K+ SET ops/sec, 2M+ GET ops/sec Low Latency: Sub-millisecond response times High Concurrency: Support for thousands of concurrent connections Scalable Architecture: Linear scaling with node addition Community and Support # GitHub: https://github.com/IceFireDB/IceFireDB Documentation: https://www.icefiredb.xyz/ Discussions: GitHub Discussions Issues: GitHub Issues Contributing # IceFireDB is open source and welcomes contributions. See the CONTRIBUTING.md guide for details.\nLicense # IceFireDB is released under the Apache License 2.0. See LICENSE for details.\n"},{"id":38,"href":"/icefiredb_docs/icefiredb/","title":"Overview","section":"IceFireDB - Building global database infrastructure","content":" IceFireDB - Building global database infrastructure # Overview # Based on cutting-edge computer science, the IceFireDB project integrates new ideas and research in the fields of message passing, event instruction processing, data consistency and replication, decentralized network, reliable data storage, high-performance network framework, etc.The team of researchers and engineers at IceFireDB combines the cutting-edge ideas from distributed systems and concurrent databases. These ideas combine collision-free replication data types (CRDT) and decentralized appendix-only logs, which can be used to simulate the variable sharing state between peers in P2P applications to create a new data infrastructure with high security, high performance and low latency.\nThe core of IceFireDB architecture is geographically distributed event source and decentralized Log source, with log-level CRDT replication.In order to realize the consistency of replication, IceFireDB provides a stable decentralized networking model, which allows the combination of public networks among different sites. Multiple IceFireDB nodes can be run inside each site, and RAFT network can be formed between nodes, which ensures the data consistency and stable storage within the same site.\nConcurrent change conflict handling # Different from the final consistent database, IceFireDB manages changes at the atomic level and merges data changes made through the network. It maintains a single version of data through CRDT, IPFS-LOG and status convergence engine, and maintains the incremental model of decentralized instructions by building the icefiredb-log library, which meets the broadcast and data sequence consistency of decentralized database logs.\nThe technical use of CRDT and IPFS-LOG is implicit. IceFireDB exposes the conventional Redis, SQL database and API to users by integrating the decentralized technology and functions with RESP and SQL protocol servers.\nAdaptive consistency # IceFireDB data network responds to data reading and updating requests, while maintaining a consistent data view in the world. IceFireDB network can effectively improve the delay and flexibly expand to meet the needs of applications.\nNon-inductive data coordination # IceFireDB allows all locations to read and write local data in parallel, and does not require users to know which data should be placed in which location, or require users to redesign the architecture every time they need to add or delete locations.\nDistributed multi-model database # IceFireDB supports a variety of data models, mainly supporting NoSQL and SQL data scenarios. Store, query and modify data in the form of NoSQL and SQL on our multi-master architecture. Respond to read and update requests with local latency, while maintaining a consistent view of global fast data.\n"},{"id":39,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/","title":"Tutorials","section":"IceFireDB NoSQL Engine","content":" Tutorials # "},{"id":40,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/deploy/","title":"Deploy","section":"IceFireDB NoSQL Engine","content":" Deploy # "},{"id":41,"href":"/icefiredb_docs/icefiredb/faqs/","title":"FAQs","section":"Overview","content":" FAQs # 1. Would you be able to write IPLD Schemas and specs for the data structures you\u0026rsquo;re using? This would allow others to implement writers/readers for their data. # Our current database underlying data storage engine implementation is divided into two categories:\nThe first kind: build database synchronization technology based on RESP instructions and SQL statements, which can only grow and cannot be tampered with, and realize data synchronization and consistency guarantee between nodes based on decentralized instruction broadcasting and storage instruction playback. Type two: realize KV storage engine based on IPFS kv, and build rich nosql data structure by coding kv-value, so that the data of database can grow on ipfs completely, instead of relying on ipfs-log function. At present, the first type of database is mainly implemented. We mainly use ipfs-log, an immutable and conflict-free replication data model for distributed systems. Based on ipfs-log, the kv engine is abstracted. Based on this kv engine, it is one of the storage drivers of IceFireDB. Other projects can rely on this kv engine to implement their own writers and readers, but we have not designed a proprietary IPLD data structure at present.\nIceFireDB abstracts a data coding layer on the kv engine, which can support more complex data structures such as hash\\list besides the basic KV operation. Currently, ipfs kv storage has been realized in the data storage directory of Icefire DB, and we hope that the data of the database can be fully grown on ipfs, but at present, we need to solve the need of kv key synchronization first. The expression of nosql and sql data relationship for ipfs kv is also conceiving the design of ipld, but it is not in the current major construction milestone. ipld is what we have been learning recently,If more complicated IPLD design is involved later, we will practice it in the process of learning.\n2. How does the DB run? # In this milestone, our implementation mode is the server running mode, which needs to run in a server somewhere. Our server will support Redis-RESP protocol and MYSQL communication protocol, and clients can use Golang, JS, PHP and other computer languages to connect.\nThere is also a framework way, which allows users to directly embed into the application code. This integration method is not in the current milestone, and we plan to concentrate on realizing it after this milestone.For example:redhub,IceFireDB-crdt-kv,ipfs-nosql-frame.These projects are open sourced by IceFireDB and are licensed under the Apache-2.0 open source license, and are mainly used to build ipfs-nosql-frame project, so that other Golang applications can be integrated more conveniently.\nCompared with the framework mode, the server operation mode has the following advantages:\nProvide standard data protocols (RESP, MYSQL), which can make the application minimize changes and use decentralized database. Mask the complexity under IPFS, including libp2p, ipfs-log and crdt. 3. How will you address mutability of data? # We know the immutability of IPFS itself, and now IceFireDB has two implementation models to solve the variability of data:\nThe first implementation: instruction broadcast model based on ipfs-log: Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nThe second implementation: full storage model based on ipfs: In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on ipfs.\n4. What program languages are you targeting? # Our program implementation is Golang.\nWe provide standard Redis-RESP and MYSQL communication protocols, so Redis and MYSQL clients of other computer programming languages can communicate with IceFireDB (JS, Rust)\n"},{"id":42,"href":"/icefiredb_docs/icefiredb/news/","title":"News","section":"Overview","content":" Record the bits and pieces of IceFireDB. # "}]