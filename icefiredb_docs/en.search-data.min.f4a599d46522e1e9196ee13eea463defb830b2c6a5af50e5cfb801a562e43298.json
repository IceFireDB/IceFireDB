[{"id":0,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/overview/","title":"Architecture Overview","section":"Architecture","content":" Architecture Overview # System Architecture # IceFireDB-PubSub is designed as a decentralized publish-subscribe system that maintains compatibility with the Redis PubSub protocol while leveraging P2P networking for decentralized message distribution. The architecture consists of several key components:\nCore Components # Redis Protocol Handler\nListens on configured port (default: 16379) Parses Redis protocol (RESP) requests Handles PubSub-specific commands (SUBSCRIBE, PUBLISH, etc.) Manages client connections and subscription state Subscription Manager\nTracks channel subscriptions across all connected clients Manages pattern-based subscriptions (PSUBSCRIBE) Handles subscription/unsubscription operations Maintains subscription persistence across connection failures Message Router\nRoutes messages from publishers to appropriate subscribers Handles pattern matching for pattern-based subscriptions Implements message delivery guarantees and retry mechanisms Manages message buffering and flow control P2P Networking Layer\nlibp2p-based decentralized networking Peer discovery using Kademlia DHT Topic-based messaging using pubsub protocols NAT traversal and relay support Connection management and fault tolerance Message Distribution Engine\nDistributes messages across the P2P network Handles message replication and delivery guarantees Implements cross-node subscription synchronization Manages message ordering and consistency Architectural Diagram # +----------------+ +-----------------+ +----------------+ | Redis Client | --\u0026gt; | Redis Protocol | --\u0026gt; | Subscription | | (Publisher) | | Handler | | Manager | +----------------+ +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | Message Router | \u0026lt;-\u0026gt; | Message | | | | Distribution | +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | P2P Networking | \u0026lt;-\u0026gt; | Other IceFireDB| | Layer | | PubSub Nodes | +-----------------+ +----------------+ | v +----------------+ | Redis Client | | (Subscriber) | +----------------+ Data Flow # Client Connection: Redis client connects and issues PubSub commands Protocol Processing: Redis protocol handler parses and validates requests Subscription Management: Subscription manager tracks client subscriptions Message Publishing: Messages are received from publishers and routed to subscribers Network Distribution: Messages are distributed across the P2P network to other nodes Message Delivery: Messages are delivered to all subscribed clients across all nodes Networking Model # IceFireDB-PubSub uses a decentralized P2P networking model with:\nDistributed Hash Table (DHT): Kademlia DHT for peer discovery and routing GossipSub Protocol: Efficient pubsub messaging with mesh networking NAT Traversal: ICE, STUN, and TURN support for NAT penetration Relay Networks: Relay support for nodes that cannot establish direct connections Connection Bootstrapping: Bootstrap nodes for initial network connection Message Delivery Semantics # The system implements at-least-once delivery semantics with:\nMessage Persistence: Messages are persisted until delivered to all subscribers Retry Mechanisms: Automatic retry of failed message deliveries Delivery Guarantees: Best-effort delivery to all subscribed nodes Ordering: Best-effort message ordering within individual channels Performance Characteristics # Low Latency: Optimized message routing and delivery paths High Throughput: Efficient message processing and network utilization Scalability: Linear scaling with additional nodes and network capacity Resource Efficiency: Minimal overhead for message processing and routing Reliability: Automatic failover and message recovery mechanisms Deployment Modes # Single Node Deployment # Single instance for development and testing Full protocol compatibility with Redis clients Local message routing without P2P networking Distributed Deployment # Multiple nodes across different networks Full P2P messaging capabilities Global message distribution with local latency Automatic peer discovery and connection management Security Features # Transport Encryption: TLS support for client connections Message Encryption: Optional end-to-end message encryption Access Control: Channel-based authentication and authorization Peer Authentication: Secure peer identity verification Network Security: Encrypted P2P communications and DHT operations Audit Logging: Comprehensive message and access logging Monitoring and Management # Connection Statistics: Active client connections and subscription counts Message Metrics: Message throughput, latency, and delivery success rates Network Status: Peer connectivity, network size, and health status Resource Usage: Memory, CPU, and network bandwidth utilization Message Tracing: End-to-end message tracing and delivery verification Alerting: Configurable alerts for system events and performance issues Advanced Features # Pattern-based Subscriptions # Support for Redis PSUBSCRIBE/PUNSUBSCRIBE commands Efficient pattern matching across distributed nodes Dynamic subscription management based on message patterns Message Persistence # Configurable message persistence settings Message replay capabilities for new subscribers Storage management and message expiration policies Quality of Service # Configurable delivery guarantees (at-most-once, at-least-once) Message priority and preferential treatment Rate limiting and traffic shaping capabilities Cross-protocol Support # Potential integration with other messaging protocols Protocol translation and interoperability features Bridge functionality to other messaging systems "},{"id":1,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":2,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase OrbitDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support KV、PubSub KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Software library integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Javascript Go Ecological client language Javascript Any client that supports the redis、mysql protocol Thanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":3,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/overview/","title":"Overview","section":"Designs","content":" IceFireDB NoSQL Engine Overview # Introduction # IceFireDB is a decentralized database infrastructure that combines cutting-edge research in distributed systems, concurrent databases, and peer-to-peer networking. The NoSQL engine provides a high-performance, decentralized key-value store with support for multiple storage backends and distributed consensus mechanisms.\nCore Architecture # System Components # Component Description Technologies Network Layer Manages node communication and data synchronization RAFT, P2P, NATS Storage Layer Pluggable storage backends with unified interface LevelDB, BadgerDB, IPFS, CRDT, OSS Protocol Layer Client communication protocols RESP (Redis), MySQL, GraphQL (planned) Codec Layer Data encoding/decoding and operation abstraction KV, Strings, Hashes, Lists, Sets, Sorted Sets Storage Drivers # IceFireDB supports multiple storage backends through a unified driver interface:\nLevelDB/BadgerDB: High-performance local disk storage IPFS: Decentralized content-addressable storage CRDT: Conflict-free replicated data types for eventual consistency IPFS-LOG: Append-only log-based storage with IPFS OSS: Object storage service integration HybridDB: Tiered hot/cold storage management Key Features # Decentralization Capabilities # P2P Automatic Networking: Nodes automatically discover and connect to each other CRDT-based Replication: Conflict-free data synchronization across nodes IPFS Integration: Leverages decentralized storage for data persistence RAFT Consensus: Strong consistency within availability zones Performance Characteristics # High Throughput: Optimized for read/write operations with minimal latency Scalable Architecture: Horizontal scaling through node addition Flexible Storage: Multiple backend options for different use cases Efficient Networking: Optimized protocol handling and data transfer Protocol Support # Redis RESP Protocol: Full compatibility with Redis clients and commands Custom Extensions: Enhanced commands for decentralized operations Future Protocols: GraphQL and additional protocol support planned Use Cases # Web2 Applications # Traditional distributed databases with RAFT consensus High-performance key-value stores with disk persistence Redis-compatible caching and session storage Web3 Applications # Decentralized applications requiring data immutability P2P data synchronization across distributed nodes IPFS-based storage for content-addressable data Blockchain integration and transparent logging Getting Started # For quick start guides and deployment instructions, see the Quick Start section.\nPerformance Benchmarks # LevelDB Driver Performance # # Benchmark results with 512 concurrent connections SET: 253,232 requests per second GET: 2,130,875 requests per second Storage Backend Comparison # Backend Read Latency Write Latency Persistence Decentralized LevelDB Low Low Disk No BadgerDB Very Low Low Disk No IPFS Medium Medium Network Yes CRDT Low Medium Eventually Consistent Yes Architecture Diagrams # Overall system architecture showing components and data flow\nBridge architecture connecting web2 and web3 applications\nDevelopment Status # ✅ Production Ready: LevelDB, BadgerDB drivers 🟡 Beta: IPFS, CRDT, IPFS-LOG drivers 🚧 In Development: AI vector database, NATS integration 📋 Planned: GraphQL protocol, advanced caching, witness layer Community and Support # IceFireDB is developed with support from:\nProtocol Labs Filecoin Foundation FVM (Filecoin Virtual Machine) libp2p community For issues, contributions, and discussions, please visit our GitHub repository.\n"},{"id":4,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/overview/","title":"Overview","section":"Architecture","content":" Architecture Overview # System Architecture # IceFireDB-Redis-Proxy is designed as a decentralized proxy layer that enhances traditional Redis deployments with P2P networking capabilities. The architecture consists of several key components:\nCore Components # Redis Protocol Handler\nListens on configured port (default: 16379) Parses Redis protocol (RESP) requests Handles authentication and connection management Routes commands to appropriate backend Redis servers Backend Redis Connector\nManages connections to backend Redis servers (standalone or cluster) Handles connection pooling and failover Supports both single-node and cluster Redis deployments Implements read-write separation and load balancing P2P Networking Layer\nlibp2p-based decentralized networking Peer discovery and automatic node connection Data synchronization and change propagation Topic-based messaging for efficient communication Synchronization Engine\nChange detection from backend Redis operations Conflict detection and resolution strategies Eventual consistency maintenance Command replication across the network Traffic Management\nMulti-tenant data isolation Read-write separation configuration Command filtering and access control Rate limiting and resource management Architectural Diagram # +----------------+ +-----------------+ +----------------+ | Redis Client | --\u0026gt; | Redis Protocol | --\u0026gt; | Backend Redis | | (Application) | | Handler | | Server/Cluster | +----------------+ +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | P2P Networking | \u0026lt;-\u0026gt; | Synchronization| | Layer | | Engine | +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | Traffic | | Other IceFireDB| | Management | | Redis-Proxies | +-----------------+ +----------------+ Data Flow # Client Request: Redis client connects and sends commands via RESP protocol Protocol Processing: Redis protocol handler parses and validates requests Backend Execution: Commands routed to appropriate backend Redis server Change Detection: Synchronization engine detects data modifications Network Propagation: Changes propagated to other proxy nodes via P2P network Consistency Maintenance: All proxy nodes eventually reach consistent state with their backends Networking Model # IceFireDB-Redis-Proxy uses a decentralized P2P networking model with:\nService Discovery: Automatic peer discovery using DHT (Distributed Hash Table) Topic-based Messaging: PubSub system for efficient data propagation NAT Traversal: Support for nodes behind firewalls and NAT devices Connection Management: Automatic reconnection and fault tolerance Mesh Networking: Full mesh connectivity for reliable message delivery Consistency Model # The system implements eventual consistency with:\nOperation-based Replication: Redis commands are replicated across nodes Conflict Resolution: Timestamp-based resolution for conflicting operations Causal Ordering: Preservation of operation causality where possible Background Synchronization: Continuous state synchronization between proxies Performance Characteristics # Low Latency: Direct connections to backend Redis servers High Throughput: Efficient command routing and parallel processing Scalability: Horizontal scaling through additional proxy nodes Resource Efficiency: Lightweight proxy layer with connection pooling Failover Support: Automatic failover to healthy backend instances Deployment Modes # Standalone Mode # Single backend Redis server Simple deployment for development and testing Full P2P synchronization capabilities Cluster Mode # Redis cluster backend support Automatic sharding and data distribution Cluster-aware command routing Failover and replica promotion support Security Features # Transport Encryption: TLS support for client and backend connections Authentication: Redis authentication with configurable credentials Access Control: Multi-tenant isolation and command filtering Network Security: Secure peer authentication and encrypted P2P communications Audit Logging: Command logging and access monitoring Monitoring and Management # Health Checks: Regular backend Redis health monitoring Metrics Collection: Performance metrics and statistics Logging: Comprehensive logging for debugging and audit purposes Configuration Management: Dynamic configuration updates "},{"id":5,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/overview/","title":"Overview","section":"Architecture","content":" Architecture Overview # System Architecture # IceFireDB-SQLite is designed as a decentralized SQL database system that combines traditional SQLite storage with peer-to-peer networking capabilities. The architecture consists of several key components:\nCore Components # MySQL Protocol Handler\nListens on configured port (default: 23306) Parses MySQL wire protocol requests Handles authentication and connection management Translates SQL commands to SQLite operations SQLite Storage Engine\nLocal SQLite database file storage Transaction management and query execution Data persistence and integrity maintenance P2P Networking Layer\nlibp2p-based decentralized networking Peer discovery and automatic node connection Data synchronization and conflict resolution Gossip protocol for state propagation Synchronization Engine\nChange detection and propagation Conflict detection and resolution strategies Eventual consistency maintenance Transaction ordering and sequencing Architectural Diagram # +----------------+ +-----------------+ +----------------+ | MySQL Client | --\u0026gt; | MySQL Protocol | --\u0026gt; | SQLite Storage | | (Application) | | Handler | | Engine | +----------------+ +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | P2P Networking | \u0026lt;-\u0026gt; | Synchronization| | Layer | | Engine | +-----------------+ +----------------+ | v +-----------------+ | Other IceFireDB | | SQLite Nodes | +-----------------+ Data Flow # Client Request: MySQL client connects and sends SQL commands Protocol Processing: MySQL protocol handler parses and validates requests Local Execution: SQL commands executed on local SQLite database Change Detection: Synchronization engine detects data changes Network Propagation: Changes propagated to other nodes via P2P network Consistency Maintenance: All nodes eventually reach consistent state Networking Model # IceFireDB-SQLite uses a decentralized P2P networking model with:\nService Discovery: Automatic peer discovery using DHT (Distributed Hash Table) Topic-based Messaging: PubSub system for efficient data propagation NAT Traversal: Support for nodes behind firewalls and NAT devices Connection Management: Automatic reconnection and fault tolerance Consistency Model # The system implements eventual consistency with:\nLast-Write-Wins: Conflict resolution based on timestamps Causal Ordering: Preservation of operation causality Background Synchronization: Continuous data synchronization Conflict Detection: Automatic detection of conflicting changes Performance Characteristics # Low Latency: Local SQLite operations for fast response times High Throughput: Parallel processing of requests and synchronization Scalability: Horizontal scaling through additional nodes Resource Efficiency: Lightweight SQLite storage with minimal overhead Security Features # Transport Encryption: TLS encryption for network communications Authentication: MySQL protocol authentication with configurable users Access Control: Tenant-based data isolation Network Security: Secure peer authentication and authorization "},{"id":6,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/overview/","title":"Overview","section":"Architecture","content":" Architecture Overview # System Architecture # IceFireDB-SQLProxy is designed as a decentralized proxy layer that enhances traditional MySQL deployments with P2P networking capabilities. The architecture consists of several key components that work together to provide decentralized SQL database functionality.\nCore Components # MySQL Protocol Handler\nListens on configured port (default: 33306) Parses MySQL wire protocol requests Handles authentication and connection management Manages client connections and session state Backend MySQL Connector\nManages connections to backend MySQL servers Supports connection pooling for efficiency Handles both admin and read-only database connections Implements failover and load balancing Query Router\nRoutes SQL queries to appropriate backend databases Supports read-write separation Handles transaction management Provides query caching and optimization P2P Networking Layer\nlibp2p-based decentralized networking Peer discovery and automatic node connection Data synchronization and change propagation Topic-based messaging for efficient communication Synchronization Engine\nChange detection from database operations SQL command replication across nodes Conflict detection and resolution strategies Eventual consistency maintenance Connection Pool Manager\nEfficient management of database connections Connection reuse and lifecycle management Health checking and connection validation Resource allocation and limits enforcement Architectural Diagram # +----------------+ +-----------------+ +----------------+ | MySQL Client | --\u0026gt; | MySQL Protocol | --\u0026gt; | Backend MySQL | | (Application) | | Handler | | Server | +----------------+ +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | Query Router | \u0026lt;-\u0026gt; | Connection | | | | Pool Manager | +-----------------+ +----------------+ | | v v +-----------------+ +----------------+ | P2P Networking | \u0026lt;-\u0026gt; | Synchronization| | Layer | | Engine | +-----------------+ +----------------+ | v +-----------------+ | Other IceFireDB | | SQLProxy Nodes | +-----------------+ Data Flow # Client Request: MySQL client connects and sends SQL queries Protocol Processing: MySQL protocol handler parses and validates requests Query Routing: Query router determines appropriate backend and connection type Backend Execution: SQL commands executed on backend MySQL server Change Detection: Synchronization engine detects data modifications Network Propagation: Changes propagated to other proxy nodes via P2P network Consistency Maintenance: All proxy nodes eventually reach consistent state Networking Model # IceFireDB-SQLProxy uses a decentralized P2P networking model with:\nService Discovery: Automatic peer discovery using DHT (Distributed Hash Table) Topic-based Messaging: PubSub system for efficient data propagation NAT Traversal: Support for nodes behind firewalls and NAT devices Connection Management: Automatic reconnection and fault tolerance Mesh Networking: Full mesh connectivity for reliable message delivery Consistency Model # The system implements eventual consistency with:\nOperation-based Replication: SQL commands are replicated across nodes Conflict Resolution: Timestamp-based resolution for conflicting operations Causal Ordering: Preservation of operation causality where possible Background Synchronization: Continuous state synchronization between proxies Transaction Awareness: Basic transaction support with conflict detection Performance Characteristics # Low Latency: Optimized protocol handling and connection pooling High Throughput: Efficient query routing and parallel processing Scalability: Horizontal scaling through additional proxy nodes Resource Efficiency: Connection pooling reduces backend database load Reliability: Automatic retry and failover mechanisms Deployment Modes # Single Backend Mode # Single backend MySQL server Simple deployment for development and testing Full P2P synchronization capabilities Multi-Backend Mode # Multiple backend MySQL servers Read-write separation support Load balancing across backend instances Failover and replica promotion support Security Features # Transport Encryption: TLS support for client and backend connections Database Authentication: MySQL authentication with configurable credentials Access Control: Multi-level access control (admin vs read-only) Network Security: Secure peer authentication and encrypted P2P communications Audit Logging: Query logging and access monitoring SQL Injection Protection: Basic query validation and sanitization Monitoring and Management # Health Checks: Regular backend MySQL health monitoring Performance Metrics: Query latency, throughput, and error rates Connection Pool Statistics: Active, idle, and total connections P2P Network Status: Peer connectivity and synchronization state Query Logging: Comprehensive query logging for debugging and audit Resource Usage: Memory, CPU, and network usage monitoring Advanced Features # Connection Pooling # Configurable minimum and maximum connections Connection reuse and lifecycle management Health checking and connection validation Timeout and idle connection management Read-Write Separation # Different connection pools for admin and read-only operations Automatic routing of read vs write queries Configurable routing rules and policies Support for multiple backend database instances Multi-Tenant Support # User-based connection isolation Database-level access control Resource limits per user or tenant Audit logging per user session "},{"id":7,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/getting-started/","title":"Getting Started Tutorial","section":"Tutorials","content":" Getting Started with IceFireDB # Introduction # This tutorial will guide you through setting up and using IceFireDB for the first time. We\u0026rsquo;ll cover installation, basic operations, and some advanced features.\nPrerequisites # Go 1.18+ installed Basic understanding of Redis commands Terminal/shell access Installation # Method 1: From Source # # Clone the repository git clone https://github.com/IceFireDB/IceFireDB.git cd IceFireDB # Build the project make build # The binary will be in the bin/ directory ls bin/ Method 2: Using Docker # # Pull the Docker image docker pull icefiredb/icefiredb:latest # Run the container docker run -d -p 11001:11001 --name icefiredb icefiredb/icefiredb:latest Method 3: Pre-built Binaries # Download the latest release from the GitHub releases page.\nBasic Setup # 1. Start IceFireDB with Default Settings # # Start with default settings (LevelDB backend, port 11001) ./bin/icefiredb-nosql # Or specify basic options ./bin/icefiredb-nosql -n 11001 -a 0.0.0.0 -j ./data You should see output like:\n[INFO] Starting IceFireDB on :11001 [INFO] Storage driver: leveldb [INFO] Data directory: ./data Basic Redis Operations # Connecting with redis-cli # # Connect to IceFireDB redis-cli -p 11001 # Or if using Docker redis-cli -h localhost -p 11001 Basic Key-Value Operations # # Set and get values 127.0.0.1:11001\u0026gt; SET mykey \u0026#34;Hello IceFireDB\u0026#34; OK 127.0.0.1:11001\u0026gt; GET mykey \u0026#34;Hello IceFireDB\u0026#34; # Work with numbers 127.0.0.1:11001\u0026gt; SET counter 100 OK 127.0.0.1:11001\u0026gt; INCR counter (integer) 101 127.0.0.1:11001\u0026gt; INCRBY counter 50 (integer) 151 # Set with expiration 127.0.0.1:11001\u0026gt; SETEX session:user123 3600 \u0026#34;active\u0026#34; OK 127.0.0.1:11001\u0026gt; TTL session:user123 (integer) 3600 Hash Operations # # Store user data as hash 127.0.0.1:11001\u0026gt; HSET user:1000 name \u0026#34;Alice\u0026#34; email \u0026#34;alice@example.com\u0026#34; age 30 (integer) 3 # Get specific fields 127.0.0.1:11001\u0026gt; HGET user:1000 name \u0026#34;Alice\u0026#34; 127.0.0.1:11001\u0026gt; HGET user:1000 email \u0026#34;alice@example.com\u0026#34; # Get all fields 127.0.0.1:11001\u0026gt; HGETALL user:1000 1) \u0026#34;name\u0026#34; 2) \u0026#34;Alice\u0026#34; 3) \u0026#34;email\u0026#34; 4) \u0026#34;alice@example.com\u0026#34; 5) \u0026#34;age\u0026#34; 6) \u0026#34;30\u0026#34; List Operations # # Push items to list 127.0.0.1:11001\u0026gt; LPUSH tasks \u0026#34;task1\u0026#34; (integer) 1 127.0.0.1:11001\u0026gt; LPUSH tasks \u0026#34;task2\u0026#34; (integer) 2 127.0.0.1:11001\u0026gt; RPUSH tasks \u0026#34;task3\u0026#34; (integer) 3 # Get list range 127.0.0.1:11001\u0026gt; LRANGE tasks 0 -1 1) \u0026#34;task2\u0026#34; 2) \u0026#34;task1\u0026#34; 3) \u0026#34;task3\u0026#34; # Pop items 127.0.0.1:11001\u0026gt; LPOP tasks \u0026#34;task2\u0026#34; 127.0.0.1:11001\u0026gt; RPOP tasks \u0026#34;task3\u0026#34; Using Different Storage Drivers # Switching Storage Backends # # Check current driver 127.0.0.1:11001\u0026gt; DRIVER.INFO 1) \u0026#34;leveldb\u0026#34; 2) \u0026#34;./data\u0026#34; # Switch to BadgerDB 127.0.0.1:11001\u0026gt; DRIVER.SELECT badger OK # Switch to IPFS (decentralized) 127.0.0.1:11001\u0026gt; DRIVER.SELECT ipfs OK Using Different Storage Drivers # Start IceFireDB with different storage backends:\n# Use LevelDB (default) ./bin/icefiredb-nosql -storage-backend leveldb -j ./leveldb_data # Use BadgerDB for better write performance ./bin/icefiredb-nosql -storage-backend badger -j ./badger_data # Use IPFS for decentralized storage ./bin/icefiredb-nosql -storage-backend ipfs -j ./ipfs_data --p2p-enable P2P Networking # Setting Up a Decentralized Cluster # First Node ./bin/icefiredb-nosql \\ -storage-backend ipfs \\ -n 11001 \\ -j ./node1_data \\ --p2p-enable \\ --discovery-id \u0026#34;my_cluster\u0026#34; \\ --p2p-listen \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; Second Node ./bin/icefiredb-nosql \\ -storage-backend ipfs \\ -n 11002 \\ -j ./node2_data \\ --p2p-enable \\ --discovery-id \u0026#34;my_cluster\u0026#34; \\ --p2p-listen \u0026#34;/ip4/0.0.0.0/tcp/4002\u0026#34; \\ --bootstrap-peers \u0026#34;/ip4/192.168.1.100/tcp/4001/p2p/QmFirstNodePeerID\u0026#34; Verify Connection # On either node 127.0.0.1:11001\u0026gt; P2P.PEERS 1) \u0026#34;/ip4/192.168.1.101/tcp/4002/p2p/QmSecondNodePeerID\u0026#34; # Data will automatically sync between nodes 127.0.0.1:11001\u0026gt; SET shared_key \u0026#34;cluster_value\u0026#34; OK 127.0.0.1:11002\u0026gt; GET shared_key \u0026#34;cluster_value\u0026#34; Advanced Features # CRDT-based Data Sync # # Enable CRDT mode 127.0.0.1:11001\u0026gt; DRIVER.SELECT crdt OK # Data written in CRDT mode will automatically resolve conflicts 127.0.0.1:11001\u0026gt; SET user:preferences \u0026#39;{\u0026#34;theme\u0026#34;: \u0026#34;dark\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;en\u0026#34;}\u0026#39; OK # Force synchronization 127.0.0.1:11001\u0026gt; CRDT.SYNC OK # Check sync status 127.0.0.1:11001\u0026gt; CRDT.STATUS 1) \u0026#34;synced\u0026#34; 2) \u0026#34;2 nodes\u0026#34; 3) \u0026#34;0 conflicts\u0026#34; Hybrid Storage Mode # # Start with hybrid storage ./bin/icefiredb-nosql \\ -storage-backend hybriddb \\ --hot-storage badger \\ --cold-storage ipfs \\ --hot-data-dir \u0026#34;./data/hot\u0026#34; \\ --cold-data-dir \u0026#34;./data/cold\u0026#34; \\ --migration-threshold 604800 # 7 days in seconds # Data automatically moves between hot and cold storage 127.0.0.1:11001\u0026gt; SET recent_data \u0026#34;frequently accessed\u0026#34; OK 127.0.0.1:11001\u0026gt; SET old_data \u0026#34;rarely accessed\u0026#34; OK # After threshold, old_data moves to cold storage Programming Examples # Python Example # import redis # Connect to IceFireDB r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Basic operations r.set(\u0026#39;python_key\u0026#39;, \u0026#39;Hello from Python!\u0026#39;) print(r.get(\u0026#39;python_key\u0026#39;)) # Hash operations r.hset(\u0026#39;user:python\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;Python User\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;python\u0026#39;) print(r.hgetall(\u0026#39;user:python\u0026#39;)) # Use IceFireDB extensions r.execute_command(\u0026#39;DRIVER.SELECT\u0026#39;, \u0026#39;ipfs\u0026#39;) Node.js Example # const redis = require(\u0026#39;redis\u0026#39;); const client = redis.createClient({ host: \u0026#39;localhost\u0026#39;, port: 11001 }); client.on(\u0026#39;connect\u0026#39;, () =\u0026gt; { console.log(\u0026#39;Connected to IceFireDB\u0026#39;); // Basic operations client.set(\u0026#39;node_key\u0026#39;, \u0026#39;Hello from Node.js!\u0026#39;, redis.print); client.get(\u0026#39;node_key\u0026#39;, (err, reply) =\u0026gt; { console.log(\u0026#39;Value:\u0026#39;, reply); }); // IceFireDB specific command client.send_command(\u0026#39;DRIVER.INFO\u0026#39;, (err, reply) =\u0026gt; { console.log(\u0026#39;Driver info:\u0026#39;, reply); }); }); Go Example # package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) func main() { ctx := context.Background() // Connect to IceFireDB client := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:11001\u0026#34;, Password: \u0026#34;\u0026#34;, // no password set DB: 0, // use default DB }) // Basic operations err := client.Set(ctx, \u0026#34;go_key\u0026#34;, \u0026#34;Hello from Go!\u0026#34;, 0).Err() if err != nil { panic(err) } val, err := client.Get(ctx, \u0026#34;go_key\u0026#34;).Result() if err != nil { panic(err) } fmt.Println(\u0026#34;go_key:\u0026#34;, val) // IceFireDB extension result, err := client.Do(ctx, \u0026#34;DRIVER.INFO\u0026#34;).Result() fmt.Println(\u0026#34;Driver info:\u0026#34;, result) } Monitoring and Maintenance # Checking Server Info # # Get comprehensive server information 127.0.0.1:11001\u0026gt; INFO # Server redis_version:IceFireDB redis_mode:standalone os:Linux # Memory used_memory:1048576 used_memory_human:1.00M # Stats total_connections_received:100 total_commands_processed:500 # Persistence rdb_last_save_time:1633046400 # Replication role:master connected_slaves:0 # CPU used_cpu_sys:10.50 used_cpu_user:15.25 Performance Monitoring # # Use redis-benchmark for performance testing redis-benchmark -h localhost -p 11001 -t set,get -c 50 -n 100000 # Example output: # SET: 253232.12 requests per second # GET: 2130875.50 requests per second Troubleshooting Common Issues # Connection Issues # # Check if IceFireDB is running netstat -tlnp | grep 11001 # Check logs tail -f icefiredb.log Storage Issues # # Check disk space df -h # Check data directory ls -la ./data/ Performance Issues # # Monitor memory usage 127.0.0.1:11001\u0026gt; INFO memory # Check slow logs 127.0.0.1:11001\u0026gt; SLOWLOG GET 10 Next Steps # Explore Advanced Features: Try CRDT mode, IPFS storage, and P2P clustering Read Architecture Docs: Understand how IceFireDB works internally Check Configuration Options: Optimize for your specific use case Join the Community: Contribute and get support on GitHub Additional Resources # API Reference - Complete command documentation Configuration Guide - Detailed configuration options Architecture Overview - System design and components GitHub Repository - Source code and issues Support # If you encounter issues:\nCheck the GitHub issues Join the Discussions Review the documentation Happy building with IceFireDB! 🚀\n"},{"id":8,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-value：memory-kv leveldb kv ：leveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":9,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":10,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":11,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":12,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":13,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/network/","title":"Network layer","section":"Designs","content":" Network layer design # The network layer undertakes the work of inter-node networking, inter-node data distribution, and inter-node data consistency consensus. The network layer of IceFireDB is divided into two layers according to the distance of the physical network link:\nData consistency network layer for short-distance networks. Decentralized database network layer for wide-distance network. The above two different network layers are supported by different technologies and have different requirements for data consistency sensitivity and timeliness.\nParallel cluster network # The smallest component unit of the IceFireDB network cluster is a parallel cluster network guaranteed by the RAFT algorithm, rather than a decentralized P2P network.\nThe IceFireDB parallel cluster uses the RAFT algorithm to form a data consistency layer. Each cluster has a master and multiple slave nodes. They synchronize with each other and maintain the master-slave state of the cluster. Each node stores the same data and writes data. The input is undertaken by the master node, and the master node is responsible for distributing data to the slave nodes. A RAFT cluster internally guarantees the consistency of each data write.\nDecentralized network # The data jumps out of the parallel cluster network and enters the decentralized network. The IceFireDB decentralized network mainly uses P2P technology for automatic networking, and relies on the P2P PubSub communication method to synchronize data commands.\nIceFireDB\u0026rsquo;s use of P2P networking is not only in node discovery, but also provides users with decentralized publish and subscribe middleware by bridging the PubSub network to the RESP protocol. Increase the decentralization capability of the SQLite database by bridging the PubSub network to the SQL protocol.\nDecentralized log synchronization network # Relying solely on P2P and PubSub technologies cannot meet the database requirements for data decentralized synchronization and decentralized data consistency.The current popular CRDT technology can meet the final data consistency, but it cannot guarantee the synchronization order of the data, so it will castrate some data instruction functions of IceFireDB-NoSQL, and the IPFS-LOG technology very well makes up for the functional gap of the decentralized log.\nIceFireDB-NoSQL has established a decentralized log synchronization network based on IPFS-LOG technology. The decentralized IceFireDB nodes broadcast database command logs and build orderly logs to complete the function of data decentralization and consistency.\nIceGiant Network structure # IceFireDB\u0026rsquo;s RAFT has exactly the same data set within the same group of nodes, and in a larger network, we are providing the IceGiant network structure, which aims to break up different tenant database data.\nAlthough all IceGiant nodes are in the same P2P network, the structure of the network can be decomposed according to the data set, which is the data tenant isolation area, which refers to the database area used by a specific application on top of the IceGiant protocol. Different from the traditional blockchain network, IceGiant nodes are only responsible for interacting with peers operating the same data set, and are not responsible for any data of other data sets. Peer IceGiant node sets form a high-availability data storage area.\nIn-process network IO model # IceFireDB supports the following two IO models:\nGolang classic netpoll model: goroutine-per-connection, suitable for situations where the number of connections is not a bottleneck.\nRawEpoll model: that is, Reactor mode, I/O multiplexing (I/O multiplexing) + non-blocking I/O (non-blocking I/O) mode. For scenarios where there are a large number of long links between the access layer and the gateway, it is more suitable for the RawEpoll model.\n"},{"id":14,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support SQL KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Binary software integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis、mysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":15,"href":"/icefiredb_docs/icefiredb/engine_overview/","title":"Engine Overview","section":"Overview","content":" IceFireDB - Decentralized database engine # Engine Description # IPFS and Filecoin are excellent decentralized data storage infrastructures, which have revolutionary significance for the construction of web3. However, with the development of the application ecology, archived data such as pictures and videos can be stored in IPFS or FileCoin, but there is still a lack of database-level storage expression. Although the community also has a decentralized storage solution similar to the KV model, it is only the KV model. It cannot meet the use of upper-layer applications. We believe that more and more complex data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as hash, list, set data structures) and SQL data models. Just like the prosperity of web2\u0026rsquo;s application ecology is inseparable from the contribution of database infrastructures such as memcached, redis, and mysql, the IPFS ecosystem also needs database infrastructure with NoSQL and SQL models.\nApart from storage, IPFS has many excellent components in the ecology, such as libp2p, crdt, ipfs-log and ipld. These components will greatly support the decentralization of data and the ecology of web2. However, since many databases under web2 are mysql and redis at present, if you want to help the applications using these databases achieve decentralization, you need to design more database middleware to facilitate application docking and connect the preceding and the following.\nIn addition to the huge demand for decentralized data storage in the web3 ecosystem, in the current digital age, decentralized networking, decentralized subscriptions, and decentralized storage are used in scenarios such as edge computing, big data, and cloud native. There are more and more strong demands. We combine the demands of web2 and the current situation of web3 to propose the IceFireDB Storage Stack project.\nIceFireDB Storage Stack is committed to creating a complete database storage and database middleware software system for the data decentralization ecosystem. The project currently mainly includes three directions of development: decentralized NoSQL database, decentralized SQLite database, decentralized communication Components and database decentralized middleware (ecological application communication middleware, database decentralized middleware).\nLet\u0026rsquo;s elaborate on the three main construction directions of IceFireDB Storage Stack:\nDecentralized NoSQL database (IceFireDB-NoSQL): Using IPFS as the underlying KV engine, using KV coding technology to achieve more complex data structures, such as hash, set, list, etc., integrate the standard Redis network protocol, allowing applications The IceFIreDB-IPFS-NoSQL database can be used by using the Redis client; the network networking is performed by using libp2p, and the decentralized NoSQL database is constructed by crdt, ipfs-log, and ipld. Decentralized SQLite database (IceFireDB-SQLite): The SQL protocol is currently widely used in the web2 application layer. We design and implement a decentralized SQLite database. The bottom layer uses IPFS-libp2p to build a decentralized network and IPFS-pubsub and peer-to-peer Wait for nodes to synchronize data, and use IPFS CRDT, ipfs-log, and ipld to ensure decentralized consistency of SQL statements. Decentralized database middleware (IceFireDB-PubSub, IceFireDB-Redis-Proxy, IceFireDB-SQLProxy): While we are paying attention to the application development of web3, we also see that IPFS provides data decentralization for applications around the world. Very good libp2p, crdt and other components, we should provide decentralized capabilities for traditional web2 databases and traditional web2 applications, but most web2 applications currently use redis, mysql databases, IceFireDB combined with libp2p, crdt, ipld, ipfs-log The technology adds the wings of data decentralization to traditional mysql and redis, and provides insensitive data decentralization and application decentralization communication capabilities for massive web2 applications, traditional nosql and SQL databases. Engine value # For the web3 world, there is currently no way to write a full-blown client-side application as easily and completely decentralized as in Web2. In Web2, you spin up a database on AWS and have your client applications call that database for reading and writing. But there is nothing like that in Web3. You can\u0026rsquo;t just write data to Ethereum, it\u0026rsquo;s too expensive for most users. Storage protocols such as Filecoin and Arweave are mainly used for archiving data, but do not provide enterprise-level performance guarantees for writing and reading data. IceFireDB Storage Stack uses IPFS as the underlying KV engine, and uses KV encoding technology to achieve more complex NoSQL data structures, For example, hash, set, list, etc. use libp2p, crdt, ipld, ipfs-log to build decentralized NoSQL and SQL databases, and provide RESP and SQL protocol support for easy application access, so that existing massive applications can be changed Solve the decentralized communication and storage capabilities of IPFS at the lowest cost, and expand the application access speed and application ecology of IPFS and Filecoin.\nFor the traditional application fields of web2, most applications currently use Nosql and SQL databases such as redis and mysql. However, with the development of edge computing, big data, cloud native and other fields, these fields also need the support of decentralized network communication and decentralized database storage. We should provide decentralized nosql database and decentralized SQL database to provide decentralized database storage and use support for these applications. It is also necessary to provide decentralized database middleware to increase the capabilities of data broadcast communication and decentralized data storage for traditional redis and mysql databases. The decentralized database, decentralized networking subscription and decentralized database middleware provided by IceFireDB Storage Stack will help IPFS and Filecoin ecology to support decentralized web2 massive applications.\nDatabase technology is the foundation of application storage and application innovation. The development of decentralized application ecology in any era is inseparable from the support of databases and database middleware. IceFireDB Storage Stack is the wings that add data decentralization to applications. It is believed that IceFireDB Storage Stack will be able to support a large number of new applications.\nIceFireDB Storage Stack has been striving to build decentralized NoSQL\\SQL databases and database middleware with richer functions and easier access to application systems based on the decentralized network and decentralized storage technologies of IPFS and Filecoin, helping massive applications to improve Easy access to network decentralization and data decentralization technologies, helping the decentralized application ecosystem to build storage infrastructure, and helping the prosperity of IPFS, Filecoin and the decentralized application ecosystem.\nEngine composition # IceFireDB-NoSQL # The Redis database based on IPFS technology can break the simple kv situation of the current IPFS database and support complex data structures such as hash and list.\nIceFireDB-SQLite # IceFireDB-SQLite database is a decentralized SQLite database. Provide a convenient mechanism to build a global distributed database system. Support users to write data to IceFireDB-SQLite using MySQL protocol. IceFireDB-SQLite stores the data in the SQLite database and synchronizes the data among the nodes in the P2P automatic network.\nIceFireDB-SQLProxy # IceFireDB-SQLProxy is a decentralized SQL database networking system that helps web2 traditional SQL database data decentralization. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. Commands are automatically synchronized between IceFireDB-SQLProxy in the network, and each IceFireDB-SQLProxy writes data to MySQL storage.\nDecentralized networking through IceFireDB-SQLProxy provides web2 program read and write support for SQL, enabling decentralized data synchronization for MySQL database read and write scenarios commonly used in web2 applications.\nIceFireDB-Redis-Proxy # IceFireDB-Redis-proxy database proxy adds decentralization wings to traditional redis databases. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. The instructions are automatically synchronized between the networked redis agents, and the redis agent writes data to the cluster or single-point redis storage. Through the decentralized middleware network proxy, decentralized data synchronization can be enabled for the Redis database commonly used in web2 applications.\nIceFireDB-PubSub # IceFireDB-PubSub is a high performance, high availability and decentralized subscription system.It can seamlessly migrate web2 applications using redis publish and subscribe into a decentralized p2p subscription network.\n"},{"id":16,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb_proxy/","title":"icefiredb-proxy","section":"Develop","content":" IceFireDB-Proxy # Project introduction # IceFireDB-Proxy is a high-performance, high-availability, and user-friendly Resp protocol cluster proxy solution. It is supporting P2P networking and is a network component in the IceFireDB ecosystem.\nFeatures # Complete data source mode support: stand-alone, cluster mode Rich command support Excellent cluster state management and failover Excellent traffic control policies: Traffic read/write separation and multi-tenant data isolation Excellent command telemetry features Bottom-fishing use of mind and base abilities that are closer to cloud native Supports P2P automatic networking, and Proxy helps traditional Redis databases achieve data decentralization. New framework for faster network, will be upgraded soon. redhub Architecture # Network Communication Model # Installing # 1. Install Go 2. git clone https://github.com/IceFireDB/IceFireDB-Proxy.git $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 3. cd $GOPATH/src/github.com/IceFireDB/IceFireDB-Proxy 4. make Usage # Run a binary file directly, if you need to run in the background can be added to the systemd system management\n./bin/Icefiredb-proxy -c ./config/config.yaml Command support # String # APPEND BITCOUNT BITPOS DECR DECRBY DEL EXISTS GET GETBIT SETBIT GETRANGE GETSET INCR INCRBY MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Set # SADD SCARD SETBIT SISMEMBER SMEMBERS SPOP SRANDMEMBER SREM SSCAN List # LINDEX LINSERT LLEN LPOP LPUSH LPUSHX LRANGE LREM LSET LTRIM RPOP RPUSH RPUSHX Hash # HDEL HEXISTS HGET HGETALL HINCRBY HINCRBYFLOAT HKEYS HLEN HMGET HMSET HSCAN HSET HSETNX HSTRLEN HVALS Sorted Sets # ZADD ZCARD ZCOUNT ZINCRBY ZLEXCOUNT ZPOPMAX ZPOPMIN ZLEXCOUNT ZRANGE ZRANGEBYLEX ZRANGEBYSCORE ZRANK ZREM ZREMRANGEBYLEX ZREMRANGEBYRANK ZREMRANGEBYSCORE ZREVRANGE ZREVRANGEBYLEX ZREVRANGEBYSCORE ZREVRANK ZSCAN ZSCORE Stream # XACK XADD XCLAIM XDEL XLEN XINFO XPENDING XRANGE XREADGROUP XREVRANGE XTRIM XGROUP Others # COMMAND PING QUIT "},{"id":17,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":18,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":19,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":20,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":21,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/storage/","title":"Storage layer","section":"Designs","content":" Storage layer design # The storage layer is responsible for data storage, and the data storage here includes different storage media of web2 and web3. For web2, the storage media we face includes disk, OSS, and for web3, the storage media we face includes IPFS, blockchain, and smart contracts.Currently, the storage types supported by IceFireDB mainly include the following.\nEngine type describe Driver directory LevelDB LevelDB is a fast key-value storage library written at Google that provides an ordered mapping from string keys to string values. Default Badger BadgerDB is an embeddable, persistent and fast key-value (KV) database written in pure Go. Badger OSS Object storage is a technology that stores and manages data in an unstructured format called objects. OSS IPFS IPFS (the InterPlanetary File System) is a hypermedia distribution protocol addressed by content and identities. It enables the creation of completely distributed applications, and in doing so aims to make the web faster, safer, and more open. IPFS CRDT-KV The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS. CRDT-KV IPFS-LOG icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log. IPFS-LOG OrbitDB OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. OrbitDB Storage model # The NoSQL storage layer of each individual IceGiant mainly includes the codec layer and the underlying KV storage layer. the underlying KV engine currently supports levelDB, badgerDB, IPFS and OSS, and the main data storage includes two ways:\ninstruction broadcast model based on IPFS-LOG\\CRDT_KV\\OrbitDB\nNative data storage model based on LevelDB\\Badger\\OSS\\IPFS\nMultiple IceFireDB nodes will be divided into groups according to data sets, and each group will form a highly available storage area structure.\nNoSQL storage engine # The core of each node is the database engine. By default, IceGiant node integrates KV storage engines such as levelDB, badgerDB, IPFS, OSS, etc., and implements the protocol coding layer of NoSQL on the KV storage relationship. Currently, the data storage of NoSQL mainly includes the following two ways:\nInstruction broadcast model # Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Full storage model # In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on LevelDB\\Badger\\OSS\\IPFS.\n+-------------------------------------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ IceGiant Synchronizer # The storage layer of IceFireDB not only includes a complete storage server, but IceGiant Synchronizer, which is currently under construction, also belongs to the ecological software layer of the storage layer.\nIceGiant Synchronizer is an application directly above the database engine. All incoming database requests pass through IceGiant Synchronizer, which determines whether the requests should be processed, whether data writes should be propagated to other parts of the network, and whether local data should be written and customer requests should be responded to.\nIceGiant Synchronizer can also provide data write aggregation function, allowing multiple data requests to be merged and written into a single network storage request. It also allows users to cross-mix data sets between different nodes, encouraging further data decentralization, while keeping the operation overhead low.\n"},{"id":22,"href":"/icefiredb_docs/icefiredb/vocabulary/","title":"Vocabulary","section":"Overview","content":" Vocabulary # The main terms used in IceFireDB products are listed below.\nItem describe NoSQL Non-relational databases, which store data in a format different from relational tables. LSM In computer science, the log-structured merge-tree (also known as LSM tree, or LSMT) is a data structure with performance characteristics that make it attractive for providing indexed access to files with high insert volume, such as transactional log data. OSS Object Storage Service IPFS The InterPlanetary File System (IPFS) is a protocol, hypermedia and file sharing peer-to-peer network for storing and sharing data in a distributed file system. P2P peer-to-peer network EVM Smart contracts A smart contract is a computer program or a transaction protocol that is intended to automatically execute, control or document legally-relevant events and actions according to the terms of a contract or an agreement. CRDT In distributed computing, a conflict-free replicated data type (CRDT) is a data structure that is replicated across multiple computers in a network, RAFT Raft is a consensus algorithm that is designed to be easy to understand. IPLD IPLD stands for InterPlanetary Linked Data,IPLD is an ecosystem of formats and data structures for building applications that can be fully decentralized. IPFS-LOG IPFS-log is an immutable, operation-based collision-free replication data structure (CRDT) for distributed systems. It is an append-only log that can be used to model the variable sharing state between peers in p2p applications. "},{"id":23,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":24,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":25,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":26,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":27,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/protocol/","title":"Protocol layer","section":"Designs","content":" Protocol layer design # A good access method of the application can accelerate the growth of the application ecology, and a good protocol design can reduce the transformation cost of the stock application, so the protocol layer is an important component of the IceFireDB software stack. The communication protocol of IceFireDB-NoSQL fully integrates the Redis RESP protocol, which mainly includes the following two parts of the protocol:\nData control protocol: Complete support for RESP clients, supporting functional requirements for database data access.\nCluster control protocol: Satisfy the client\u0026rsquo;s command protocol for controlling the nodes of the IceFireDB cluster and viewing the status and availability status of the cluster nodes.\n+-------------------------------------------------------------+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +-------------------------------------------------------------+ As can be seen from the figure above, the cluster control protocol is located above the data read and write protocol. The client (redis cluster client, IceFireDB-Proxy) obtains the master-slave structure of the cluster nodes according to the cluster status, and selects the relevant master and slave nodes Perform data read and write operations. For the request traffic carried by IceFireDB, it will enter the request processing cycle. During the request processing cycle, it will analyze and optimize the client request.\nProtocol advantages # At present, the web2 ecology of RESP protocol clients is very rich, and mainstream computer languages have been fully covered. IceFireDB is firstly compatible with the RESP NoSQL protocol, which can quickly meet the needs of existing application systems to access IceFireDB-NoSQL.\nAs a decentralized database, IceFireDB is compatible with the RESP protocol, and can hide high technical intelligence from the user layer. Users do not need to understand P2P, RAFT, CRDT, IPFS-LOG and other technologies, and only need to follow the RESP protocol to choose the appropriate client for application You can operate the database to read and write, and quickly meet the access and transformation of the business system.\nData control protocol # Strings Hashes Lists Sets Sorted Sets APPEND HSET RPUSH SADD ZADD BITCOUNT HGET LPOP SCARD ZCARD BITOP HDEL LINDEX SDIFF ZCOUNT BITPOS HEXISTS LPUSH SDIFFSTORE ZREM DECR HGETALL RPOP SINTER ZCLEAR DECRBY HINCRBY LRANGE SINTERSTORE ZRANK DEL HKEYS LSET SISMEMBER ZRANGE EXISTS HLEN LLEN SMEMBERS ZREVRANGE GET HMGET RPOPLPUSH SREM ZSCORE GETBIT HMSET LCLEAR SUNION ZINCRBY SETBIT HSETEX LCLEAR SUNIONSTORE ZREVRANK GETRANGE HSTRLEN LMCLEAR SCLEAR ZRANGEBYSCORE GETSET HVALS LEXPIRE SMCLEAR ZREVRANGEBYSCORE INCR HCLEAR LEXPIREAT SEXPIRE ZREMRANGEBYSCORE EXISTS HMCLEAR LKEYEXISTS SEXPIRE ZREMRANGEBYRANK GET HEXPIRE LTRIM SEXPIREAT GETBIT HEXPIREAT LTTL STTL SETBIT HKEYEXIST SPERSIST GETRANGE HTTL SKEYEXISTS GETSET INCRBY GET MGET MSET SET SETEX SETEXAT SETRANGE EXPIRE EXPIREAT TTL Cluster control protocol # IceFireDB-NoSQL integrates some Redis cluster status instructions in the RAFT parallel database scenario.\nVERSION # show the application version MACHINE # show information about the state machine RAFT LEADER # show the address of the current raft leader RAFT INFO [pattern] # show information about the raft server and cluster RAFT SERVER LIST # show all servers in cluster RAFT SERVER ADD id address # add a server to cluster RAFT SERVER REMOVE id # remove a server from the cluster RAFT SNAPSHOT NOW # make a snapshot of the data RAFT SNAPSHOT LIST # show a list of all snapshots on server RAFT SNAPSHOT FILE id # show the file path of a snapshot on server RAFT SNAPSHOT READ id [RANGE start end] # download all or part of a snapshot "},{"id":28,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 【Redis-server5.0.3】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second 【Redis-server6.2.5】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second 【Redis-server6.2.5】 Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second 【RedCon】 Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second 【RedHub】 Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":29,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/codec/","title":"Codec","section":"Architecture","content":" Codec Engine Details # "},{"id":30,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/codec/","title":"Codec layer","section":"Designs","content":" Codec layer design # The codec layer is the glue of the IceFireDB data expression layer, because the bottom layer of IceFireDB supports many storage engines, including centralized storage such as web2 disk, OSS, leveldb, and badger, as well as web3\u0026rsquo;s IPFS, crdt-kv, and IPFS-LOG For this kind of decentralized storage, the storage interface provided by any kind of storage is simple and not standardized. The codec layer of IceFireDB-NoSQL is abstracted through a unified driver layer, and by encoding and decoding many instruction semantics into a KV model, a richer data expression layer is built to support more data scenarios, such as Strings\\Hashs\\Sets\\Lists \\Sorted Sets.\n+-------------------------------------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ As an important glue layer, the codec layer connects the network layer, request layer, and KV storage layer.\nCodec advantages # In the decentralized database scenario, although the community also has a decentralized storage solution similar to the KV model, the KV model cannot meet the complex use of upper-level applications. We believe that rich data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as strings\\hashs\\lists\\sets\\sorted sets data structures). Just as the prosperity of the web2 application ecology is inseparable from the contribution of database infrastructure such as memcached, redis, and mysql, IceFireDB provides richer data types and can support data decentralization in more complex scenarios.\nThe encoding and decoding layer encodes data commands and parameters to meet the conversion of many rich instruction data models to KV models. Under the function of this conversion layer, the complex shielding layer of the data model layer and the storage engine layer is effectively constructed.\nThe data instruction layer does not need to care about the underlying storage engine, so it can adapt to various storage engine drivers with the help of the codec layer.\nThe underlying storage engine continues to provide a simple data operation interface (put\\get\\del\\iterator), without direct coupling and customization with the data presentation layer, and the abstract storage driver layer of the codec layer maintains the easy work of the two layers.\nAny problem in computer science can be solved by another layer of indirection.\nCodec Layer value # In addition to adding a richer data model and isolating the complexity of the request layer and storage layer, the codec layer of IceFireDB also has the following functions:\nImprove data access speed: A memory buffer layer is added to the encoding layer to effectively improve the performance of data access. Calculate the middle layer：The support of complex data structures often requires a certain amount of calculation. At present, the coding layer mainly performs CPU calculations, and subsequent calculations can be combined with GPU and FPGA, which can expand high-performance data calculations and add more possibilities for decentralized databases. blockchain/web3 connection layer: At present, whether it is a web2 or web3 database scenario, data access is the focus of attention, but with the development of blockchain and web3, trusted computing of data and non-tamperable proofs are becoming more and more important. The codec layer of IceFireDB is playing an important role , can be used to combine the blockchain data structure to build web3 side chain infrastructure, reducing the calculation and storage burden of the blockchain layer. "},{"id":31,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/deploy/configuration/","title":"Configuration Guide","section":"Deploy","content":" IceFireDB Configuration Guide # Overview # IceFireDB NoSQL component uses command-line flags for configuration instead of YAML configuration files. This guide covers all available command-line options for the NoSQL component.\nCommand-Line Interface # The IceFireDB NoSQL component is configured using command-line flags. Here\u0026rsquo;s the basic usage:\n# Basic usage ./icefiredb-nosql [options] # Example with common flags ./icefiredb-nosql \\ -storage-backend leveldb \\ -n 11001 \\ -a 0.0.0.0 \\ -j ./data \\ --openreads Command-Line Options # Network Configuration # Flag Description Default -n, --port Port to listen on 11001 -a, --address IP address to bind to \u0026quot;0.0.0.0\u0026quot; --max-connections Maximum client connections 10000 --timeout Connection timeout (seconds) 30 Storage Configuration # Flag Description Default -storage-backend Storage backend driver \u0026quot;leveldb\u0026quot; -j, --data-dir Data directory path \u0026quot;./data\u0026quot; --compression Enable compression true --cache-size Cache size (MB) 1024 Supported Storage Drivers # leveldb - Google LevelDB (default) badger - Dgraph BadgerDB ipfs - IPFS decentralized storage crdt - CRDT-based storage ipfs-log - IPFS append-only log oss - Object storage service hybriddb - Tiered hot/cold storage P2P Network Configuration # Flag Description Default --p2p-enable Enable P2P networking false --discovery-id Network discovery ID \u0026quot;icefiredb_cluster\u0026quot; --bootstrap-peers Bootstrap peer addresses [] --p2p-listen P2P listen address \u0026quot;/ip4/0.0.0.0/tcp/4001\u0026quot; Logging Configuration # Flag Description Default --log-level Log level (debug, info, warn, error) \u0026quot;info\u0026quot; --log-output Output destination (stdout, file) \u0026quot;stdout\u0026quot; --log-file Log file path (if output=file) \u0026quot;./icefiredb.log\u0026quot; --log-max-size Max log file size (MB) 100 --log-max-backups Max backup files 7 Advanced Configuration Options # Performance Tuning # Flag Description Default --max-memory Maximum memory usage (bytes) 1073741824 (1GB) --cache-size Cache size (MB) 1024 --max-clients Maximum client connections 10000 --openreads Enable open reads optimization false Security Configuration # Flag Description Default --tls-enable Enable TLS encryption false --tls-cert TLS certificate file \u0026quot;./certs/server.crt\u0026quot; --tls-key TLS private key file \u0026quot;./certs/server.key\u0026quot; --auth-password Authentication password \u0026quot;\u0026quot; --ip-whitelist IP whitelist (comma-separated) \u0026quot;\u0026quot; Usage Examples # Basic Deployment # # Start with default settings ./icefiredb-nosql # Start on specific port and address ./icefiredb-nosql -n 12000 -a 127.0.0.1 # Use BadgerDB storage backend ./icefiredb-nosql -storage-backend badger -j ./badger_data # Enable P2P networking ./icefiredb-nosql --p2p-enable --discovery-id \u0026#34;my_cluster\u0026#34; Production Deployment # ./icefiredb-nosql \\ -storage-backend badger \\ -n 11001 \\ -a 0.0.0.0 \\ -j /var/lib/icefiredb \\ --log-level warn \\ --log-file /var/log/icefiredb.log \\ --max-memory 4294967296 \\ --cache-size 2048 \\ --openreads Decentralized Deployment # ./icefiredb-nosql \\ -storage-backend ipfs \\ -n 11001 \\ -j ./ipfs_data \\ --p2p-enable \\ --discovery-id \u0026#34;decentralized_cluster\u0026#34; \\ --bootstrap-peers \u0026#34;/ip4/192.168.1.100/tcp/4001/p2p/QmPeer1\u0026#34; Environment Variables # IceFireDB NoSQL also supports configuration through environment variables:\nEnvironment Variable Equivalent Flag ICEFIREDB_PORT -n, --port ICEFIREDB_ADDRESS -a, --address ICEFIREDB_DATA_DIR -j, --data-dir ICEFIREDB_STORAGE_BACKEND -storage-backend ICEFIREDB_LOG_LEVEL --log-level ICEFIREDB_P2P_ENABLE --p2p-enable Getting Help # To see all available command-line options:\n# Show help and all available flags ./icefiredb-nosql --help # Show version information ./icefiredb-nosql --version Configuration Best Practices # Use descriptive data directories for different storage backends Set appropriate memory limits based on your system resources Enable compression for better storage efficiency Use environment variables for sensitive configuration Monitor performance and adjust settings as needed Test different storage backends for your specific workload Use the --openreads flag for read-intensive workloads Monitoring and Debugging # Built-in Monitoring # IceFireDB provides built-in monitoring through Redis commands:\n# Get server information 127.0.0.1:11001\u0026gt; INFO # Monitor memory usage 127.0.0.1:11001\u0026gt; INFO memory # Check storage statistics 127.0.0.1:11001\u0026gt; INFO storage # View slow queries 127.0.0.1:11001\u0026gt; SLOWLOG GET 10 Debug Mode # Enable debug logging for troubleshooting:\n./icefiredb-nosql --log-level debug --log-output stdout Troubleshooting # Common Issues # Port already in use: Use a different port with -n flag Permission denied: Check permissions on data directory Out of memory: Adjust --max-memory setting Storage backend not available: Verify the backend is compiled in Getting Support # If you encounter issues:\nCheck the logs with --log-level debug Verify your command-line syntax Consult the GitHub issues Join the community discussions Configuration Validation # IceFireDB validates command-line options on startup. Common validation errors include:\nInvalid flag syntax Unknown configuration options Invalid file paths Port conflicts Memory limits exceeding available system memory Runtime Configuration # Some configuration options can be changed at runtime using Redis commands:\n# Get configuration information 127.0.0.1:11001\u0026gt; CONFIG GET log-level # Change log level at runtime 127.0.0.1:11001\u0026gt; CONFIG SET log-level \u0026#34;debug\u0026#34; # Get storage driver information 127.0.0.1:11001\u0026gt; DRIVER.INFO Best Practices # Use environment variables for sensitive data like passwords Document your deployment commands for reproducibility Test configurations before production deployment Use version control for deployment scripts Test different storage backends for your workload Monitor performance and adjust settings accordingly Use the --help flag to explore all available options Troubleshooting # Common Issues # Port already in use: Change network.port Permission denied: Check file permissions for data directory Out of memory: Adjust performance.max_memory Storage driver not found: Verify driver name and dependencies Debug Mode # Enable debug logging for troubleshooting:\nlog: level: \u0026#34;debug\u0026#34; output: \u0026#34;stdout\u0026#34; See Also # Deployment Guide - Deployment instructions API Reference - Command documentation Storage Drivers - Storage backend details "},{"id":32,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/api-reference/","title":"API Reference","section":"Develop","content":" IceFireDB NoSQL API Reference # Overview # IceFireDB provides a Redis-compatible API with additional commands for decentralized operations. This document covers the supported commands, their syntax, and usage examples.\nCommand Categories # Server Management Commands # INFO # Returns server information and statistics.\nSyntax:\nINFO [section] Example:\nINFO INFO memory INFO replication Response: Returns a multi-bulk string with server information.\nFLUSHALL # Removes all keys from all databases.\nSyntax:\nFLUSHALL Response: Returns the number of keys removed.\nFLUSHDB # Removes all keys from the current database.\nSyntax:\nFLUSHDB Response: Returns the number of keys removed.\nString Commands # Supported String Operations # Command Description Status SET Set key value ✅ Implemented GET Get key value ✅ Implemented APPEND Append to string ✅ Implemented INCR Increment integer ✅ Implemented DECR Decrement integer ✅ Implemented MGET Get multiple values ✅ Implemented MSET Set multiple values ✅ Implemented SETEX Set with expiration ✅ Implemented Hash Commands # Supported Hash Operations # Command Description Status HSET Set hash field ✅ Implemented HGET Get hash field ✅ Implemented HGETALL Get all fields ✅ Implemented HDEL Delete field ✅ Implemented HINCRBY Increment field ✅ Implemented HKEYS Get all keys ✅ Implemented HVALS Get all values ✅ Implemented List Commands # Supported List Operations # Command Description Status LPUSH Push to left ✅ Implemented RPUSH Push to right ✅ Implemented LPOP Pop from left ✅ Implemented RPOP Pop from right ✅ Implemented LLEN Get list length ✅ Implemented LRANGE Get range ✅ Implemented LINDEX Get by index ✅ Implemented Set Commands # Supported Set Operations # Command Description Status SADD Add to set ✅ Implemented SREM Remove from set ✅ Implemented SMEMBERS Get all members ✅ Implemented SISMEMBER Check membership ✅ Implemented SCARD Get cardinality ✅ Implemented SINTER Intersection ✅ Implemented SUNION Union ✅ Implemented Sorted Set Commands # Supported Sorted Set Operations # Command Description Status ZADD Add to sorted set ✅ Implemented ZREM Remove from sorted set ✅ Implemented ZRANGE Get range ✅ Implemented ZREVRANGE Get reverse range ✅ Implemented ZCARD Get cardinality ✅ Implemented ZSCORE Get score ✅ Implemented ZRANK Get rank ✅ Implemented Extended Commands for Decentralized Operations # Storage Driver Management # DRIVER.SELECT # Select active storage driver.\nSyntax:\nDRIVER.SELECT driver_name Supported Drivers:\nleveldb - Local LevelDB storage badger - Local BadgerDB storage ipfs - IPFS decentralized storage crdt - CRDT-based storage ipfs-log - IPFS log storage oss - Object storage service hybriddb - Tiered storage Example:\nDRIVER.SELECT ipfs DRIVER.INFO # Get current driver information.\nSyntax:\nDRIVER.INFO P2P Network Commands # P2P.CONNECT # Connect to P2P network node.\nSyntax:\nP2P.CONNECT peer_address Example:\nP2P.CONNECT /ip4/192.168.1.100/tcp/4001/p2p/QmPeerID P2P.PEERS # List connected P2P peers.\nSyntax:\nP2P.PEERS CRDT Operations # CRDT.SYNC # Force CRDT synchronization.\nSyntax:\nCRDT.SYNC CRDT.STATUS # Get CRDT synchronization status.\nSyntax:\nCRDT.STATUS Error Codes # IceFireDB uses standard Redis error responses with additional error codes:\nERR wrong number of arguments - Incorrect command syntax ERR unknown command - Unsupported command ERR storage driver error - Storage backend failure ERR p2p network error - P2P connectivity issues ERR crdt sync conflict - CRDT synchronization conflict Response Types # IceFireDB supports all Redis response types:\nSimple Strings: +OK\\r\\n Errors: -ERR message\\r\\n Integers: :1000\\r\\n Bulk Strings: $5\\r\\nhello\\r\\n Arrays: *2\\r\\n$5\\r\\nhello\\r\\n$5\\r\\nworld\\r\\n Protocol Compatibility # IceFireDB is fully compatible with Redis RESP (REdis Serialization Protocol). Clients can use any Redis client library to connect to IceFireDB.\nConnection Example # import redis # Connect to IceFireDB r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Standard Redis operations r.set(\u0026#39;key\u0026#39;, \u0026#39;value\u0026#39;) value = r.get(\u0026#39;key\u0026#39;) print(value) # Output: \u0026#39;value\u0026#39; # IceFireDB extended operations r.execute_command(\u0026#39;DRIVER.SELECT\u0026#39;, \u0026#39;ipfs\u0026#39;) Performance Considerations # Use pipelining for bulk operations Choose appropriate storage driver for your use case Monitor P2P network latency for decentralized operations Consider data consistency requirements when selecting CRDT vs RAFT mode See Also # Command Implementation - Detailed command implementation details Storage Drivers - Storage backend documentation Network Protocol - Protocol layer documentation "},{"id":33,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/performance-benchmarking/","title":"Performance Benchmarking","section":"Tutorials","content":" Performance Benchmarking Guide # Overview # This guide covers performance testing and benchmarking methodologies for IceFireDB. Learn how to measure, analyze, and optimize IceFireDB performance for different workloads and storage drivers.\nBenchmarking Tools # redis-benchmark (Recommended) # The standard Redis benchmarking tool works perfectly with IceFireDB:\n# Basic benchmark redis-benchmark -h localhost -p 11001 -t set,get -c 50 -n 100000 # Comprehensive test redis-benchmark -h localhost -p 11001 \\ -t set,get,incr,lpush,lpop,sadd,spop,lpush,lrange \\ -c 100 -n 1000000 -P 16 # Specific command testing redis-benchmark -h localhost -p 11001 \\ -t set -c 50 -n 500000 --csv # Pipeline testing redis-benchmark -h localhost -p 11001 \\ -t set,get -c 100 -n 1000000 -P 100 memtier_benchmark # For more advanced benchmarking:\nmemtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=1:1 --test-time=300 --key-pattern=S:S Custom Scripts # Python example for custom workload testing:\nimport redis import time import statistics r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001, decode_responses=True) # Warm up for i in range(1000): r.set(f\u0026#39;key_{i}\u0026#39;, f\u0026#39;value_{i}\u0026#39;) # Benchmark SET operations times = [] for i in range(10000): start = time.time() r.set(f\u0026#39;bench_{i}\u0026#39;, \u0026#39;x\u0026#39; * 100) times.append(time.time() - start) print(f\u0026#34;SET ops/sec: {10000/sum(times):.0f}\u0026#34;) print(f\u0026#34;Average latency: {statistics.mean(times)*1000:.2f}ms\u0026#34;) print(f\u0026#34;P95 latency: {sorted(times)[9500]*1000:.2f}ms\u0026#34;) Benchmarking Methodology # Test Environment Setup # Hardware Considerations:\nCPU: Modern multi-core processor Memory: Sufficient RAM for workload Storage: SSD recommended for disk-based drivers Network: Gigabit Ethernet for distributed tests IceFireDB Configuration:\nnetwork: max_connections: 10000 performance: cache_size: 2048 max_memory: 4294967296 log: level: \u0026#34;warn\u0026#34; # Reduce logging overhead System Tuning:\n# Increase file limits ulimit -n 100000 # Network tuning sysctl -w net.core.somaxconn=65535 sysctl -w net.ipv4.tcp_max_syn_backlog=65535 Benchmark Scenarios # 1. Throughput Testing # # Maximum throughput test redis-benchmark -h localhost -p 11001 \\ -t set -c 512 -n 10000000 -P 512 -q # Expected output: # SET: 253232.12 requests per second 2. Latency Testing # # Low concurrency latency test redis-benchmark -h localhost -p 11001 \\ -t set -c 1 -n 10000 -P 1 --csv # High percentiles memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=10 --threads=2 \\ --ratio=1:0 --test-time=60 --key-pattern=S:S \\ --hide-histogram 3. Mixed Workload Testing # # Read-heavy workload (80% read, 20% write) memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=4:1 --test-time=300 # Write-heavy workload memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=50 --threads=4 \\ --ratio=1:4 --test-time=300 4. Data Size Impact # # Different value sizes for size in 100 500 1000 5000; do redis-benchmark -h localhost -p 11001 \\ -t set -c 50 -n 100000 -d $size -q echo \u0026#34;Value size: $size bytes\u0026#34; done Performance by Storage Driver # LevelDB Driver Performance # Best for: Balanced read/write workloads\n# LevelDB benchmark results SET: 250,000 - 300,000 ops/sec GET: 2,000,000 - 2,500,000 ops/sec Latency: 0.1 - 2ms (P99) Optimization Tips:\nIncrease write_buffer_size for write-heavy workloads Use compression for smaller data sizes Adjust block_size for your access patterns BadgerDB Driver Performance # Best for: Write-intensive workloads\n# BadgerDB benchmark results SET: 300,000 - 400,000 ops/sec GET: 1,800,000 - 2,200,000 ops/sec Latency: 0.1 - 3ms (P99) Optimization Tips:\nUse SSD storage for best performance Tune value_log_file_size for your workload Enable compression for value logs IPFS Driver Performance # Best for: Decentralized storage\n# IPFS benchmark results (local node) SET: 15,000 - 25,000 ops/sec GET: 40,000 - 60,000 ops/sec Latency: 5 - 50ms (P99) # IPFS benchmark results (remote nodes) SET: 5,000 - 10,000 ops/sec GET: 10,000 - 20,000 ops/sec Latency: 20 - 200ms (P99) Optimization Tips:\nIncrease hot_cache_size for better read performance Use local IPFS nodes for lower latency Optimize network connectivity between nodes CRDT Driver Performance # Best for: Distributed consistency\n# CRDT benchmark results SET: 120,000 - 180,000 ops/sec GET: 700,000 - 900,000 ops/sec Latency: 1 - 5ms (P99) Sync Latency: 10 - 100ms (cross-node) Optimization Tips:\nAdjust sync_interval based on consistency requirements Monitor conflict resolution overhead Use appropriate conflict resolution strategy Advanced Benchmarking # Long-running Tests # # 24-hour endurance test memtier_benchmark -s localhost -p 11001 \\ --protocol=redis --clients=100 --threads=8 \\ --ratio=1:1 --test-time=86400 --key-pattern=R:R # Monitor memory usage over time while true; do redis-cli -p 11001 info memory | grep used_memory_human sleep 60 done Cluster Benchmarking # # Multi-node cluster test # On node 1 memtier_benchmark -s node1 -p 11001 --clients=50 # On node 2 memtier_benchmark -s node2 -p 11001 --clients=50 # Monitor cluster sync performance redis-cli -p 11001 info replication Custom Workload Generation # Python script for realistic workload simulation:\nimport redis import random import time r = redis.Redis(host=\u0026#39;localhost\u0026#39;, port=11001) # Realistic key distribution (power law) def generate_key(): if random.random() \u0026lt; 0.8: # 80% hot keys return f\u0026#39;hot_{random.randint(1, 1000)}\u0026#39; else: # 20% cold keys return f\u0026#39;cold_{random.randint(1, 100000)}\u0026#39; # Simulate real workload operations = [] for _ in range(1000000): key = generate_key() if random.random() \u0026lt; 0.7: # 70% reads operations.append((\u0026#39;GET\u0026#39;, key)) else: # 30% writes operations.append((\u0026#39;SET\u0026#39;, key, f\u0026#39;value_{random.randint(1, 10000)}\u0026#39;)) # Execute and measure start = time.time() for op in operations: if op[0] == \u0026#39;GET\u0026#39;: r.get(op[1]) else: r.set(op[1], op[2]) duration = time.time() - start print(f\u0026#34;Operations: {len(operations)}\u0026#34;) print(f\u0026#34;Duration: {duration:.2f}s\u0026#34;) print(f\u0026#34;Throughput: {len(operations)/duration:.0f} ops/sec\u0026#34;) Performance Metrics # Key Metrics to Monitor # Throughput: Operations per second Latency: Response time percentiles (P50, P95, P99) Memory Usage: RSS, used_memory, peak memory CPU Utilization: User vs system time Network I/O: Bytes in/out, connections Disk I/O: Read/write operations, throughput Monitoring Commands # # Real-time monitoring redis-cli -p 11001 --stat # Detailed metrics redis-cli -p 11001 info all # Specific sections redis-cli -p 11001 info memory redis-cli -p 11001 info stats redis-cli -p 11001 info persistence # Slow log analysis redis-cli -p 11001 slowlog get 10 Optimization Techniques # Configuration Optimizations # # High-performance configuration network: max_connections: 100000 tcp_keepalive: 300 performance: cache_size: 4096 # 4GB max_memory: 8589934592 # 8GB max_memory_policy: \u0026#34;volatile-lru\u0026#34; storage: driver: \u0026#34;badger\u0026#34; value_log_file_size: 2147483648 # 2GB num_compactors: 4 num_level_zero_tables: 8 OS-Level Optimizations # # Linux performance tuning echo \u0026#39;net.core.somaxconn=65535\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026#39;vm.overcommit_memory=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026#39;vm.swappiness=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf # SSD optimization echo \u0026#39;deadline\u0026#39; \u0026gt; /sys/block/sda/queue/scheduler # Memory management echo \u0026#39;never\u0026#39; \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled Application-Level Optimizations # Use pipelining for bulk operations Batch small operations together Use appropriate data types for your workload Monitor and evict unused keys regularly Use compression for large values Troubleshooting Performance Issues # Common Performance Problems # High Latency:\nCheck network connectivity Monitor system resource usage Review slow log queries Low Throughput:\nVerify client configuration Check for resource bottlenecks Review storage driver performance Memory Issues:\nMonitor memory usage patterns Adjust max memory policy Implement key eviction strategies Diagnostic Commands # # Check current performance redis-cli -p 11001 info commandstats # Monitor real-time performance redis-cli -p 11001 monitor | head -100 # Analyze memory usage redis-cli -p 11001 memory stats # Check persistence performance redis-cli -p 11001 info persistence Benchmark Results Interpretation # Expected Performance Ranges # Driver SET ops/sec GET ops/sec P99 Latency LevelDB 200-300K 1.8-2.5M 1-2ms BadgerDB 300-400K 1.6-2.2M 1-3ms IPFS 15-25K 40-60K 5-50ms CRDT 120-180K 700-900K 1-5ms Factors Affecting Performance # Data Size: Larger values reduce throughput Concurrency: Higher concurrency increases throughput but may increase latency Network: Latency and bandwidth affect distributed performance Hardware: CPU, memory, and storage speed determine maximum performance Workload Pattern: Read vs write ratio affects optimal configuration Continuous Performance Testing # Automated Benchmarking # Set up automated performance testing:\n#!/bin/bash # daily-benchmark.sh DATE=$(date +%Y%m%d) RESULTS=\u0026#34;benchmark_results_$DATE.csv\u0026#34; echo \u0026#34;date,driver,test,throughput,latency_p99\u0026#34; \u0026gt; $RESULTS # Test different drivers for driver in leveldb badger ipfs; do # Switch driver redis-cli -p 11001 DRIVER.SELECT $driver # Run benchmarks redis-benchmark -h localhost -p 11001 -t set -c 50 -n 100000 -q | \\ awk -v d=\u0026#34;$driver\u0026#34; -v date=\u0026#34;$DATE\u0026#34; \u0026#39;{print date \u0026#34;,\u0026#34; d \u0026#34;,set,\u0026#34; $1 \u0026#34;,\u0026#34;}\u0026#39; \u0026gt;\u0026gt; $RESULTS redis-benchmark -h localhost -p 11001 -t get -c 50 -n 100000 -q | \\ awk -v d=\u0026#34;$driver\u0026#34; -v date=\u0026#34;$DATE\u0026#34; \u0026#39;{print date \u0026#34;,\u0026#34; d \u0026#34;,get,\u0026#34; $1 \u0026#34;,\u0026#34;}\u0026#39; \u0026gt;\u0026gt; $RESULTS done Performance Regression Testing # Monitor performance over time to detect regressions:\n# Weekly performance report #!/bin/bash WEEK=$(date +%Y-%U) redis-cli -p 11001 info stats \u0026gt; \u0026#34;stats_$WEEK.log\u0026#34; redis-cli -p 11001 info memory \u0026gt; \u0026#34;memory_$WEEK.log\u0026#34; # Compare with previous week # Alert on significant changes Best Practices # Test realistic workloads that match production usage Run long-term tests to identify memory leaks or degradation Monitor system resources during testing Document benchmark configurations for reproducibility Compare across versions to track performance improvements Test failure scenarios and recovery performance Validate with multiple tools for comprehensive analysis See Also # Storage Drivers - Driver-specific performance characteristics Configuration Guide - Performance-related configuration options API Reference - Performance monitoring commands "},{"id":34,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/quick_start/","title":"Quick Start","section":"IceFireDB NoSQL Engine","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test cluster，Simulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key cmd value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write tests：\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normally，The current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":35,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/storage-drivers/","title":"Storage Drivers","section":"Designs","content":" IceFireDB Storage Drivers # Overview # IceFireDB supports multiple storage backends through a unified driver interface. This allows you to choose the best storage engine for your specific use case, whether you need high-performance local storage or decentralized distributed storage.\nNote: Currently, IceFireDB NoSQL uses command-line flags for configuration rather than YAML configuration files. All storage driver configuration is done through runtime flags.\nDriver Architecture # All storage drivers implement the same interface:\ntype IDB interface { Put(key, value []byte) error Get(key []byte) ([]byte, error) Delete(key []byte) error Close() error // ... additional methods } This consistent interface enables seamless switching between different storage engines.\nConfiguration Approach # Currently, IceFireDB NoSQL uses command-line flags for all configuration. YAML configuration files are not yet supported for the NoSQL component. All storage driver settings are specified as command-line arguments when starting the server.\nBasic Usage # # Select storage backend and data directory ./icefiredb-nosql -storage-backend leveldb -j ./data # With additional options ./icefiredb-nosql -storage-backend badger -j ./badger_data --cache-size 2048 Available Drivers # LevelDB Driver # Best for: General purpose, high-performance local storage\nDescription: Google LevelDB is a fast key-value storage library that provides ordered mapping from string keys to string values.\nFeatures:\nHigh performance for read-intensive workloads Snappy compression support Atomic batch operations Crash consistency Configuration: Configured through command-line flags:\n./icefiredb-nosql \\ -storage-backend leveldb \\ -j ./leveldb_data \\ --compression \\ --block-size 4096 \\ --write-buffer-size 4194304 Performance:\nRead: ~2M ops/sec Write: ~250K ops/sec Latency: \u0026lt;1ms BadgerDB Driver # Best for: Write-intensive workloads, SSD optimization\nDescription: Dgraph BadgerDB is a fast key-value store written purely in Go, optimized for SSDs.\nFeatures:\nFaster writes than LevelDB SSD-optimized design Memory-mapped I/O Encryption support Configuration: Configured through command-line flags:\nConfiguration:\nPerformance:\nRead: ~1.8M ops/sec Write: ~300K ops/sec Latency: \u0026lt;1ms IPFS Driver # Best for: Decentralized storage, content-addressable data\nDescription: InterPlanetary File System driver stores data on IPFS network with local caching.\nFeatures:\nContent-addressable storage Distributed and decentralized Data deduplication Local hot cache for performance Configuration: Configured through command-line flags:\nConfiguration:\nPerformance:\nRead: ~50K ops/sec (depends on network) Write: ~20K ops/sec (depends on network) Latency: 10-100ms CRDT Driver # Best for: Conflict-free replicated data, eventual consistency\nDescription: Conflict-free Replicated Data Types driver for automatic conflict resolution in distributed systems.\nFeatures:\nAutomatic conflict resolution Eventual consistency Multi-master replication Type-specific merge semantics Configuration: Configured through command-line flags:\nConfiguration:\nPerformance:\nRead: ~800K ops/sec Write: ~150K ops/sec Latency: 2-5ms IPFS-LOG Driver # Best for: Append-only logs, audit trails, immutable data\nDescription: IPFS-based append-only log storage for immutable data records.\nFeatures:\nImmutable append-only storage Cryptographic verification Distributed audit trails Historical data preservation Configuration: Configured through command-line flags:\nConfiguration:\nPerformance:\nAppend: ~15K ops/sec Read: ~100K ops/sec Latency: 5-20ms OSS Driver # Best for: Cloud object storage integration, large datasets\nDescription: Object Storage Service driver for cloud storage integration (S3-compatible).\nFeatures:\nCloud storage integration Cost-effective for large datasets Durability and availability Multi-region support Configuration: Configured through command-line flags:\nConfiguration:\nPerformance:\nRead: ~5K ops/sec (depends on cloud) Write: ~2K ops/sec (depends on cloud) Latency: 50-200ms HybridDB Driver # Best for: Tiered storage, cost optimization\nDescription: Hybrid storage driver that automatically moves data between hot and cold storage tiers.\nFeatures:\nAutomatic data tiering Cost optimization Performance tuning Custom migration policies Configuration: Configured through command-line flags:\nCommand Line:\n./icefiredb-nosql \\ -storage-backend hybriddb \\ --hot-storage badger \\ --cold-storage oss \\ --hot-data-dir \u0026#34;./data/hot\u0026#34; \\ --migration-threshold 604800 \\ --hot-cache-size 2048 \\ --cold-bucket \u0026#34;my-cold-storage\u0026#34; Performance:\nHot storage: Same as underlying driver Cold storage: Same as underlying driver Migration: Background process Driver Selection Guide # Performance Considerations # Driver Read Performance Write Performance Latency Best Use Case LevelDB ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ General purpose BadgerDB ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Write-intensive IPFS ⭐⭐ ⭐⭐ ⭐⭐ Decentralized CRDT ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ Distributed sync IPFS-LOG ⭐⭐⭐ ⭐⭐ ⭐⭐⭐ Immutable logs OSS ⭐ ⭐ ⭐ Cloud storage HybridDB ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Tiered storage Consistency Models # Driver Consistency Model Replication Conflict Resolution LevelDB Strong Single-node N/A BadgerDB Strong Single-node N/A IPFS Eventual Multi-master Manual CRDT Eventual Multi-master Automatic IPFS-LOG Strong Multi-master Append-only OSS Eventual Multi-region Last-write-wins HybridDB Configurable Depends on tiers Depends on tiers Storage Requirements # Driver Memory Disk Network Special Requirements LevelDB Medium High Low None BadgerDB High High Low SSD recommended IPFS Medium Medium High IPFS node CRDT High Medium Medium None IPFS-LOG Low High High IPFS node OSS Low Low High Cloud credentials HybridDB Medium Variable Variable Multiple backends Configuration Examples # Basic LevelDB Configuration # ./icefiredb-nosql \\ -storage-backend leveldb \\ -j \u0026#34;/var/lib/icefiredb\u0026#34; \\ --compression \\ --block-size 4096 \\ --write-buffer-size 4194304 \\ --max-open-files 1000 Production IPFS Configuration # ./icefiredb-nosql \\ -storage-backend ipfs \\ -j \u0026#34;/var/lib/ipfs\u0026#34; \\ --hot-cache-size 2048 \\ --p2p-listen \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; \\ --bootstrap-peers \u0026#34;/ip4/192.168.1.100/tcp/4001/p2p/QmPeer1,/dns4/bootstrap.libp2p.io/tcp/443/p2p/QmNnoo...\u0026#34; Hybrid Storage Configuration # ./icefiredb-nosql \\ -storage-backend hybriddb \\ --hot-storage badger \\ --cold-storage oss \\ --hot-data-dir \u0026#34;/var/lib/icefiredb/hot\u0026#34; \\ --migration-threshold 2592000 \\ --hot-cache-size 4096 \\ --oss-endpoint \u0026#34;https://oss-cn-hangzhou.aliyuncs.com\u0026#34; \\ --oss-bucket \u0026#34;icefiredb-cold-storage\u0026#34; \\ --oss-access-key \u0026#34;your-access-key\u0026#34; \\ --oss-secret-key \u0026#34;your-secret-key\u0026#34; \\ --oss-region \u0026#34;cn-hangzhou\u0026#34; Driver Selection # Selecting Storage Drivers # Storage drivers are selected at startup using command-line flags. Driver selection cannot be changed at runtime - you must restart IceFireDB with the new storage backend.\n# Start with LevelDB backend ./icefiredb-nosql -storage-backend leveldb -j ./leveldb_data # Start with BadgerDB backend ./icefiredb-nosql -storage-backend badger -j ./badger_data # Start with IPFS backend ./icefiredb-nosql -storage-backend ipfs -j ./ipfs_data --p2p-enable Checking Current Driver # You can check the current storage driver using Redis commands:\n# Get driver information 127.0.0.1:11001\u0026gt; DRIVER.INFO Driver-specific Commands # Each driver may support additional commands:\n# CRDT synchronization 127.0.0.1:11001\u0026gt; CRDT.SYNC # IPFS peer management 127.0.0.1:11001\u0026gt; P2P.PEERS # HybridDB migration status 127.0.0.1:11001\u0026gt; HYBRID.STATUS Performance Tuning # LevelDB/BadgerDB Tuning # ./icefiredb-nosql \\ --cache-size 2048 \\ --max-open-files 5000 \\ --compression \\ --write-buffer-size 67108864 IPFS Performance Optimization # ./icefiredb-nosql \\ -storage-backend ipfs \\ --hot-cache-size 4096 \\ --chunk-size 262144 \\ --replication-factor 3 \\ --low-water 100 \\ --high-water 200 Monitoring Storage Performance # # Get storage statistics 127.0.0.1:11001\u0026gt; INFO storage # Monitor driver performance 127.0.0.1:11001\u0026gt; DRIVER.STATS Migration Between Drivers # Data Migration Process # Backup current data Install new driver dependencies Update configuration Restart IceFireDB Verify data integrity Automated Migration Tools # IceFireDB provides tools for migrating between storage drivers while maintaining data availability.\nTroubleshooting # Common Issues # Driver not found: Install required dependencies Permission denied: Check filesystem permissions Out of memory: Adjust cache sizes Network issues: Check connectivity for cloud/ipfs drivers Debug Commands # # Get detailed driver information 127.0.0.1:11001\u0026gt; DRIVER.DEBUG # Check storage health 127.0.0.1:11001\u0026gt; STORAGE.HEALTH # Reset driver configuration 127.0.0.1:11001\u0026gt; DRIVER.RESET Best Practices # Choose the right driver for your workload Monitor performance and adjust configuration Regularly backup important data Test migrations in staging environment Keep drivers updated with latest versions Use hybrid approaches for cost optimization Monitor storage health proactively Configuration Status # Current Implementation: IceFireDB NoSQL currently uses command-line flags for all configuration. YAML configuration file support has not yet been implemented for the NoSQL component.\nFuture Plans: YAML configuration support may be added in future releases to provide a more structured configuration approach alongside the existing CLI flags.\nSee Also # Configuration Guide - Command-line configuration options Performance Benchmarking - Performance testing guide API Reference - Driver management commands "},{"id":36,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/","title":"Designs","section":"IceFireDB NoSQL Engine","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer 1. RAFT guarantees data consistency within a single availability zone. 2. P2P network construction decentralized database communication. 3. NATS is a new network layer being built. P2P、RAFT、NATS Storage layer Many types of storage are currently supported. Under the codec computing layer, we abstract the KV storage driver layer, which is compatible with different storage engines of web2 and web3. goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS Protocol layer Based on the codec layer, we have built a protocol layer. A good communication protocol allows more applications to easily access the IceFireDB data network. Currently, we support the Redis-RESP NoSQL protocol and the MySQL protocol. RESP、SQL Codec layer The codec layer is the core of our system. For NoSQL scenarios, any data type will be abstracted into a KV storage model. With the flexible coding layer, we can build rich data operation structures and instructions, such as hash, sets, strings, etc. KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub "},{"id":37,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/","title":"IceFireDB NoSQL Engine","section":"Overview","content":" IceFireDB NoSQL Engine # Overview # IceFireDB NoSQL is a high-performance, decentralized key-value database that provides Redis compatibility with advanced distributed systems capabilities. It supports multiple storage backends, P2P networking, and various consistency models.\nKey Features # Redis Protocol Compatibility: Full support for Redis RESP protocol Multiple Storage Backends: LevelDB, BadgerDB, IPFS, CRDT, and more Decentralized Architecture: P2P automatic networking and data synchronization High Performance: Optimized for low latency and high throughput Flexible Consistency: RAFT for strong consistency, CRDT for eventual consistency Documentation Sections # Designs # Architecture overview and system design Network layer implementation Storage engine details Protocol specifications Codec layer architecture Develop # API reference and command documentation Development guides and examples Custom driver development Extension development Deploy # Installation and configuration Deployment strategies Performance tuning Monitoring and maintenance Quick Start # Getting started tutorials Basic usage examples First steps with IceFireDB Tutorials # Step-by-step guides Practical examples Advanced usage scenarios Integration tutorials Getting Started # Quick Installation # # Using Docker docker run -d -p 11001:11001 icefiredb/icefiredb:latest # From source git clone https://github.com/IceFireDB/IceFireDB.git cd IceFireDB \u0026amp;\u0026amp; make build Basic Usage # # Connect with redis-cli redis-cli -p 11001 # Basic operations SET mykey \u0026#34;Hello IceFireDB\u0026#34; GET mykey Use Cases # Web2 Applications # Traditional distributed caching Session storage Real-time data processing High-performance key-value stores Web3 Applications # Decentralized application data storage P2P data synchronization Immutable data logging Blockchain-integrated databases Hybrid Applications # Tiered storage (hot/cold data) Multi-cloud data distribution Edge computing data persistence Cross-region data replication Performance # IceFireDB delivers excellent performance across different workloads:\nLevelDB Driver: 250K+ SET ops/sec, 2M+ GET ops/sec Low Latency: Sub-millisecond response times High Concurrency: Support for thousands of concurrent connections Scalable Architecture: Linear scaling with node addition Community and Support # GitHub: https://github.com/IceFireDB/IceFireDB Documentation: https://www.icefiredb.xyz/ Discussions: GitHub Discussions Issues: GitHub Issues Contributing # IceFireDB is open source and welcomes contributions. See the CONTRIBUTING.md guide for details.\nLicense # IceFireDB is released under the Apache License 2.0. See LICENSE for details.\n"},{"id":38,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/","title":"IceFireDB-SQLite","section":"Overview","content":" IceFireDB-SQLite # IceFireDB-SQLite is a decentralized SQLite database designed to facilitate the construction of a global distributed database system. It allows users to write data to IceFireDB using the MySQL protocol while storing data in SQLite databases that are automatically synchronized among nodes through P2P networking.\nOverview # IceFireDB-SQLite combines the simplicity of SQLite with the power of decentralized P2P networking, enabling seamless data synchronization across multiple nodes. This setup is ideal for applications requiring high availability, fault tolerance, and global data distribution.\nKey Features # MySQL Protocol Support: Full compatibility with MySQL clients and tools Decentralized P2P Networking: Automatic data synchronization across nodes SQLite Storage: Lightweight, file-based database storage High Availability: Distributed architecture ensures continuous operation Easy Integration: Works with existing MySQL-compatible applications Architecture # IceFireDB-SQLite operates as a middleware layer that:\nAccepts MySQL protocol connections from clients Processes SQL commands and executes them on local SQLite databases Synchronizes data changes across the P2P network Maintains consistency through decentralized consensus Getting Started # Installation # Clone the Repository\ngit clone https://github.com/IceFireDB/IceFireDB.git Build the Project\ncd IceFireDB-SQLite make Run the Service\n./bin/IceFireDB-SQLite -c ./config/config.yaml Configuration # IceFireDB-SQLite uses a YAML configuration file. Key configuration options include:\nServer Address: MySQL protocol listening port (default: :23306) SQLite Database File: Path to the SQLite database file P2P Networking: Enable/disable decentralized synchronization User Authentication: Configure tenant access credentials Usage Examples # Connect using any MySQL client:\nmysql -h 127.0.0.1 -P 23306 -u root -p Execute SQL commands as you would with any MySQL database:\nCREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, email TEXT); INSERT INTO users (name, email) VALUES (\u0026#39;John Doe\u0026#39;, \u0026#39;john@example.com\u0026#39;); SELECT * FROM users; Application Scenarios # Decentralized Applications: Build dApps with SQL database capabilities Edge Computing: Distributed SQL databases for edge devices Data Synchronization: Multi-region data replication and synchronization Web2 to Web3 Migration: Transition traditional SQL applications to decentralized infrastructure Technical Details # Protocol: MySQL wire protocol (version 5.7 compatible) Storage Engine: SQLite with P2P synchronization Networking: libp2p for decentralized peer discovery and communication Consensus: Eventual consistency through gossip protocols Support \u0026amp; Community # Documentation: IceFireDB Documentation GitHub: IceFireDB GitHub Repository Community: Join our Discord community for support and discussions "},{"id":39,"href":"/icefiredb_docs/icefiredb/","title":"Overview","section":"IceFireDB - Building global database infrastructure","content":" IceFireDB - Building global database infrastructure # Overview # Based on cutting-edge computer science, the IceFireDB project integrates new ideas and research in the fields of message passing, event instruction processing, data consistency and replication, decentralized network, reliable data storage, high-performance network framework, etc.The team of researchers and engineers at IceFireDB combines the cutting-edge ideas from distributed systems and concurrent databases. These ideas combine collision-free replication data types (CRDT) and decentralized appendix-only logs, which can be used to simulate the variable sharing state between peers in P2P applications to create a new data infrastructure with high security, high performance and low latency.\nThe core of IceFireDB architecture is geographically distributed event source and decentralized Log source, with log-level CRDT replication.In order to realize the consistency of replication, IceFireDB provides a stable decentralized networking model, which allows the combination of public networks among different sites. Multiple IceFireDB nodes can be run inside each site, and RAFT network can be formed between nodes, which ensures the data consistency and stable storage within the same site.\nConcurrent change conflict handling # Different from the final consistent database, IceFireDB manages changes at the atomic level and merges data changes made through the network. It maintains a single version of data through CRDT, IPFS-LOG and status convergence engine, and maintains the incremental model of decentralized instructions by building the icefiredb-log library, which meets the broadcast and data sequence consistency of decentralized database logs.\nThe technical use of CRDT and IPFS-LOG is implicit. IceFireDB exposes the conventional Redis, SQL database and API to users by integrating the decentralized technology and functions with RESP and SQL protocol servers.\nAdaptive consistency # IceFireDB data network responds to data reading and updating requests, while maintaining a consistent data view in the world. IceFireDB network can effectively improve the delay and flexibly expand to meet the needs of applications.\nNon-inductive data coordination # IceFireDB allows all locations to read and write local data in parallel, and does not require users to know which data should be placed in which location, or require users to redesign the architecture every time they need to add or delete locations.\nDistributed multi-model database # IceFireDB supports a variety of data models, mainly supporting NoSQL and SQL data scenarios. Store, query and modify data in the form of NoSQL and SQL on our multi-master architecture. Respond to read and update requests with local latency, while maintaining a consistent view of global fast data.\n"},{"id":40,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/","title":"Tutorials","section":"IceFireDB NoSQL Engine","content":" Tutorials # "},{"id":41,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/deploy/","title":"Deploy","section":"IceFireDB NoSQL Engine","content":" Deploy # "},{"id":42,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/","title":"IceFireDB-Redis-Proxy","section":"Overview","content":" IceFireDB-Redis-Proxy # IceFireDB-Redis-Proxy is a database proxy that adds decentralization capabilities to traditional Redis databases. It provides a convenient mechanism to build a globally distributed storage system with automatic networking, enabling decentralized data synchronization for Redis databases commonly used in web2 applications.\nOverview # IceFireDB-Redis-Proxy acts as a middleware layer between Redis clients and backend Redis servers, adding P2P networking capabilities to enable decentralized data synchronization across multiple Redis instances.\nKey Features # Complete Redis Protocol Support: Full compatibility with Redis clients and commands Multiple Data Source Modes: Support for both standalone and cluster Redis deployments P2P Automatic Networking: Decentralized data synchronization across nodes Traffic Control: Read-write separation and multi-tenant data isolation Cluster Management: Excellent cluster state management and failover capabilities High Availability: Distributed architecture ensures continuous operation Architecture # IceFireDB-Redis-Proxy operates as a proxy layer that:\nAccepts Redis protocol connections from clients Routes commands to backend Redis servers (standalone or cluster) Synchronizes data changes across the P2P network Provides traffic control and multi-tenant isolation Getting Started # Installation # Clone the Repository\ngit clone https://github.com/IceFireDB/IceFireDB.git Build the Project\ncd IceFireDB-Redis-Proxy make Run the Service\n./bin/IceFireDB-Redis-Proxy -c ./config/config.yaml Configuration # IceFireDB-Redis-Proxy uses a YAML configuration file. Key configuration options include:\nProxy Settings: Local port, TLS configuration P2P Networking: Enable/disable decentralized synchronization Redis Backend: Connection details for backend Redis servers Service Discovery: P2P service discovery and topic configuration Usage Examples # Connect using any Redis client:\nredis-cli -p 16379 Execute Redis commands as you would with any Redis server:\nSET mykey \u0026#34;Hello World\u0026#34; GET mykey HSET user:1000 name \u0026#34;John Doe\u0026#34; email \u0026#34;john@example.com\u0026#34; HGETALL user:1000 Supported Commands # IceFireDB-Redis-Proxy supports a comprehensive set of Redis commands including:\nString Operations # SET, GET, MSET, MGET, INCR, DECR, APPEND, GETRANGE, SETRANGE Hash Operations # HSET, HGET, HMSET, HMGET, HGETALL, HKEYS, HVALS, HDEL, HEXISTS List Operations # LPUSH, RPUSH, LPOP, RPOP, LRANGE, LINDEX, LLEN, LINSERT Set Operations # SADD, SREM, SMEMBERS, SISMEMBER, SCARD, SPOP, SRANDMEMBER Sorted Set Operations # ZADD, ZRANGE, ZREVRANGE, ZRANK, ZREVRANK, ZSCORE, ZCARD Stream Operations # XADD, XREAD, XRANGE, XREVRANGE, XLEN, XDEL, XGROUP Administrative Commands # PING, QUIT, COMMAND, INFO, CLIENT, CONFIG Application Scenarios # Decentralized Caching: Distributed Redis cache with automatic synchronization Session Storage: Distributed session management for web applications Real-time Data: Decentralized real-time data storage and synchronization Web2 to Web3 Migration: Transition traditional Redis-based applications to decentralized infrastructure Multi-region Deployment: Global data distribution with local latency Technical Details # Protocol: Redis protocol (RESP) compatibility Networking: libp2p for decentralized peer discovery and communication Consistency: Eventual consistency with conflict resolution Performance: Low latency proxy with minimal overhead Performance Characteristics # High Throughput: Efficient command routing and processing Low Latency: Direct connections to backend Redis servers Scalability: Horizontal scaling through additional proxy nodes Resource Efficiency: Lightweight proxy layer with minimal resource consumption Security Features # Transport Encryption: TLS support for secure communications Authentication: Redis authentication with configurable credentials Access Control: Multi-tenant isolation and command filtering Network Security: Secure peer authentication and encrypted P2P communications Support \u0026amp; Community # Documentation: IceFireDB Documentation GitHub: IceFireDB GitHub Repository Community: Join our Discord community for support and discussions Issue Tracking: Report bugs and request features on GitHub Issues "},{"id":43,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/","title":"IceFireDB-SQLProxy","section":"Overview","content":" IceFireDB-SQLProxy # IceFireDB-SQLProxy is a powerful middleware that facilitates the decentralization of traditional SQL database data. It provides a seamless mechanism to construct a globally distributed storage system with automatic networking capabilities, enabling decentralized data synchronization for MySQL databases commonly used in web2 applications.\nOverview # IceFireDB-SQLProxy acts as a bridge between traditional MySQL clients and backend MySQL servers, adding P2P networking capabilities to enable decentralized data synchronization across multiple MySQL instances. This makes it an ideal solution for bridging the gap between traditional web2 applications and the emerging web3 architecture.\nKey Features # MySQL Protocol Support: Full compatibility with MySQL clients and tools Decentralized P2P Networking: Automatic data synchronization across nodes Multiple Access Levels: Support for admin and read-only database connections Connection Pooling: Efficient connection management and reuse High Availability: Distributed architecture ensures continuous operation Web2 to Web3 Transition: Facilitates migration from traditional to decentralized infrastructure Architecture # IceFireDB-SQLProxy operates as a proxy layer that:\nAccepts MySQL protocol connections from clients Routes SQL commands to backend MySQL servers Synchronizes data changes across the P2P network Provides connection pooling and access control Enables decentralized data distribution Getting Started # Installation # Clone the Repository\ngit clone https://github.com/IceFireDB/IceFireDB.git Build the Project\ncd IceFireDB-SQLProxy make Run the Service\n./IceFireDB-SQLProxy -c ./config/config.yaml Configuration # IceFireDB-SQLProxy uses a YAML configuration file. Key configuration options include:\nServer Settings: MySQL protocol listening port (default: :33306) MySQL Connections: Admin and read-only database connection details P2P Networking: Enable/disable decentralized synchronization Connection Pooling: Min/max connections and idle connection settings User Authentication: Configure tenant access credentials Usage Examples # Connect using any MySQL client:\nmysql -h 127.0.0.1 -P 33306 -u host1 -p Execute SQL commands as you would with any MySQL database:\nCREATE DATABASE exampledb; USE exampledb; CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(100), email VARCHAR(100)); INSERT INTO users (name, email) VALUES (\u0026#39;Jane Smith\u0026#39;, \u0026#39;jane@example.com\u0026#39;); SELECT * FROM users; Supported Features # SQL Command Support # DDL Commands: CREATE, ALTER, DROP databases and tables DML Commands: SELECT, INSERT, UPDATE, DELETE Transaction Support: BEGIN, COMMIT, ROLLBACK Stored Procedures: Support for stored procedure execution Prepared Statements: Parameterized query support Advanced Features # Connection Pooling: Efficient management of database connections Read-Write Separation: Different connections for admin and read-only operations Multi-Tenant Support: Isolation between different users and databases Load Balancing: Distribution of queries across multiple backend servers Failover Support: Automatic failover to healthy database instances Application Scenarios # Decentralized MySQL Storage: Build distributed MySQL storage with automatic synchronization Database Federation: Combine multiple MySQL instances into a unified decentralized system Web2 to Web3 Migration: Transition traditional web applications to decentralized infrastructure Multi-Region Deployment: Global database distribution with local access latency Disaster Recovery: Automated data replication across geographic regions Edge Computing: Distributed database nodes for edge computing scenarios Technical Details # Protocol: MySQL wire protocol (version 5.6+ compatible) Networking: libp2p for decentralized peer discovery and communication Consistency: Eventual consistency with operation-based replication Performance: Low latency proxy with connection pooling and query optimization Performance Characteristics # High Throughput: Efficient query routing and connection management Low Latency: Optimized protocol handling and network communications Scalability: Horizontal scaling through additional proxy nodes Resource Efficiency: Connection pooling reduces backend database load Reliability: Automatic retry and failover mechanisms Security Features # Transport Encryption: TLS support for secure client connections Database Authentication: MySQL authentication with configurable credentials Access Control: Multi-level access control (admin vs read-only) Network Security: Secure peer authentication and encrypted P2P communications Audit Logging: Query logging and access monitoring capabilities Deployment Considerations # Backend Database Requirements # MySQL 5.6 or later recommended Proper database user permissions configured Network connectivity between proxy and database servers Adequate database resources for expected workload Network Configuration # Firewall rules for MySQL protocol ports P2P networking port configuration NAT traversal considerations for distributed deployments DNS or service discovery for backend database hosts Monitoring and Management # Health Monitoring: Regular checks of backend database connectivity Performance Metrics: Query latency, throughput, and error rates Connection Pool Statistics: Active, idle, and total connections P2P Network Status: Peer connectivity and synchronization state Logging: Comprehensive logs for debugging and operational monitoring Support \u0026amp; Community # Documentation: IceFireDB Documentation GitHub: IceFireDB GitHub Repository Community: Join our Discord community for support and discussions Issue Tracking: Report bugs and request features on GitHub Issues Contributions: Welcome community contributions and improvements "},{"id":44,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/","title":"IceFireDB-PubSub","section":"Overview","content":" IceFireDB-PubSub # IceFireDB-PubSub is a high-performance, highly available, and decentralized subscription system designed to seamlessly migrate web2 applications using Redis\u0026rsquo;s publish and subscribe functionality into a decentralized peer-to-peer (P2P) subscription network.\nOverview # IceFireDB-PubSub implements the Redis PubSub protocol while leveraging P2P networking to create a globally distributed publish-subscribe system. It allows traditional Redis clients to participate in decentralized messaging without requiring any code changes.\nKey Features # Redis PubSub Protocol Support: Full compatibility with Redis publish/subscribe commands Decentralized P2P Networking: Built on libp2p for resilient peer-to-peer communication High Performance: Optimized for low-latency message delivery High Availability: Distributed architecture ensures continuous operation NAT Traversal: Support for nodes behind firewalls and NAT devices Seamless Migration: No changes required for existing Redis clients Architecture # IceFireDB-PubSub operates as a protocol-compatible layer that:\nAccepts Redis PubSub protocol connections from clients Translates Redis PubSub commands to P2P messaging Distributes messages across the decentralized network Maintains subscription state and message delivery guarantees Getting Started # Installation # Clone the Repository\ngit clone https://github.com/IceFireDB/IceFireDB.git Build the Project\ncd IceFireDB-PubSub make Run the Service\n./bin/IceFireDB-PubSub -c ./config/config.yaml Configuration # IceFireDB-PubSub uses a YAML configuration file. Key configuration options include:\nProxy Settings: Redis protocol listening port (default: 16379) P2P Networking: Enable/disable decentralized messaging Service Discovery: P2P service discovery and topic configuration Security Settings: TLS configuration and authentication options Usage Examples # Connect using any Redis client:\nredis-cli -p 16379 Subscribe to a channel:\nSUBSCRIBE news Publish messages to a channel:\nPUBLISH news \u0026#34;Hello World\u0026#34; PUBLISH news \u0026#34;Latest updates\u0026#34; Supported Commands # IceFireDB-PubSub supports the core Redis PubSub commands:\nSubscription Commands # SUBSCRIBE: Subscribe to one or more channels UNSUBSCRIBE: Unsubscribe from channels PSUBSCRIBE: Subscribe to channels using patterns PUNSUBSCRIBE: Unsubscribe from pattern-based subscriptions Publishing Commands # PUBLISH: Publish a message to a channel Monitoring Commands # PING: Check server responsiveness QUIT: Close the connection Application Scenarios # Decentralized Messaging: Build distributed messaging systems without central brokers Real-time Notifications: Distributed notification systems for applications IoT Device Communication: Messaging between IoT devices across networks Multi-region Chat: Global chat applications with local latency Event-driven Architectures: Distributed event processing systems Web2 to Web3 Migration: Transition Redis-based pub/sub systems to decentralized infrastructure Technical Details # Protocol: Redis protocol (RESP) with PubSub extension Networking: libp2p for decentralized peer discovery and communication Message Delivery: At-least-once delivery semantics Performance: Low latency message routing and delivery Scalability: Horizontal scaling through additional nodes Performance Characteristics # Low Latency: Optimized message routing and delivery High Throughput: Efficient message processing and distribution Scalability: Linear scaling with additional nodes Resource Efficiency: Lightweight message processing with minimal overhead Reliability: Automatic message retry and delivery guarantees Networking Model # IceFireDB-PubSub uses a decentralized P2P networking model with:\nService Discovery: Automatic peer discovery using Kademlia DHT Topic-based Routing: Efficient message routing using pubsub topics NAT Traversal: Hole punching and relay support for NAT devices Connection Management: Automatic reconnection and fault tolerance Mesh Networking: Resilient mesh topology for reliable delivery Security Features # Transport Encryption: TLS support for secure client connections Message Encryption: Optional end-to-end message encryption Access Control: Channel-based access control and authentication Network Security: Secure peer authentication and encrypted P2P communications Audit Logging: Message logging and access monitoring Deployment Considerations # Network Requirements # Open ports for Redis protocol (default: 16379) P2P networking ports for node communication NAT/firewall configuration for distributed deployments DNS or static IP configuration for client connectivity Resource Requirements # Adequate network bandwidth for message traffic Sufficient memory for connection and subscription management CPU resources for message processing and encryption Storage for logging and monitoring data Monitoring and Management # Connection Statistics: Active connections and subscription counts Message Metrics: Message throughput, latency, and error rates Network Status: Peer connectivity and network health Resource Usage: Memory, CPU, and network utilization Logging: Comprehensive logs for debugging and operational monitoring Support \u0026amp; Community # Documentation: IceFireDB Documentation GitHub: IceFireDB GitHub Repository Community: Join our Discord community for support and discussions Issue Tracking: Report bugs and request features on GitHub Issues Contributions: Welcome community contributions and improvements "},{"id":45,"href":"/icefiredb_docs/icefiredb/faqs/","title":"Frequently Asked Questions","section":"Overview","content":" Frequently Asked Questions # General Questions # What is IceFireDB? # IceFireDB is a decentralized database infrastructure that combines Redis compatibility with advanced distributed systems capabilities. It supports multiple storage backends, P2P networking, and various consistency models for both web2 and web3 applications.\nHow does IceFireDB differ from Redis? # While IceFireDB is fully compatible with Redis protocol, it offers several advanced features:\nMultiple storage backends (LevelDB, BadgerDB, IPFS, CRDT, etc.) Decentralized P2P networking Pluggable storage architecture Cross-region data synchronization Web3 integration capabilities Is IceFireDB production ready? # Yes, IceFireDB is production ready for many use cases:\n✅ LevelDB and BadgerDB drivers are stable and production-ready ✅ Redis protocol compatibility is complete 🟡 IPFS and CRDT drivers are in beta 🚧 Advanced web3 features are in development What license does IceFireDB use? # IceFireDB is open source and released under the Apache License 2.0.\nInstallation \u0026amp; Setup # How do I install IceFireDB? # You can install IceFireDB in several ways:\nFrom source:\ngit clone https://github.com/IceFireDB/IceFireDB.git cd IceFireDB make build Using Docker:\ndocker run -d -p 11001:11001 icefiredb/icefiredb:latest Pre-built binaries: Download from GitHub Releases\nWhat are the system requirements? # Minimum:\n1GB RAM 2CPU cores 10GB disk space Linux/macOS/Windows (WSL) Recommended for production:\n4GB+ RAM 4+ CPU cores SSD storage 50GB+ disk space Linux How do I configure IceFireDB? # Create a config.yaml file:\nnetwork: port: 11001 storage: driver: \u0026#34;leveldb\u0026#34; data_dir: \u0026#34;./data\u0026#34; log: level: \u0026#34;info\u0026#34; Start with: ./icefiredb -c config.yaml\nStorage \u0026amp; Performance # Which storage driver should I use? # Use Case Recommended Driver General purpose LevelDB Write-intensive BadgerDB Decentralized storage IPFS Conflict-free replication CRDT Immutable logs IPFS-LOG Cloud integration OSS Tiered storage HybridDB What performance can I expect? # LevelDB Driver:\nSET: 250,000+ ops/sec GET: 2,000,000+ ops/sec Latency: \u0026lt;2ms P99 BadgerDB Driver:\nSET: 300,000+ ops/sec GET: 1,800,000+ ops/sec Latency: \u0026lt;3ms P99 IPFS Driver:\nSET: 15,000-25,000 ops/sec GET: 40,000-60,000 ops/sec Latency: 5-50ms P99 How do I optimize performance? # Choose the right storage driver for your workload Allocate sufficient memory for caching Use SSD storage for disk-based drivers Enable compression for large values Tune network settings for distributed deployments Use pipelining for bulk operations How much memory does IceFireDB need? # Memory requirements depend on:\nStorage driver selection Cache size configuration Workload characteristics Data size and access patterns Minimum: 1GB Recommended: 4GB+ Production: 8GB+\nDistributed Features # How does P2P networking work? # IceFireDB uses libp2p for decentralized networking:\nNodes automatically discover each other Data syncs across connected nodes Supports NAT traversal Works across different networks What consistency models are supported? # Strong consistency: RAFT consensus within availability zones Eventual consistency: CRDT-based conflict resolution Configurable consistency: Per-operation consistency levels How do I set up a cluster? # Configure each node: p2p: enable: true discovery_id: \u0026#34;my_cluster\u0026#34; listen_address: \u0026#34;/ip4/0.0.0.0/tcp/4001\u0026#34; Connect nodes: p2p: bootstrap_peers: - \u0026#34;/ip4/node1-ip/tcp/4001/p2p/QmPeer1\u0026#34; - \u0026#34;/ip4/node2-ip/tcp/4001/p2p/QmPeer2\u0026#34; Verify connectivity: P2P.PEERS How does data replication work? # Data replication depends on the storage driver:\nLevelDB/BadgerDB: Single-node only IPFS: Full mesh replication CRDT: Automatic conflict resolution RAFT: Strong consistency within groups Operations \u0026amp; Maintenance # How do I backup IceFireDB? # For disk-based drivers:\n# Stop IceFireDB ./icefiredb --stop # Backup data directory tar czf backup-$(date +%Y%m%d).tar.gz ./data/ # Restart IceFireDB ./icefiredb --start For cloud drivers: Use native cloud backup tools\nFor decentralized drivers: Data is automatically replicated\nHow do I monitor IceFireDB? # Built-in monitoring:\n# Basic info INFO # Memory usage INFO memory # Statistics INFO stats # Replication status INFO replication # Slow queries SLOWLOG GET 10 External monitoring:\nPrometheus metrics endpoint Health check endpoints Log aggregation Performance monitoring tools How do I upgrade IceFireDB? # Backup your data Stop the running instance Install the new version Verify configuration compatibility Start the new version Monitor for issues How do I troubleshoot issues? # Check logs:\ntail -f icefiredb.log Debug commands:\n# Server info INFO # Connection info CLIENT LIST # Memory info MEMORY STATS # Storage health STORAGE.HEALTH Common issues:\nPort conflicts: Change network.port Permission errors: Check file permissions Memory issues: Adjust performance.max_memory Network issues: Check firewall settings Security # Is IceFireDB secure? # IceFireDB provides several security features:\nTLS/SSL encryption for network traffic Authentication support IP whitelisting Storage encryption (some drivers) Secure P2P communications How do I enable authentication? # security: authentication: enable: true password: \u0026#34;your-secret-password\u0026#34; Clients must authenticate:\nredis-cli -h localhost -p 11001 -a your-secret-password How do I enable TLS? # security: tls: enable: true cert_file: \u0026#34;/path/to/cert.pem\u0026#34; key_file: \u0026#34;/path/to/key.pem\u0026#34; ca_file: \u0026#34;/path/to/ca.pem\u0026#34; How do I restrict access? # IP whitelisting:\nip_white_list: enable: true list: - \u0026#34;192.168.1.0/24\u0026#34; - \u0026#34;10.0.0.1\u0026#34; - \u0026#34;localhost\u0026#34; Development # How do I contribute to IceFireDB? # Fork the repository on GitHub Create a feature branch Make your changes Write tests Submit a pull request Address review comments See CONTRIBUTING.md for details.\nHow do I create a custom storage driver? # Implement the driver interface: type MyDriver struct{} func (d MyDriver) Open(path string, cfg *config.Config) (driver.IDB, error) { // implementation } Register your driver: func init() { driver.Register(MyDriver{}) } Use your driver: storage: driver: \u0026#34;mydriver\u0026#34; What APIs are available? # IceFireDB supports:\nRedis RESP protocol - Full Redis compatibility HTTP API - RESTful interface gRPC API - High-performance RPC Custom extensions - IceFireDB-specific commands Web3 Integration # How does IceFireDB support web3? # IceFireDB provides several web3 features:\nIPFS integration for decentralized storage CRDT for conflict-free data sync P2P automatic networking Blockchain integration capabilities Immutable data logging Can I use IceFireDB with blockchain? # Yes, IceFireDB can integrate with blockchain systems:\nStore blockchain data in IceFireDB Use IceFireDB as off-chain storage Implement hybrid on-chain/off-chain architectures Support decentralized applications (dApps) How does IPFS integration work? # IceFireDB can use IPFS as a storage backend:\nData stored on IPFS network Local caching for performance Content-addressable storage Automatic data deduplication IPLD and Data Structures # Would you be able to write IPLD Schemas and specs for the data structures? # Our current database storage engine implementation is divided into two categories:\nInstruction-based synchronization: Built on RESP instructions and SQL statements, providing immutable, append-only operation logs with decentralized instruction broadcasting and playback for data synchronization and consistency.\nIPFS-based KV storage: Implementing KV storage engine based on IPFS, encoding rich NoSQL data structures through KV-value coding, enabling complete database data growth on IPFS without relying solely on ipfs-log functionality.\nWe primarily use ipfs-log, an immutable conflict-free replicated data model for distributed systems. Based on ipfs-log, we\u0026rsquo;ve abstracted a KV engine that serves as one of IceFireDB\u0026rsquo;s storage drivers. Other projects can build upon this KV engine for their own readers/writers.\nIceFireDB abstracts a data coding layer on the KV engine that supports complex data structures like hashes and lists beyond basic KV operations. While we currently implement IPFS KV storage, we\u0026rsquo;re working towards complete database data growth on IPFS and are exploring IPLD designs for NoSQL and SQL data relationships.\nDatabase Operation # How does the DB run? # IceFireDB operates in server mode, running on servers and supporting:\nRedis RESP protocol - Full Redis compatibility MySQL communication protocol - SQL database access Multiple client languages - Golang, JavaScript, PHP, etc. We also support framework integration for direct embedding into application code through projects like:\nredhub IceFireDB-crdt-kv ipfs-nosql-frame Server mode advantages:\nStandard data protocols (RESP, MySQL) minimize application changes Abstracts complexity of underlying IPFS, libp2p, ipfs-log, and CRDT Data Mutability # How do you address mutability of data? # IceFireDB provides two implementation models for data variability:\n1. Instruction Broadcast Model (ipfs-log based):\nImmutable operation-based conflict-free replication using ipfs-log, CRDT, and libp2p pubsub Encapsulates various data structures (event, kv) on ipfs-log Implements multi-node database instruction broadcasting Abstract variable KV engine on BadgerDB/LevelDB foundation Nodes broadcast write instructions network-wide Each node\u0026rsquo;s IceFireDB driver executes broadcast instructions for eventual consistency 2. Full Storage Model (IPFS based):\nComplete data growth on IPFS IPFS driver encodes upper-level commands into unified KV data structure Stores and changes values with new CIDs linked to keys Building towards multi-node key broadcast network and data synchronization Programming Languages # What program languages are you targeting? # Primary Implementation: Golang\nClient Support: All languages with Redis and MySQL clients:\nJavaScript/Node.js Python Java Ruby PHP Rust And many more through standard Redis RESP and MySQL protocols Troubleshooting # IceFireDB won\u0026rsquo;t start # Common causes:\nPort already in use Invalid configuration Permission issues Missing dependencies Solutions:\nCheck port availability: netstat -tlnp | grep 11001 Validate config: ./icefiredb --check-config Check permissions: ls -la ./data/ Review logs: tail -f icefiredb.log High memory usage # Solutions:\nReduce cache size Enable key eviction Monitor memory usage patterns Upgrade to more memory Configuration:\nperformance: max_memory: 4294967296 # 4GB max_memory_policy: \u0026#34;volatile-lru\u0026#34; Slow performance # Diagnosis:\nCheck system resources Monitor network latency Review storage performance Analyze workload patterns Solutions:\nChoose faster storage driver Increase cache size Optimize configuration Use pipelining for bulk operations Connection issues # Check:\nFirewall settings Network connectivity Port availability Authentication configuration Test connectivity:\ntelnet localhost 11001 nc -zv localhost 11001 Community \u0026amp; Support # Where can I get help? # Community resources:\nGitHub Issues GitHub Discussions Documentation Discord Community How do I report bugs? # Create an issue on GitHub with:\nIceFireDB version Configuration details Steps to reproduce Error messages/logs Expected vs actual behavior Where can I suggest features? # Use GitHub Discussions to suggest new features or improvements.\nMigration # How do I migrate from Redis? # Install IceFireDB Configure IceFireDB with same port as Redis Stop Redis Start IceFireDB Verify data accessibility Update clients if needed How do I migrate between storage drivers? # Backup current data Export data from current driver Import data to new driver Update configuration Verify data integrity Can I use Redis tools with IceFireDB? # Yes, most Redis tools work with IceFireDB:\nredis-cli redis-benchmark Redis desktop managers Redis client libraries Advanced Topics # How does IceFireDB handle large datasets? # IceFireDB supports large datasets through:\nEfficient storage engines Memory management Data compression Tiered storage options Distributed architecture What monitoring tools are available? # Built-in:\nINFO command SLOWLOG STATS External:\nPrometheus metrics Grafana dashboards APM tools Log aggregators How do I tune for specific workloads? # Read-heavy:\nLarger cache size Faster storage drivers Read replicas Write-heavy:\nWrite-optimized drivers (BadgerDB) Batch operations Async replication Mixed workload:\nBalanced configuration Monitoring and adjustment Hybrid approaches Still have questions? # If you didn\u0026rsquo;t find answers to your questions:\nCheck the documentation Search GitHub Issues Ask in GitHub Discussions Join the Discord community "},{"id":46,"href":"/icefiredb_docs/icefiredb/news/","title":"News","section":"Overview","content":" Record the bits and pieces of IceFireDB. # "}]