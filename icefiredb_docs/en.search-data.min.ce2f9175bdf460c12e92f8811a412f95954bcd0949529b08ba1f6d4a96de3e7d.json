[{"id":0,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-crdt-kv/","title":"icefiredb-crdt-kv","section":"Develop","content":" icefiredb-crdt-kv # Project introduction # The IceFireDB-CRDT-KV engine can support decentralized P2P networking, data synchronization and consistency between nodes. It is a component of the IceFireDB software ecosystem, thanks to the open source of IPFS.\nFeatures # Easy access to P2P data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-crdt-kv Example # package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } RoadMap # Optimize project structure. Encapsulates the kv engine layer for easy reference by upper-layer applications. "},{"id":1,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/orbitdb/","title":"OrbitDB","section":"Project Comparison","content":" Compared with OrbitDB # OrbitDB is a serverless, distributed, peer-to-peer database.\nOrbitDB uses IPFS as its data storage and IPFS Pubsub and uses CRDTs to automatically sync databases with peers, achieving strong eventual consistency - when all updates are eventually received, all nodes will have the same state.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\n√ | web3 support | No smart contract plan |Smart contracts are being supported、Build data dao database platform | | computer language used to implement | Javascript |Go | | Ecological client language | Javascript |Any client that supports the redis、mysql protocol |\nThanks OrbitDB # During the construction of IceFireDB, we learned a lot of excellent ideas from OrbitDB, and we stood on the shoulders of OrbitDB giants to move forward.\n"},{"id":2,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/overview/","title":"OverView","section":"Designs","content":" OverView # "},{"id":3,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":4,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":5,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":6,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/overview/","title":"OverView","section":"Architecture","content":" OverView # "},{"id":7,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/icefiredb-ipfs-log/","title":"icefiredb-ipfs-log","section":"Develop","content":" icefiredb-ipfs-log # Project introduction # icefiredb-ipfs-log is a distributed immutable, operation-based conflict-free replication data structure that relies on ipfs to store data and merges each peer node data based on pubsub conflict-free. You can easily implement custom data structures such as kv, event, nosql, etc. based on icefiredb-ipfs-log.\nConflict-free log replication model\nLog A Log B | | logA.append(\u0026#34;one\u0026#34;) logB.append(\u0026#34;hello\u0026#34;) | | v v +-----+ +-------+ |\u0026#34;one\u0026#34;| |\u0026#34;hello\u0026#34;| +-----+ +-------+ | | logA.append(\u0026#34;two\u0026#34;) logB.append(\u0026#34;world\u0026#34;) | | v v +-----------+ +---------------+ |\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;| |\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;| +-----------+ +---------------+ | | | | logA.join(logB) \u0026lt;----------+ | v +---------------------------+ |\u0026#34;one\u0026#34;,\u0026#34;hello\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;world\u0026#34;| +---------------------------+ Features # Easy access to P2P \u0026amp;\u0026amp; ipfs-log data consistency function Stable decentralized networking function Friendly program access interface Installing # go get -u github.com/IceFireDB/icefiredb-ipfs-log Example # Example of building a key-value database using icefiredb-ipfs-log # memory key-value：memory-kv leveldb kv ：leveldb-kv Use of key-value databases # Detailed usage example reference\nfunc main() { ctx := context.TODO() // disk cache directory rootPath := \u0026#34;./kvdb\u0026#34; node, api, err := iflog.CreateNode(ctx, rootPath) if err != nil { panic(err) } hostAddr, _ := ma.NewMultiaddr(fmt.Sprintf(\u0026#34;/ipfs/%s\u0026#34;, node.PeerHost.ID().Pretty())) for _, a := range node.PeerHost.Addrs() { fmt.Println(a.Encapsulate(hostAddr).String()) } log := zap.NewNop() dbname := \u0026#34;iflog-event-kv\u0026#34; ev, err := iflog.NewIpfsLog(ctx, api, dbname, \u0026amp;iflog.EventOptions{ Directory: rootPath, Logger: log, }) if err != nil { panic(err) } if err := ev.AnnounceConnect(ctx, node); err != nil { panic(err) } kvdb, err := kv.NewKeyValueDB(ctx, ev, log) if err != nil { panic(err) } // Load old data from disk if err := ev.LoadDisk(ctx); err != nil { panic(err) } kvdb.Put(ctx, \u0026#34;one\u0026#34;, \u0026#34;one\u0026#34;) kvdb.Get(\u0026#34;one\u0026#34;) kvdb.Delete(ctx, \u0026#34;one\u0026#34;) } package main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; icefiredb_crdt_kv \u0026#34;github.com/IceFireDB/icefiredb-crdt-kv/kv\u0026#34; badger2 \u0026#34;github.com/dgraph-io/badger\u0026#34; \u0026#34;github.com/ipfs/go-datastore/query\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; ) func main() { ctx := context.TODO() log := logrus.New() db, err := icefiredb_crdt_kv.NewCRDTKeyValueDB(ctx, icefiredb_crdt_kv.Config{ NodeServiceName: \u0026#34;icefiredb-crdt-kv\u0026#34;, DataSyncChannel: \u0026#34;icefiredb-crdt-kv-data\u0026#34;, NetDiscoveryChannel: \u0026#34;icefiredb-crdt-kv-net\u0026#34;, Namespace: \u0026#34;test\u0026#34;, Logger: log, }) if err != nil { panic(err) } defer db.Close() fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { text := scanner.Text() fields := strings.Fields(text) if len(fields) == 0 { fmt.Printf(\u0026#34;\u0026gt; \u0026#34;) continue } cmd := fields[0] switch cmd { case \u0026#34;exit\u0026#34;, \u0026#34;quit\u0026#34;: return case \u0026#34;get\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } val, err := db.Get(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(string(val)) case \u0026#34;put\u0026#34;: if len(fields) \u0026lt; 3 { printVal(\u0026#34;Missing parameters\u0026#34;) continue } printVal(db.Put(ctx, []byte(fields[1]), []byte(fields[2]))) case \u0026#34;delete\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } printVal(db.Delete(ctx, []byte(fields[1]))) case \u0026#34;has\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing key\u0026#34;) continue } is, err := db.Has(ctx, []byte(fields[1])) if err != nil { printVal(err) continue } printVal(is) case \u0026#34;list\u0026#34;: result, err := db.Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;query\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } //fmt.Println(fields[1], len(fields[1])) q := query.Query{ //Prefix: fields[1], Filters: []query.Filter{ query.FilterKeyPrefix{ Prefix: fields[1], }, }, } result, err := db.Query(ctx, q) if err != nil { printVal(err) continue } //time.Sleep(time.Second) for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;connect\u0026#34;: // 主动连接 if len(fields) \u0026lt; 2 { printVal(\u0026#34;Missing connection address\u0026#34;) continue } err = db.Connect(fields[1]) if err == nil { printVal(\u0026#34;connection succeeded!\u0026#34;) } else { printVal(err) } case \u0026#34;slist\u0026#34;: result, err := db.Store().Query(ctx, query.Query{}) if err != nil { printVal(err) continue } for val := range result.Next() { fmt.Printf(fmt.Sprintf(\u0026#34;%s =\u0026gt; %v\\n\u0026#34;, val.Key, string(val.Value))) } fmt.Print(\u0026#34;\u0026gt; \u0026#34;) case \u0026#34;bquery\u0026#34;: if len(fields) \u0026lt; 2 { printVal(\u0026#34;missing query condition\u0026#34;) continue } db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() prefix := []byte(fields[1]) for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) case \u0026#34;blist\u0026#34;: db.DB().View(func(txn *badger2.Txn) error { opts := badger2.DefaultIteratorOptions opts.PrefetchSize = 10 it := txn.NewIterator(opts) defer it.Close() for it.Rewind(); it.Valid(); it.Next() { item := it.Item() k := item.Key() err := item.Value(func(v []byte) error { fmt.Printf(\u0026#34;key=%s, value=%s\\n\u0026#34;, k, v) return nil }) if err != nil { return err } } return nil }) default: printVal(\u0026#34;\u0026#34;) } } } func printVal(v interface{}) { fmt.Printf(\u0026#34;%v\\n\u0026gt; \u0026#34;, v) } Some code reference sources # go-ipfs-log License # icefiredb-ipfs-log is under the Apache 2.0 license. See the LICENSE directory for details.\n"},{"id":8,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/network/","title":"NetWork","section":"Designs","content":" NetWork Details # "},{"id":9,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":10,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":11,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":12,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/network/","title":"NetWork","section":"Architecture","content":" NetWork Details # "},{"id":13,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/project-comparison/threaddb/","title":"ThreadDB","section":"Project Comparison","content":" Compared with ThreadDB # ThreadDB is a serverless, distributed, peer-to-peer database.\nIceFireDB is a database built for web3 and web2,The core mission of the project is to help applications quickly achieve decentralization,built for Data dao.\nDatabase ThreadDB IceFireDB system target P2P Databases A decentralized database platform built for Data dao. storage engine support IPFS goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS network support type P2P P2P、RAFT、NATS Data type support SQL KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub Software integration method Binary software integration Software library integration, binary software integration、web3 platform integration web3 support No smart contract plan Smart contracts are being supported、Build data dao database platform computer language used to implement Go Go Ecological client language Go Any client that supports the redis、mysql protocol Thanks ThreadDB # Thanks to ThreadDB for letting us see the excellent implementation of decentralized SQL database.\n"},{"id":14,"href":"/icefiredb_docs/icefiredb/engine_overview/","title":"Engine Overview","section":"Overview","content":" IceFireDB - Decentralized database engine # Engine Description # IPFS and Filecoin are excellent decentralized data storage infrastructures, which have revolutionary significance for the construction of web3. However, with the development of the application ecology, archived data such as pictures and videos can be stored in IPFS or FileCoin, but there is still a lack of database-level storage expression. Although the community also has a decentralized storage solution similar to the KV model, it is only the KV model. It cannot meet the use of upper-layer applications. We believe that more and more complex data structures need to be supported to support the decentralized development of applications, including NoSQL data models (such as hash, list, set data structures) and SQL data models. Just like the prosperity of web2\u0026rsquo;s application ecology is inseparable from the contribution of database infrastructures such as memcached, redis, and mysql, the IPFS ecosystem also needs database infrastructure with NoSQL and SQL models.\nApart from storage, IPFS has many excellent components in the ecology, such as libp2p, crdt, ipfs-log and ipld. These components will greatly support the decentralization of data and the ecology of web2. However, since many databases under web2 are mysql and redis at present, if you want to help the applications using these databases achieve decentralization, you need to design more database middleware to facilitate application docking and connect the preceding and the following.\nIn addition to the huge demand for decentralized data storage in the web3 ecosystem, in the current digital age, decentralized networking, decentralized subscriptions, and decentralized storage are used in scenarios such as edge computing, big data, and cloud native. There are more and more strong demands. We combine the demands of web2 and the current situation of web3 to propose the IceFireDB Storage Stack project.\nIceFireDB Storage Stack is committed to creating a complete database storage and database middleware software system for the data decentralization ecosystem. The project currently mainly includes three directions of development: decentralized NoSQL database, decentralized SQLite database, decentralized communication Components and database decentralized middleware (ecological application communication middleware, database decentralized middleware).\nLet\u0026rsquo;s elaborate on the three main construction directions of IceFireDB Storage Stack:\nDecentralized NoSQL database (IceFireDB-NoSQL): Using IPFS as the underlying KV engine, using KV coding technology to achieve more complex data structures, such as hash, set, list, etc., integrate the standard Redis network protocol, allowing applications The IceFIreDB-IPFS-NoSQL database can be used by using the Redis client; the network networking is performed by using libp2p, and the decentralized NoSQL database is constructed by crdt, ipfs-log, and ipld. Decentralized SQLite database (IceFireDB-SQLite): The SQL protocol is currently widely used in the web2 application layer. We design and implement a decentralized SQLite database. The bottom layer uses IPFS-libp2p to build a decentralized network and IPFS-pubsub and peer-to-peer Wait for nodes to synchronize data, and use IPFS CRDT, ipfs-log, and ipld to ensure decentralized consistency of SQL statements. Decentralized database middleware (IceFireDB-PubSub, IceFireDB-Redis-Proxy, IceFireDB-SQLProxy): While we are paying attention to the application development of web3, we also see that IPFS provides data decentralization for applications around the world. Very good libp2p, crdt and other components, we should provide decentralized capabilities for traditional web2 databases and traditional web2 applications, but most web2 applications currently use redis, mysql databases, IceFireDB combined with libp2p, crdt, ipld, ipfs-log The technology adds the wings of data decentralization to traditional mysql and redis, and provides insensitive data decentralization and application decentralization communication capabilities for massive web2 applications, traditional nosql and SQL databases. Engine value # For the web3 world, there is currently no way to write a full-blown client-side application as easily and completely decentralized as in Web2. In Web2, you spin up a database on AWS and have your client applications call that database for reading and writing. But there is nothing like that in Web3. You can\u0026rsquo;t just write data to Ethereum, it\u0026rsquo;s too expensive for most users. Storage protocols such as Filecoin and Arweave are mainly used for archiving data, but do not provide enterprise-level performance guarantees for writing and reading data. IceFireDB Storage Stack uses IPFS as the underlying KV engine, and uses KV encoding technology to achieve more complex NoSQL data structures, For example, hash, set, list, etc. use libp2p, crdt, ipld, ipfs-log to build decentralized NoSQL and SQL databases, and provide RESP and SQL protocol support for easy application access, so that existing massive applications can be changed Solve the decentralized communication and storage capabilities of IPFS at the lowest cost, and expand the application access speed and application ecology of IPFS and Filecoin.\nFor the traditional application fields of web2, most applications currently use Nosql and SQL databases such as redis and mysql. However, with the development of edge computing, big data, cloud native and other fields, these fields also need the support of decentralized network communication and decentralized database storage. We should provide decentralized nosql database and decentralized SQL database to provide decentralized database storage and use support for these applications. It is also necessary to provide decentralized database middleware to increase the capabilities of data broadcast communication and decentralized data storage for traditional redis and mysql databases. The decentralized database, decentralized networking subscription and decentralized database middleware provided by IceFireDB Storage Stack will help IPFS and Filecoin ecology to support decentralized web2 massive applications.\nDatabase technology is the foundation of application storage and application innovation. The development of decentralized application ecology in any era is inseparable from the support of databases and database middleware. IceFireDB Storage Stack is the wings that add data decentralization to applications. It is believed that IceFireDB Storage Stack will be able to support a large number of new applications.\nIceFireDB Storage Stack has been striving to build decentralized NoSQL\\SQL databases and database middleware with richer functions and easier access to application systems based on the decentralized network and decentralized storage technologies of IPFS and Filecoin, helping massive applications to improve Easy access to network decentralization and data decentralization technologies, helping the decentralized application ecosystem to build storage infrastructure, and helping the prosperity of IPFS, Filecoin and the decentralized application ecosystem.\nEngine composition # IceFireDB-NoSQL # The Redis database based on IPFS technology can break the simple kv situation of the current IPFS database and support complex data structures such as hash and list.\nIceFireDB-SQLite # IceFireDB-SQLite database is a decentralized SQLite database. Provide a convenient mechanism to build a global distributed database system. Support users to write data to IceFireDB-SQLite using MySQL protocol. IceFireDB-SQLite stores the data in the SQLite database and synchronizes the data among the nodes in the P2P automatic network.\nIceFireDB-SQLProxy # IceFireDB-SQLProxy is a decentralized SQL database networking system that helps web2 traditional SQL database data decentralization. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. Commands are automatically synchronized between IceFireDB-SQLProxy in the network, and each IceFireDB-SQLProxy writes data to MySQL storage.\nDecentralized networking through IceFireDB-SQLProxy provides web2 program read and write support for SQL, enabling decentralized data synchronization for MySQL database read and write scenarios commonly used in web2 applications.\nIceFireDB-Redis-Proxy # IceFireDB-Redis-proxy database proxy adds decentralization wings to traditional redis databases. Provide a convenient mechanism to build a globally distributed storage system with automatic networking. The instructions are automatically synchronized between the networked redis agents, and the redis agent writes data to the cluster or single-point redis storage. Through the decentralized middleware network proxy, decentralized data synchronization can be enabled for the Redis database commonly used in web2 applications.\nIceFireDB-PubSub # IceFireDB-PubSub is a high performance, high availability and decentralized subscription system.It can seamlessly migrate web2 applications using redis publish and subscribe into a decentralized p2p subscription network.\n"},{"id":15,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/develop/redhub/","title":"redhub-frame","section":"Develop","content":" redhub-frame # Project introduction # High-performance Redis-Server multi-threaded framework, based on RawEpoll model.\nFeatures # Ultra high performance Fully multi-threaded support Low CPU resource consumption Compatible with redis protocol Create a Redis compatible server with RawEpoll model in Go Installing # go get -u github.com/IceFireDB/redhub Example # Here is a simple framework usage example,support the following redis commands:\nSET key value GET key DEL key PING QUIT You can run this example in terminal:\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;github.com/IceFireDB/redhub\u0026#34; \u0026#34;github.com/IceFireDB/redhub/pkg/resp\u0026#34; ) func main() { var mu sync.RWMutex var items = make(map[string][]byte) var network string var addr string var multicore bool var reusePort bool var pprofDebug bool var pprofAddr string flag.StringVar(\u0026amp;network, \u0026#34;network\u0026#34;, \u0026#34;tcp\u0026#34;, \u0026#34;server network (default \\\u0026#34;tcp\\\u0026#34;)\u0026#34;) flag.StringVar(\u0026amp;addr, \u0026#34;addr\u0026#34;, \u0026#34;127.0.0.1:6380\u0026#34;, \u0026#34;server addr (default \\\u0026#34;:6380\\\u0026#34;)\u0026#34;) flag.BoolVar(\u0026amp;multicore, \u0026#34;multicore\u0026#34;, true, \u0026#34;multicore\u0026#34;) flag.BoolVar(\u0026amp;reusePort, \u0026#34;reusePort\u0026#34;, false, \u0026#34;reusePort\u0026#34;) flag.BoolVar(\u0026amp;pprofDebug, \u0026#34;pprofDebug\u0026#34;, false, \u0026#34;open pprof\u0026#34;) flag.StringVar(\u0026amp;pprofAddr, \u0026#34;pprofAddr\u0026#34;, \u0026#34;:8888\u0026#34;, \u0026#34;pprof address\u0026#34;) flag.Parse() if pprofDebug { go func() { http.ListenAndServe(pprofAddr, nil) }() } protoAddr := fmt.Sprintf(\u0026#34;%s://%s\u0026#34;, network, addr) option := redhub.Options{ Multicore: multicore, ReusePort: reusePort, } rh := redhub.NewRedHub( func(c *redhub.Conn) (out []byte, action redhub.Action) { return }, func(c *redhub.Conn, err error) (action redhub.Action) { return }, func(cmd resp.Command, out []byte) ([]byte, redhub.Action) { var status redhub.Action switch strings.ToLower(string(cmd.Args[0])) { default: out = resp.AppendError(out, \u0026#34;ERR unknown command \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39;\u0026#34;) case \u0026#34;ping\u0026#34;: out = resp.AppendString(out, \u0026#34;PONG\u0026#34;) case \u0026#34;quit\u0026#34;: out = resp.AppendString(out, \u0026#34;OK\u0026#34;) status = redhub.Close case \u0026#34;set\u0026#34;: if len(cmd.Args) != 3 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() items[string(cmd.Args[1])] = cmd.Args[2] mu.Unlock() out = resp.AppendString(out, \u0026#34;OK\u0026#34;) case \u0026#34;get\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.RLock() val, ok := items[string(cmd.Args[1])] mu.RUnlock() if !ok { out = resp.AppendNull(out) } else { out = resp.AppendBulk(out, val) } case \u0026#34;del\u0026#34;: if len(cmd.Args) != 2 { out = resp.AppendError(out, \u0026#34;ERR wrong number of arguments for \u0026#39;\u0026#34;+string(cmd.Args[0])+\u0026#34;\u0026#39; command\u0026#34;) break } mu.Lock() _, ok := items[string(cmd.Args[1])] delete(items, string(cmd.Args[1])) mu.Unlock() if !ok { out = resp.AppendInt(out, 0) } else { out = resp.AppendInt(out, 1) } case \u0026#34;config\u0026#34;: // This simple (blank) response is only here to allow for the // redis-benchmark command to work with this example. out = resp.AppendArray(out, 2) out = resp.AppendBulk(out, cmd.Args[2]) out = resp.AppendBulkString(out, \u0026#34;\u0026#34;) } return out, status }, ) log.Printf(\u0026#34;started redhub server at %s\u0026#34;, addr) err := redhub.ListendAndServe(protoAddr, option, rh) if err != nil { log.Fatal(err) } } Benchmarks # Machine information OS : Debian Buster 10.6 64bit CPU : 8 CPU cores Memory : 64.0 GiB Go Version : go1.16.5 linux/amd64 【Redis-server5.0.3】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2306060.50 requests per second GET: 3096742.25 requests per second 【Redis-server6.2.5】 Single-threaded, no disk persistence. # $ ./redis-server --port 6380 --appendonly no $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2076325.75 requests per second GET: 2652801.50 requests per second 【Redis-server6.2.5】 Multi-threaded, no disk persistence. # io-threads-do-reads yes io-threads 8 $ ./redis-server redis.conf $ redis-benchmark -h 127.0.0.1 -p 6379 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 1944692.88 requests per second GET: 2375184.00 requests per second 【RedCon】 Multi-threaded, no disk persistence # $ go run example/clone.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 2332742.25 requests per second GET: 14654162.00 requests per second 【RedHub】 Multi-threaded, no disk persistence # $ go run example/server.go $ redis-benchmark -h 127.0.0.1 -p 6380 -n 50000000 -t set,get -c 512 -P 1024 -q SET: 4087305.00 requests per second GET: 16490765.00 requests per second "},{"id":16,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/storage/","title":"Storage","section":"Designs","content":" Storage Engine Details # "},{"id":17,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":18,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":19,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":20,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/storage/","title":"Storage","section":"Architecture","content":" Storage Engine Details # "},{"id":21,"href":"/icefiredb_docs/icefiredb/vocabulary/","title":"Vocabulary","section":"Overview","content":" Vocabulary # The main terms used in IceFireDB products are listed below.\nItem describe NoSQL Non-relational databases, which store data in a format different from relational tables. LSM In computer science, the log-structured merge-tree (also known as LSM tree, or LSMT) is a data structure with performance characteristics that make it attractive for providing indexed access to files with high insert volume, such as transactional log data. OSS Object Storage Service IPFS The InterPlanetary File System (IPFS) is a protocol, hypermedia and file sharing peer-to-peer network for storing and sharing data in a distributed file system. P2P peer-to-peer network EVM Smart contracts A smart contract is a computer program or a transaction protocol that is intended to automatically execute, control or document legally-relevant events and actions according to the terms of a contract or an agreement. CRDT In distributed computing, a conflict-free replicated data type (CRDT) is a data structure that is replicated across multiple computers in a network, RAFT Raft is a consensus algorithm that is designed to be easy to understand. IPLD IPLD stands for InterPlanetary Linked Data,IPLD is an ecosystem of formats and data structures for building applications that can be fully decentralized. IPFS-LOG IPFS-log is an immutable, operation-based collision-free replication data structure (CRDT) for distributed systems. It is an append-only log that can be used to model the variable sharing state between peers in p2p applications. "},{"id":22,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/protocol/","title":"Protocol","section":"Designs","content":" Protocol Details # "},{"id":23,"href":"/icefiredb_docs/icefiredb/icefiredb-pubsub/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":24,"href":"/icefiredb_docs/icefiredb/icefiredb-redis-proxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":25,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":26,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlproxy/architecture/protocol/","title":"Protocol","section":"Architecture","content":" Protocol Details # "},{"id":27,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/codec/","title":"Codec","section":"Designs","content":" Codec Engine Details # "},{"id":28,"href":"/icefiredb_docs/icefiredb/icefiredb-sqlite/architecture/codec/","title":"Codec","section":"Architecture","content":" Codec Engine Details # "},{"id":29,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/quick_start/","title":"Quick Start","section":"IceFireDB-NoSQL","content":" Quick Start Guide for the IceFireDB Database # This guide walks you through the quickest way to get started with IceFireDB. For non-production environments, you can deploy your IceFireDB database by either of the following methods:\nDeploy a local test cluster，Simulate production deployment on a single machine Deploy a local test cluster # Scenario: Quickly deploy a local IceFireDB cluster for testing using a single macOS or Linux server. As a distributed system, in the same availability zone network, a basic IceFireDB cluster usually consists of 3 IceFireDB instances.\n1.Download and compile the program # git clone https://github.com/IceFireDB/IceFireDB.git IceFireDB-NoSQL cd IceFireDB-NoSQL make \u0026amp;\u0026amp; ls ./bin/IceFireDB If the following message is displayed, you have build IceFireDB-NoSQL successfully:\nif [ ! -d \u0026#34;./bin/\u0026#34; ]; then \\ mkdir bin; \\ fi go build -ldflags \u0026#34;-s -w -X \\\u0026#34;main.BuildVersion=1c102f3\\\u0026#34; -X \\\u0026#34;main.BuildDate=2022-11-21 06:17:29\\\u0026#34;\u0026#34; -o bin/IceFireDB . ./bin/IceFireDB 2.Declare the global environment variable # IceFireDB implements many engines at the bottom, mainly including the following categories. The choice of the bottom engine is initialized through cmd variables.\nEngine type cmd key environment value LevelDB storage-backend goleveldb Badger storage-backend badger IPFS storage-backend ipfs CRDT-KV storage-backend crdt IPFS-LOG storage-backend ipfs-log OrbitDB storage-backend orbitdb OSS storage-backend oss 3.Start the cluster in the current session # mkdir 6001 \u0026amp;\u0026amp; mkdir 6002 \u0026amp;\u0026amp; mkdir 6003 cp ./bin/IceFireDB ./6001 cp ./bin/IceFireDB ./6002 cp ./bin/IceFireDB ./6003 # start node1 /pwd/IceFireDB-NoSQL/6001/IceFireDB -storage-backend ipfs-log -n 1 -a 127.0.0.1:6001 --openreads # start node2 /pwd/IceFireDB-NoSQL/6002/IceFireDB -storage-backend ipfs-log -n 2 -a 127.0.0.1:6002 -j 127.0.0.1:6001 --openreads # start node3 /pwd/IceFireDB-NoSQL/6003/IceFireDB -storage-backend ipfs-log -n 3 -a 127.0.0.1:6003 -j 127.0.0.1:6001 --openreads In the same network availability zone, multiple IceFireDB instances can be added to the same raft network, and the same raft network exposes the standard Redis cluster access interface to meet the access requirements of the Redis client.\n4.Start a new session to access IceFireDB # The above steps start three IceFireDB nodes and form a highly available network with each other.We can use redis-cli to observe the cluster status\nsudo apt-get -y install redis-tools redis-cli cluster nodes We execute the cluster nodes command in the redis-cli terminal, and we can view the cluster status as follows:\n127.0.0.1:6002\u0026gt; cluster nodes 356a192b7913b04c54574d18c28d46e6395428ab 127.0.0.1:6001@6001 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 da4b9237bacccdf19c0760cab7aec4a8359010b0 127.0.0.1:6002@6002 slave 77de68daecd823babbb58edb1c8e14d7106e83bb 0 0 connected 0-16383 77de68daecd823babbb58edb1c8e14d7106e83bb 127.0.0.1:6003@6003 master - 0 0 connected 0-16383 We use redis-cli for data read and write tests：\nredis-cli -c -h 127.0.0.1 -p 6002 127.0.0.1:6002\u0026gt; set foo bar -\u0026gt; Redirected to slot [0] located at 127.0.0.1:6003 OK 127.0.0.1:6003\u0026gt; get foo \u0026#34;bar\u0026#34; We can see that the data can be read and written normally，The current master is an instance of 6003. Since we have enabled the read data permission of all nodes, we can view data in other slave nodes.\nredis-cli -h 127.0.0.1 -p 6001 127.0.0.1:6001\u0026gt; get foo \u0026#34;bar\u0026#34; # Although we can read data in the slave node, we cannot write data directly on the slave node. 127.0.0.1:6001\u0026gt; set foo2 bar2 (error) MOVED 0 127.0.0.1:6003 Advanced Eco Tools # IceFireDB-Proxy: Intelligent network proxy # In the above case, we fully demonstrated the cluster construction and data read-write access, but the master-slave relationship between high-availability nodes, data read-write fault tolerance of the client, and the perception of the status of each node in the cluster are complex, so We launched the IceFireDB-Proxy software, which can shield users from understanding the complexity of the IceFireDB high-availability cluster, and use the IceFireDB cluster like a single IceFireDB.\n"},{"id":30,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/designs/","title":"Designs","section":"IceFireDB-NoSQL","content":" System Design # In order to build a decentralized database, the core of the IceFireDB system is to provide data decentralization and immutability for applications. Aiming at the above goals, we have designed the following core system levels.\n+-+---------------+----+---------------+----+---------------+-+ | Transport | | +---------------+ +---------------+ +---------------+ | | | Cluster | | Cluster | | Cluster | | | | communication | | communication | | communication | | | +---------------+ +---------------+ +---------------+ | +-+---------------+----+-------^-------+----+---------------+-+ | +------------------------------v------------------------------+ | Query Processor | | +-----------------------------------------------------+ | | | Query Parser | | | +-----------------------------------------------------+ | | +-----------------------------------------------------+ | | | Query Optimizer | | | +-----------------------------------------------------+ | +---+--------------------------+--------------------------+---+ | +------------------------------v------------------------------+ | Codec | | +-----------+ +-----------+ | | | Encode | | Decode | | | +-----------+ +-----------+ | | support: kv、list、hash、set | +----------+---------------------------------------^----------+ | | +----------+---------------------------------------+----------+ | |put KV Engine |Get | | +-----v----+ +-----+----+ | | | put(a,b) | | Get(a) | | | +-----+----+ +-----+----+ | | | a:b +-------+ | a | | +-----v----+ +------\u0026gt; store \u0026lt;----+ +-----v----+ | | | CID(b) +----+ +-------+ +---+ cat(hash)| | | +-----+----+ +-----+----+ | | | add(b) | cat | | --------v---------------------------------------v----- | | Leveldb\\Badger\\OSS\\IPFS\\CRDT\\IPFS-LOG | +-------------------------------------------------------------+ At the above system level, IceFireDB refines and implements the following important system components.\nSystem components describe technology used Network layer P2P、RAFT、NATS Storage layer goleveldb、badger、IPFS、CRDT、IPFS-LOG、OSS Protocol layer RESP、SQL Codec layer KV、Strings、Hashes、Lists、Sorted Sets、Sets、SQL、PubSub "},{"id":31,"href":"/icefiredb_docs/icefiredb/","title":"Overview","section":"IceFireDB - Building global database infrastructure","content":" IceFireDB - Building global database infrastructure # Overview # Based on cutting-edge computer science, the IceFireDB project integrates new ideas and research in the fields of message passing, event instruction processing, data consistency and replication, decentralized network, reliable data storage, high-performance network framework, etc.The team of researchers and engineers at IceFireDB combines the cutting-edge ideas from distributed systems and concurrent databases. These ideas combine collision-free replication data types (CRDT) and decentralized appendix-only logs, which can be used to simulate the variable sharing state between peers in P2P applications to create a new data infrastructure with high security, high performance and low latency.\nThe core of IceFireDB architecture is geographically distributed event source and decentralized Log source, with log-level CRDT replication.In order to realize the consistency of replication, IceFireDB provides a stable decentralized networking model, which allows the combination of public networks among different sites. Multiple IceFireDB nodes can be run inside each site, and RAFT network can be formed between nodes, which ensures the data consistency and stable storage within the same site.\nConcurrent change conflict handling # Different from the final consistent database, IceFireDB manages changes at the atomic level and merges data changes made through the network. It maintains a single version of data through CRDT, IPFS-LOG and status convergence engine, and maintains the incremental model of decentralized instructions by building the icefiredb-log library, which meets the broadcast and data sequence consistency of decentralized database logs.\nThe technical use of CRDT and IPFS-LOG is implicit. IceFireDB exposes the conventional Redis, SQL database and API to users by integrating the decentralized technology and functions with RESP and SQL protocol servers.\nAdaptive consistency # IceFireDB data network responds to data reading and updating requests, while maintaining a consistent data view in the world. IceFireDB network can effectively improve the delay and flexibly expand to meet the needs of applications.\nNon-inductive data coordination # IceFireDB allows all locations to read and write local data in parallel, and does not require users to know which data should be placed in which location, or require users to redesign the architecture every time they need to add or delete locations.\nDistributed multi-model database # IceFireDB supports a variety of data models, mainly supporting NoSQL and SQL data scenarios. Store, query and modify data in the form of NoSQL and SQL on our multi-master architecture. Respond to read and update requests with local latency, while maintaining a consistent view of global fast data.\n"},{"id":32,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/tutorials/","title":"Tutorials","section":"IceFireDB-NoSQL","content":" Tutorials # "},{"id":33,"href":"/icefiredb_docs/icefiredb/icefiredb-nosql/deploy/","title":"Deploy","section":"IceFireDB-NoSQL","content":" Deploy # "},{"id":34,"href":"/icefiredb_docs/icefiredb/faqs/","title":"FAQs","section":"Overview","content":" FAQs # 1. Would you be able to write IPLD Schemas and specs for the data structures you\u0026rsquo;re using? This would allow others to implement writers/readers for their data. # Our current database underlying data storage engine implementation is divided into two categories:\nThe first kind: build database synchronization technology based on RESP instructions and SQL statements, which can only grow and cannot be tampered with, and realize data synchronization and consistency guarantee between nodes based on decentralized instruction broadcasting and storage instruction playback. Type two: realize KV storage engine based on IPFS kv, and build rich nosql data structure by coding kv-value, so that the data of database can grow on ipfs completely, instead of relying on ipfs-log function. At present, the first type of database is mainly implemented. We mainly use ipfs-log, an immutable and conflict-free replication data model for distributed systems. Based on ipfs-log, the kv engine is abstracted. Based on this kv engine, it is one of the storage drivers of IceFireDB. Other projects can rely on this kv engine to implement their own writers and readers, but we have not designed a proprietary IPLD data structure at present.\nIceFireDB abstracts a data coding layer on the kv engine, which can support more complex data structures such as hash\\list besides the basic KV operation. Currently, ipfs kv storage has been realized in the data storage directory of Icefire DB, and we hope that the data of the database can be fully grown on ipfs, but at present, we need to solve the need of kv key synchronization first. The expression of nosql and sql data relationship for ipfs kv is also conceiving the design of ipld, but it is not in the current major construction milestone. ipld is what we have been learning recently,If more complicated IPLD design is involved later, we will practice it in the process of learning.\n2. How does the DB run? # In this milestone, our implementation mode is the server running mode, which needs to run in a server somewhere. Our server will support Redis-RESP protocol and MYSQL communication protocol, and clients can use Golang, JS, PHP and other computer languages to connect.\nThere is also a framework way, which allows users to directly embed into the application code. This integration method is not in the current milestone, and we plan to concentrate on realizing it after this milestone.For example:redhub,IceFireDB-crdt-kv,ipfs-nosql-frame.These projects are open sourced by IceFireDB and are licensed under the Apache-2.0 open source license, and are mainly used to build ipfs-nosql-frame project, so that other Golang applications can be integrated more conveniently.\nCompared with the framework mode, the server operation mode has the following advantages:\nProvide standard data protocols (RESP, MYSQL), which can make the application minimize changes and use decentralized database. Mask the complexity under IPFS, including libp2p, ipfs-log and crdt. 3. How will you address mutability of data? # We know the immutability of IPFS itself, and now IceFireDB has two implementation models to solve the variability of data:\nThe first implementation: instruction broadcast model based on ipfs-log: Based on ipfs-log,crdt and libp2p(pubsub), an immutable and operation-based conflict-free replication data model for distributed systems is implemented. Based on ipfs-log, various data structures such as event and kv are encapsulated, and multi-node database instruction broadcast is implemented based on this engine;At that bottom of IceFireDB, we abstract the variable kv engine base on badgerdb and leveldb. any node will broadcast the whole network when it is writing instruction, and the bottom driver of IceFireDB of each node will execute the broadcast instruction to ensure the final consistency of data.\nThe second implementation: full storage model based on ipfs: In addition to the first implementation mode, we are also building the structure of the second type of data, so that the complete data will grow on ipfs. At first, there is an ipfs driver in the IceFireDB driver layer, which will encode and process the upper-level commands into a unified kv data structure, store and change the value, and the generated new cid will be connected with key. However, at present, there is no key broadcast network with multiple nodes and data synchronization. When we connect with the broadcast network, we can build a data model originally grown on ipfs.\n4. What program languages are you targeting? # Our program implementation is Golang.\nWe provide standard Redis-RESP and MYSQL communication protocols, so Redis and MYSQL clients of other computer programming languages can communicate with IceFireDB (JS, Rust)\n"},{"id":35,"href":"/icefiredb_docs/icefiredb/news/","title":"News","section":"Overview","content":" Record the bits and pieces of IceFireDB. # "}]